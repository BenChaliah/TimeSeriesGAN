{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import optuna\nimport mlflow\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.nn.utils import weight_norm\n\nfrom torch.optim.lr_scheduler import StepLR\nfrom mlflow import pytorch\n\nimport pandas as pd\nimport numpy as np\n\nimport datetime"
      ],
      "metadata": {
        "id": "4e52bb00",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a54e0279-c5ba-478a-a50e-f781d3278a1c"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, explode, col\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom pyspark.sql.types import *\nimport struct, time\nimport numpy as np\nimport pandas as pd\nimport urllib\nimport json\n\ntickers = [\"GOOG\", \"AAPL\"]\n\ndf = spark.createDataFrame([(i,) for i in tickers],(\"ticker\",))\n\nreturn_type = ArrayType(MapType(StringType(), StringType()))\n\n@udf(returnType=return_type)\ndef yahoo_udf(ticker_label):\n\n    headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:74.0) Gecko/20100101 Firefox/74.0'}\n    end_ts = int(time.time())\n    start_ts = end_ts - 3600*24*7\n    ticker = ticker_label\n    data_url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}?symbol={ticker}&period1={start_ts}&period2={end_ts}&useYfid=true&interval=1m&includePrePost=true\"\n    values = None\n    data_dict = {i: [] for i in ['timestamp', 'low', 'high', 'close', 'open', 'volume']}\n    try: \n        f = urllib.request.urlopen(urllib.request.Request(data_url, headers=headers))\n        status = f.status\n        if status == 200:\n            jresp = json.loads(f.read().decode(\"utf-8\"))    \n            data_dict['timestamp'] = jresp['chart']['result'][0]['timestamp']\n            for k in ['low', 'high', 'close', 'open', 'volume']:\n                data_dict[k] = jresp['chart']['result'][0]['indicators']['quote'][0][k]\n    except:\n        pass\n    df = pd.DataFrame(data_dict)\n    values = df.to_dict(\"index\").values()\n    return list(values)\n\n\n\nextracted = yahoo_udf(\"ticker\")\nexploded = explode(extracted).alias(\"exploded\")\nexpanded = [\n    col(\"exploded\").getItem(k).alias(k) for k in ['timestamp', 'low', 'high', 'close', 'open', 'volume']\n]\n\npdf = df.select(\"ticker\", exploded).select(\"ticker\", *expanded).toPandas()\npdf.to_csv(\"data.csv\")\npdf"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c4a16e00-072e-4299-89dd-987ecb25f17f"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ticker</th>\n      <th>timestamp</th>\n      <th>low</th>\n      <th>high</th>\n      <th>close</th>\n      <th>open</th>\n      <th>volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GOOG</td>\n      <td>1662451200</td>\n      <td>109.01</td>\n      <td>109.4</td>\n      <td>109.28</td>\n      <td>109.01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GOOG</td>\n      <td>1662451260</td>\n      <td>109.31</td>\n      <td>109.5</td>\n      <td>109.5</td>\n      <td>109.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GOOG</td>\n      <td>1662451320</td>\n      <td>109.31</td>\n      <td>109.5</td>\n      <td>109.37</td>\n      <td>109.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GOOG</td>\n      <td>1662451380</td>\n      <td>109.33</td>\n      <td>109.4</td>\n      <td>109.33</td>\n      <td>109.38</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GOOG</td>\n      <td>1662451440</td>\n      <td>109.4</td>\n      <td>109.5</td>\n      <td>109.4</td>\n      <td>109.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6988</th>\n      <td>AAPL</td>\n      <td>1662767700</td>\n      <td>157.46</td>\n      <td>157.48</td>\n      <td>157.46</td>\n      <td>157.48</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6989</th>\n      <td>AAPL</td>\n      <td>1662767760</td>\n      <td>157.45</td>\n      <td>157.5</td>\n      <td>157.48</td>\n      <td>157.47</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6990</th>\n      <td>AAPL</td>\n      <td>1662767820</td>\n      <td>157.48</td>\n      <td>157.52</td>\n      <td>157.51</td>\n      <td>157.48</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6991</th>\n      <td>AAPL</td>\n      <td>1662767880</td>\n      <td>157.5</td>\n      <td>157.53</td>\n      <td>157.52</td>\n      <td>157.51</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6992</th>\n      <td>AAPL</td>\n      <td>1662767940</td>\n      <td>157.5</td>\n      <td>157.52</td>\n      <td>157.52</td>\n      <td>157.52</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6993 rows × 7 columns</p>\n</div>",
              "textData": null,
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "htmlSandbox",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ticker</th>\n      <th>timestamp</th>\n      <th>low</th>\n      <th>high</th>\n      <th>close</th>\n      <th>open</th>\n      <th>volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GOOG</td>\n      <td>1662451200</td>\n      <td>109.01</td>\n      <td>109.4</td>\n      <td>109.28</td>\n      <td>109.01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GOOG</td>\n      <td>1662451260</td>\n      <td>109.31</td>\n      <td>109.5</td>\n      <td>109.5</td>\n      <td>109.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GOOG</td>\n      <td>1662451320</td>\n      <td>109.31</td>\n      <td>109.5</td>\n      <td>109.37</td>\n      <td>109.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GOOG</td>\n      <td>1662451380</td>\n      <td>109.33</td>\n      <td>109.4</td>\n      <td>109.33</td>\n      <td>109.38</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GOOG</td>\n      <td>1662451440</td>\n      <td>109.4</td>\n      <td>109.5</td>\n      <td>109.4</td>\n      <td>109.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6988</th>\n      <td>AAPL</td>\n      <td>1662767700</td>\n      <td>157.46</td>\n      <td>157.48</td>\n      <td>157.46</td>\n      <td>157.48</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6989</th>\n      <td>AAPL</td>\n      <td>1662767760</td>\n      <td>157.45</td>\n      <td>157.5</td>\n      <td>157.48</td>\n      <td>157.47</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6990</th>\n      <td>AAPL</td>\n      <td>1662767820</td>\n      <td>157.48</td>\n      <td>157.52</td>\n      <td>157.51</td>\n      <td>157.48</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6991</th>\n      <td>AAPL</td>\n      <td>1662767880</td>\n      <td>157.5</td>\n      <td>157.53</td>\n      <td>157.52</td>\n      <td>157.51</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6992</th>\n      <td>AAPL</td>\n      <td>1662767940</td>\n      <td>157.5</td>\n      <td>157.52</td>\n      <td>157.52</td>\n      <td>157.52</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6993 rows × 7 columns</p>\n</div>"
            ]
          }
        }
      ],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetLoader(Dataset):\n\n    def __init__(self, filename, normalize=True):\n        min_seq_arr = []\n        all_v = []\n        pdf = pd.read_csv(filename)\n        for t in pdf.groupby('ticker').apply(lambda df: df.sample(1))['ticker'].values.tolist():\n            a_values_arr = []\n            for k in ['low', 'high', 'close', 'open']:\n                df = pdf.loc[pdf['ticker']==t][['timestamp', k]].rename(columns={k:'value'})\n                df['timestamp'] = pd.to_datetime(df[\"timestamp\"], unit='s')\n                df = df.set_index(\"timestamp\")\n                df['value'] = df['value'].astype(np.float32)\n                df['value'] = df['value'].interpolate()\n                min_seq = min([df.loc[df.index.day==d].shape[0] for d in df.index.day.unique().tolist()])\n                if min_seq > 300:\n                    a_values_arr += [df.value]\n                    min_seq_arr += [min_seq]\n            all_v.append([group[1].to_numpy().tolist()[:min(min_seq_arr)] for i in a_values_arr for group in i.groupby(df.index.date)])\n\n        all_v = [i for i in all_v[:] if len(i)]\n        min_seq = min([np.array(i).shape[1] for i in all_v])\n        data = torch.from_numpy(np.expand_dims(np.array([np.array(j).T[:min_seq].T for i in all_v for j in i]), -1)).float()\n        self.data = self.normalize(data) if normalize else data\n        self.seq_len = data.size(1)\n\n    def normalize(self, x):\n        self.max = x.max()\n        self.min = x.min()\n        return (2 * (x - x.min())/(x.max() - x.min()) - 1)\n    \n    def denormalize(self, x):\n        return 0.5 * (x*self.max - x*self.min + self.max + self.min)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n"
      ],
      "metadata": {
        "id": "eca17654",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8e95d677-37bb-4b2c-ab31-5a8b76aa4780"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "class _Block(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(_Block, self).__init__()\n\n        self.padding = padding\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))       \n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n        \n        self.resample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.resample != None:\n            self.resample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        x0 = self.conv1(x)\n        x0 = x0[:, :, :-self.padding].contiguous()\n        x0 = self.relu1(x0)\n        x0 = self.dropout1(x0)\n\n        x0 = self.conv2(x0)\n        x0 = x0[:, :, :-self.padding].contiguous()\n        x0 = self.relu2(x0)\n        out = self.dropout2(x0)\n\n        res = x if self.resample is None else self.resample(x)\n        return self.relu(out + res)\n\n\n\nclass CausalDialationBlock(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super(CausalDialationBlock, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n         \n        layers = [_Block(num_inputs, num_channels[0], kernel_size, stride=1, dilation=1, padding=kernel_size-1, dropout=dropout)]\n        layers += [_Block(num_channels[i-1], num_channels[i], kernel_size, stride=1, dilation=((2 ** i) % 512), padding=(kernel_size-1) * ((2 ** i) % 512), dropout=dropout) for i in range(1, num_levels)]\n\n        self.network = nn.Sequential(*layers)\n        self.lstm = nn.LSTM(num_inputs, 256, 1, batch_first=True)\n        self.linear = nn.Sequential(nn.Linear(256, 1), nn.Tanh())\n\n    def  forward(self, x):\n        return self.network(x)\n\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_dim, kernel_size=8, n_layers=1, hidden_dim=256, n_channel=10, cnn_layers=4, dropout=0):\n        super().__init__()\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.lstm = nn.LSTM(in_dim, hidden_dim, n_layers, batch_first=True)\n        self.linear_1 = nn.Sequential(nn.Linear(hidden_dim, 1), nn.Sigmoid())\n\n        num_channels = [n_channel] * cnn_layers\n\n        self.causalBlock = CausalDialationBlock(in_dim, num_channels, kernel_size=kernel_size, dropout=dropout)\n        self.rectify = nn.ReLU()\n        self.linear_2 = nn.Linear(num_channels[-1], 1)\n        self.linear_2.weight.data.normal_(0, 0.01)\n\n    def forward(self, input, channel_last=True):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        batch_size, seq_len = input.size(0), input.size(1)\n        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n        c_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n\n        recurrent_features, _ = self.lstm(input, (h_0, c_0))\n        outputs = self.linear_1(recurrent_features.contiguous().view(batch_size * seq_len, self.hidden_dim))\n        outputs = outputs.view(batch_size, seq_len, 1)\n\n        y1 = self.causalBlock(outputs.transpose(1, 2) if channel_last else outputs)\n\n        j = self.linear_2(y1.transpose(1, 2))\n        j0 = self.rectify(j)\n        r = torch.tanh(j0)\n        return r\n\n\nclass Generator(nn.Module):\n    def __init__(self, in_dim, out_dim, n_channel, kernel_size, dropout=0.2, n_layers=1, hidden_dim=256):\n        super().__init__()\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n\n        self.lstm = nn.LSTM(in_dim, hidden_dim, n_layers, batch_first=True)\n        self.linear_1 = nn.Sequential(nn.Linear(hidden_dim, out_dim), nn.Tanh())\n        num_channels = [n_channel] * n_layers\n\n        self.causalBlock = CausalDialationBlock(in_dim, num_channels, kernel_size=kernel_size, dropout=dropout)\n        self.linear_2 = nn.Linear(num_channels[-1], 1)\n        self.linear_2.weight.data.normal_(0, 0.01)\n\n    def forward(self, input, channel_last=True):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        batch_size, seq_len = input.size(0), input.size(1)\n        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n        c_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n\n        recurrent_features, _ = self.lstm(input, (h_0, c_0))\n        y1 = self.causalBlock(recurrent_features.transpose(1, 2) if channel_last else recurrent_features)\n        j = self.linear_2(y1.transpose(1, 2))\n        r = torch.tanh(j)\n        return r"
      ],
      "metadata": {
        "id": "535da177",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f2f1aeac-e4ca-4333-bcf7-921849e675b6"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "def train(netG, netD, device, dataloader, optimizerG, optimizerD, epochs):\n    nz_dim = 100\n    netG.train()\n    netD.train()\n    seq_len = dataloader.dataset[0].size(0)\n    criterion = nn.BCELoss().to(device)\n\n    real_label = 1.0\n    fake_label = 0.0\n    for epoch in range(epochs):\n        for i, data in enumerate(dataloader, 0):\n            if i == 0:\n                real_zero = data.cpu()\n\n            netD.zero_grad()\n\n            real = data.to(device)\n            batch_size, seq_len = real.size(0), real.size(1)\n            label = torch.full((batch_size, seq_len, 1), real_label, device=device)\n            output = netD(real)\n\n            errD_real = criterion(output, label)\n            errD_real.backward()\n            D_x = output.mean().item()\n\n            noise = torch.randn(batch_size, seq_len, nz_dim, device=device)\n\n            fake = netG(noise)    \n\n            label.fill_(fake_label)\n            output = netD(fake.detach())\n\n            errD_fake = criterion(output, label)\n            errD_fake.backward()\n            D_G_z1 = output.mean().item()\n            errD = errD_real + errD_fake\n            optimizerD.step()\n\n            netG.zero_grad()\n            label.fill_(real_label)\n            output = netD(fake)\n\n            errG = criterion(output, label)\n            errG.backward()\n            D_G_z2 = output.mean().item()\n\n            optimizerG.step()\n            print(f\"[{epoch}/{epochs}][{i}/{len(dataloader)}] Loss_D:{errD.item():.4f} Loss_G:{errG.item():.4f} D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}\")\n\n\ndef test(netG, device, dataloader):\n    nz_dim = 100\n    errG = []\n    for i, data in enumerate(dataloader, 0):\n        real = data.to(device)\n        batch_size, seq_len = real.size(0), real.size(1)\n        noise = torch.randn(batch_size, seq_len, nz_dim, device=device)\n        mse = nn.MSELoss()\n        with torch.no_grad():\n            fake = netG(noise)    \n            errG += [mse(fake, real)]\n    v = torch.tensor(errG).float().mean().item()\n    return v"
      ],
      "metadata": {
        "id": "5d909337",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ba34b608-af59-4da3-af42-ccf2ecbc9c58"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_hyperparameters(trial):\n    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n    dropoutG = trial.suggest_float(\"dropoutG\", 0.0, 0.4, step=0.1)\n    dropoutD = trial.suggest_float(\"dropoutD\", 0.0, 0.4, step=0.1)\n    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"Adadelta\"])\n\n    return lr, dropoutG, dropoutD, optimizer_name"
      ],
      "metadata": {
        "id": "31d20316",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3e9c0d4f-e1ce-49fa-a0e9-e573a34ec9ee"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n    best_val_loss = float('Inf')\n    nz_dim = 100\n    best_mse_val = None\n    \n    full_dataset = DatasetLoader(\"data.csv\")\n    train_size = int(0.8 * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n\n    with mlflow.start_run():\n\n        lr, dropoutG, dropoutD, optimizer_name = suggest_hyperparameters(trial)\n        n_epochs = 100\n        torch.manual_seed(123)\n        mlflow.log_params(trial.params)\n\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        mlflow.log_param(\"device\", device)\n\n        \n        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True)\n        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=True)\n\n        netG = Generator(in_dim=nz_dim, n_channel=10, kernel_size=8, out_dim=1, hidden_dim=100, dropout=dropoutG).to(device)\n        netD = Discriminator(in_dim=1, cnn_layers=4, n_layers=1, kernel_size=8, n_channel=10, dropout=dropoutD, hidden_dim=100).to(device)\n\n        optimizerD = getattr(optim, optimizer_name)(netD.parameters(), lr=lr)\n        optimizerG = getattr(optim, optimizer_name)(netG.parameters(), lr=lr)\n        \n        train(netG, netD, device, train_dataloader, optimizerG, optimizerD, n_epochs)\n        mse_errG = test(netG, device, test_dataloader)\n        \n        if best_mse_val is None:\n            best_mse_val = mse_errG\n        best_mse_val = min(best_mse_val, mse_errG)\n        mlflow.log_metric(\"mse_errG\", mse_errG)\n\n    return best_mse_val"
      ],
      "metadata": {
        "id": "9166b175",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "134d0980-8246-499a-a3d0-bdcc2ecafaed"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "run_tag = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M\")\n\nexperiment_id = mlflow.create_experiment(\n    f\"/Users/myusername/TSGAN_Exp_{run_tag}\",\n    tags={\"version\": \"v1\", \"priority\": \"P1\"},\n)\n\nmlflow.set_experiment(experiment_id=experiment_id)\nstudy = optuna.create_study(study_name=f\"TSGAN_study_{run_tag}\", direction=\"minimize\")\nstudy.optimize(objective, n_trials=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ac40629",
        "outputId": "4b94c11e-f50a-48fa-b54b-3a75c9145787",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "84d4d4bf-2f61-47c3-a1e5-4764deaf753b"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "\u001b[32m[I 2022-09-11 09:55:05,615]\u001b[0m A new study created in memory with name: TSGAN_study_2022-09-11T09:55\u001b[0m\n[0/100][0/3] Loss_D:2.0937 Loss_G:1.9368 D(x): 0.1440 D(G(z)): 0.1440 / 0.1442\n[0/100][1/3] Loss_D:2.0925 Loss_G:1.9354 D(x): 0.1442 D(G(z)): 0.1442 / 0.1444\n[0/100][2/3] Loss_D:2.0914 Loss_G:1.9339 D(x): 0.1444 D(G(z)): 0.1444 / 0.1446\n[1/100][0/3] Loss_D:2.0901 Loss_G:1.9324 D(x): 0.1446 D(G(z)): 0.1446 / 0.1448\n[1/100][1/3] Loss_D:2.0889 Loss_G:1.9309 D(x): 0.1448 D(G(z)): 0.1448 / 0.1450\n[1/100][2/3] Loss_D:2.0877 Loss_G:1.9294 D(x): 0.1450 D(G(z)): 0.1450 / 0.1453\n[2/100][0/3] Loss_D:2.0864 Loss_G:1.9279 D(x): 0.1453 D(G(z)): 0.1453 / 0.1455\n[2/100][1/3] Loss_D:2.0851 Loss_G:1.9263 D(x): 0.1455 D(G(z)): 0.1455 / 0.1457\n[2/100][2/3] Loss_D:2.0839 Loss_G:1.9248 D(x): 0.1457 D(G(z)): 0.1457 / 0.1459\n[3/100][0/3] Loss_D:2.0826 Loss_G:1.9232 D(x): 0.1459 D(G(z)): 0.1459 / 0.1462\n[3/100][1/3] Loss_D:2.0813 Loss_G:1.9217 D(x): 0.1462 D(G(z)): 0.1462 / 0.1464\n[3/100][2/3] Loss_D:2.0799 Loss_G:1.9201 D(x): 0.1464 D(G(z)): 0.1464 / 0.1466\n[4/100][0/3] Loss_D:2.0787 Loss_G:1.9185 D(x): 0.1466 D(G(z)): 0.1466 / 0.1468\n[4/100][1/3] Loss_D:2.0774 Loss_G:1.9169 D(x): 0.1468 D(G(z)): 0.1468 / 0.1471\n[4/100][2/3] Loss_D:2.0761 Loss_G:1.9154 D(x): 0.1471 D(G(z)): 0.1471 / 0.1473\n[5/100][0/3] Loss_D:2.0747 Loss_G:1.9138 D(x): 0.1473 D(G(z)): 0.1473 / 0.1476\n[5/100][1/3] Loss_D:2.0735 Loss_G:1.9122 D(x): 0.1475 D(G(z)): 0.1476 / 0.1478\n[5/100][2/3] Loss_D:2.0720 Loss_G:1.9105 D(x): 0.1478 D(G(z)): 0.1478 / 0.1480\n[6/100][0/3] Loss_D:2.0707 Loss_G:1.9089 D(x): 0.1480 D(G(z)): 0.1480 / 0.1483\n[6/100][1/3] Loss_D:2.0695 Loss_G:1.9073 D(x): 0.1483 D(G(z)): 0.1483 / 0.1485\n[6/100][2/3] Loss_D:2.0680 Loss_G:1.9057 D(x): 0.1485 D(G(z)): 0.1485 / 0.1488\n[7/100][0/3] Loss_D:2.0668 Loss_G:1.9040 D(x): 0.1487 D(G(z)): 0.1488 / 0.1490\n[7/100][1/3] Loss_D:2.0654 Loss_G:1.9024 D(x): 0.1490 D(G(z)): 0.1490 / 0.1492\n[7/100][2/3] Loss_D:2.0641 Loss_G:1.9008 D(x): 0.1492 D(G(z)): 0.1492 / 0.1495\n[8/100][0/3] Loss_D:2.0627 Loss_G:1.8991 D(x): 0.1495 D(G(z)): 0.1495 / 0.1497\n[8/100][1/3] Loss_D:2.0614 Loss_G:1.8974 D(x): 0.1497 D(G(z)): 0.1497 / 0.1500\n[8/100][2/3] Loss_D:2.0601 Loss_G:1.8958 D(x): 0.1500 D(G(z)): 0.1500 / 0.1502\n[9/100][0/3] Loss_D:2.0587 Loss_G:1.8941 D(x): 0.1502 D(G(z)): 0.1502 / 0.1505\n[9/100][1/3] Loss_D:2.0572 Loss_G:1.8925 D(x): 0.1505 D(G(z)): 0.1505 / 0.1507\n[9/100][2/3] Loss_D:2.0560 Loss_G:1.8908 D(x): 0.1507 D(G(z)): 0.1507 / 0.1510\n[10/100][0/3] Loss_D:2.0545 Loss_G:1.8891 D(x): 0.1510 D(G(z)): 0.1510 / 0.1512\n[10/100][1/3] Loss_D:2.0531 Loss_G:1.8874 D(x): 0.1512 D(G(z)): 0.1512 / 0.1515\n[10/100][2/3] Loss_D:2.0518 Loss_G:1.8857 D(x): 0.1515 D(G(z)): 0.1515 / 0.1518\n[11/100][0/3] Loss_D:2.0504 Loss_G:1.8840 D(x): 0.1517 D(G(z)): 0.1518 / 0.1520\n[11/100][1/3] Loss_D:2.0489 Loss_G:1.8823 D(x): 0.1520 D(G(z)): 0.1520 / 0.1523\n[11/100][2/3] Loss_D:2.0477 Loss_G:1.8806 D(x): 0.1523 D(G(z)): 0.1523 / 0.1525\n[12/100][0/3] Loss_D:2.0461 Loss_G:1.8789 D(x): 0.1525 D(G(z)): 0.1525 / 0.1528\n[12/100][1/3] Loss_D:2.0448 Loss_G:1.8772 D(x): 0.1528 D(G(z)): 0.1528 / 0.1531\n[12/100][2/3] Loss_D:2.0432 Loss_G:1.8755 D(x): 0.1531 D(G(z)): 0.1531 / 0.1533\n[13/100][0/3] Loss_D:2.0419 Loss_G:1.8737 D(x): 0.1533 D(G(z)): 0.1533 / 0.1536\n[13/100][1/3] Loss_D:2.0405 Loss_G:1.8720 D(x): 0.1536 D(G(z)): 0.1536 / 0.1539\n[13/100][2/3] Loss_D:2.0392 Loss_G:1.8702 D(x): 0.1538 D(G(z)): 0.1539 / 0.1541\n[14/100][0/3] Loss_D:2.0377 Loss_G:1.8685 D(x): 0.1541 D(G(z)): 0.1541 / 0.1544\n[14/100][1/3] Loss_D:2.0362 Loss_G:1.8667 D(x): 0.1544 D(G(z)): 0.1544 / 0.1547\n[14/100][2/3] Loss_D:2.0346 Loss_G:1.8650 D(x): 0.1547 D(G(z)): 0.1547 / 0.1549\n[15/100][0/3] Loss_D:2.0334 Loss_G:1.8632 D(x): 0.1549 D(G(z)): 0.1549 / 0.1552\n[15/100][1/3] Loss_D:2.0319 Loss_G:1.8614 D(x): 0.1552 D(G(z)): 0.1552 / 0.1555\n[15/100][2/3] Loss_D:2.0306 Loss_G:1.8596 D(x): 0.1555 D(G(z)): 0.1555 / 0.1558\n[16/100][0/3] Loss_D:2.0290 Loss_G:1.8578 D(x): 0.1558 D(G(z)): 0.1558 / 0.1561\n[16/100][1/3] Loss_D:2.0276 Loss_G:1.8561 D(x): 0.1560 D(G(z)): 0.1561 / 0.1563\n[16/100][2/3] Loss_D:2.0259 Loss_G:1.8543 D(x): 0.1564 D(G(z)): 0.1563 / 0.1566\n[17/100][0/3] Loss_D:2.0246 Loss_G:1.8525 D(x): 0.1566 D(G(z)): 0.1566 / 0.1569\n[17/100][1/3] Loss_D:2.0232 Loss_G:1.8506 D(x): 0.1569 D(G(z)): 0.1569 / 0.1572\n[17/100][2/3] Loss_D:2.0218 Loss_G:1.8488 D(x): 0.1572 D(G(z)): 0.1572 / 0.1575\n[18/100][0/3] Loss_D:2.0202 Loss_G:1.8470 D(x): 0.1575 D(G(z)): 0.1575 / 0.1578\n[18/100][1/3] Loss_D:2.0187 Loss_G:1.8452 D(x): 0.1578 D(G(z)): 0.1578 / 0.1580\n[18/100][2/3] Loss_D:2.0174 Loss_G:1.8433 D(x): 0.1580 D(G(z)): 0.1580 / 0.1583\n[19/100][0/3] Loss_D:2.0158 Loss_G:1.8415 D(x): 0.1583 D(G(z)): 0.1583 / 0.1586\n[19/100][1/3] Loss_D:2.0143 Loss_G:1.8397 D(x): 0.1586 D(G(z)): 0.1586 / 0.1589\n[19/100][2/3] Loss_D:2.0129 Loss_G:1.8378 D(x): 0.1589 D(G(z)): 0.1589 / 0.1592\n[20/100][0/3] Loss_D:2.0113 Loss_G:1.8359 D(x): 0.1592 D(G(z)): 0.1592 / 0.1595\n[20/100][1/3] Loss_D:2.0098 Loss_G:1.8341 D(x): 0.1595 D(G(z)): 0.1595 / 0.1598\n[20/100][2/3] Loss_D:2.0084 Loss_G:1.8322 D(x): 0.1598 D(G(z)): 0.1598 / 0.1601\n[21/100][0/3] Loss_D:2.0068 Loss_G:1.8303 D(x): 0.1601 D(G(z)): 0.1601 / 0.1604\n[21/100][1/3] Loss_D:2.0052 Loss_G:1.8284 D(x): 0.1604 D(G(z)): 0.1604 / 0.1607\n[21/100][2/3] Loss_D:2.0038 Loss_G:1.8265 D(x): 0.1607 D(G(z)): 0.1607 / 0.1610\n[22/100][0/3] Loss_D:2.0021 Loss_G:1.8246 D(x): 0.1610 D(G(z)): 0.1610 / 0.1613\n[22/100][1/3] Loss_D:2.0007 Loss_G:1.8227 D(x): 0.1613 D(G(z)): 0.1613 / 0.1616\n[22/100][2/3] Loss_D:1.9992 Loss_G:1.8208 D(x): 0.1616 D(G(z)): 0.1616 / 0.1620\n[23/100][0/3] Loss_D:1.9976 Loss_G:1.8189 D(x): 0.1619 D(G(z)): 0.1620 / 0.1623\n[23/100][1/3] Loss_D:1.9960 Loss_G:1.8170 D(x): 0.1623 D(G(z)): 0.1623 / 0.1626\n[23/100][2/3] Loss_D:1.9946 Loss_G:1.8150 D(x): 0.1626 D(G(z)): 0.1626 / 0.1629\n[24/100][0/3] Loss_D:1.9929 Loss_G:1.8131 D(x): 0.1629 D(G(z)): 0.1629 / 0.1632\n[24/100][1/3] Loss_D:1.9914 Loss_G:1.8111 D(x): 0.1632 D(G(z)): 0.1632 / 0.1635\n[24/100][2/3] Loss_D:1.9899 Loss_G:1.8092 D(x): 0.1635 D(G(z)): 0.1635 / 0.1639\n[25/100][0/3] Loss_D:1.9882 Loss_G:1.8072 D(x): 0.1639 D(G(z)): 0.1639 / 0.1642\n[25/100][1/3] Loss_D:1.9867 Loss_G:1.8053 D(x): 0.1642 D(G(z)): 0.1642 / 0.1645\n[25/100][2/3] Loss_D:1.9852 Loss_G:1.8033 D(x): 0.1645 D(G(z)): 0.1645 / 0.1648\n[26/100][0/3] Loss_D:1.9834 Loss_G:1.8013 D(x): 0.1648 D(G(z)): 0.1648 / 0.1652\n[26/100][1/3] Loss_D:1.9819 Loss_G:1.7993 D(x): 0.1651 D(G(z)): 0.1652 / 0.1655\n[26/100][2/3] Loss_D:1.9800 Loss_G:1.7973 D(x): 0.1655 D(G(z)): 0.1655 / 0.1658\n[27/100][0/3] Loss_D:1.9787 Loss_G:1.7953 D(x): 0.1658 D(G(z)): 0.1658 / 0.1661\n[27/100][1/3] Loss_D:1.9770 Loss_G:1.7933 D(x): 0.1661 D(G(z)): 0.1661 / 0.1665\n[27/100][2/3] Loss_D:1.9756 Loss_G:1.7913 D(x): 0.1665 D(G(z)): 0.1665 / 0.1668\n[28/100][0/3] Loss_D:1.9739 Loss_G:1.7892 D(x): 0.1668 D(G(z)): 0.1668 / 0.1672\n[28/100][1/3] Loss_D:1.9722 Loss_G:1.7872 D(x): 0.1672 D(G(z)): 0.1672 / 0.1675\n[28/100][2/3] Loss_D:1.9704 Loss_G:1.7852 D(x): 0.1675 D(G(z)): 0.1675 / 0.1678\n[29/100][0/3] Loss_D:1.9690 Loss_G:1.7831 D(x): 0.1678 D(G(z)): 0.1678 / 0.1682\n[29/100][1/3] Loss_D:1.9673 Loss_G:1.7811 D(x): 0.1682 D(G(z)): 0.1682 / 0.1685\n[29/100][2/3] Loss_D:1.9658 Loss_G:1.7790 D(x): 0.1685 D(G(z)): 0.1685 / 0.1689\n[30/100][0/3] Loss_D:1.9640 Loss_G:1.7769 D(x): 0.1689 D(G(z)): 0.1689 / 0.1692\n[30/100][1/3] Loss_D:1.9624 Loss_G:1.7748 D(x): 0.1692 D(G(z)): 0.1692 / 0.1696\n[30/100][2/3] Loss_D:1.9609 Loss_G:1.7727 D(x): 0.1696 D(G(z)): 0.1696 / 0.1700\n[31/100][0/3] Loss_D:1.9591 Loss_G:1.7706 D(x): 0.1699 D(G(z)): 0.1700 / 0.1703\n[31/100][1/3] Loss_D:1.9574 Loss_G:1.7685 D(x): 0.1703 D(G(z)): 0.1703 / 0.1707\n[31/100][2/3] Loss_D:1.9559 Loss_G:1.7664 D(x): 0.1706 D(G(z)): 0.1707 / 0.1710\n[32/100][0/3] Loss_D:1.9541 Loss_G:1.7643 D(x): 0.1710 D(G(z)): 0.1710 / 0.1714\n[32/100][1/3] Loss_D:1.9524 Loss_G:1.7622 D(x): 0.1714 D(G(z)): 0.1714 / 0.1718\n[32/100][2/3] Loss_D:1.9505 Loss_G:1.7600 D(x): 0.1718 D(G(z)): 0.1718 / 0.1721\n[33/100][0/3] Loss_D:1.9490 Loss_G:1.7579 D(x): 0.1721 D(G(z)): 0.1721 / 0.1725\n[33/100][1/3] Loss_D:1.9473 Loss_G:1.7557 D(x): 0.1725 D(G(z)): 0.1725 / 0.1729\n[33/100][2/3] Loss_D:1.9454 Loss_G:1.7536 D(x): 0.1729 D(G(z)): 0.1729 / 0.1733\n[34/100][0/3] Loss_D:1.9439 Loss_G:1.7514 D(x): 0.1732 D(G(z)): 0.1733 / 0.1736\n[34/100][1/3] Loss_D:1.9422 Loss_G:1.7492 D(x): 0.1736 D(G(z)): 0.1736 / 0.1740\n[34/100][2/3] Loss_D:1.9406 Loss_G:1.7470 D(x): 0.1740 D(G(z)): 0.1740 / 0.1744\n[35/100][0/3] Loss_D:1.9388 Loss_G:1.7448 D(x): 0.1744 D(G(z)): 0.1744 / 0.1748\n[35/100][1/3] Loss_D:1.9370 Loss_G:1.7426 D(x): 0.1748 D(G(z)): 0.1748 / 0.1752\n[35/100][2/3] Loss_D:1.9354 Loss_G:1.7404 D(x): 0.1751 D(G(z)): 0.1752 / 0.1756\n[36/100][0/3] Loss_D:1.9336 Loss_G:1.7382 D(x): 0.1755 D(G(z)): 0.1756 / 0.1759\n[36/100][1/3] Loss_D:1.9318 Loss_G:1.7360 D(x): 0.1759 D(G(z)): 0.1759 / 0.1763\n[36/100][2/3] Loss_D:1.9302 Loss_G:1.7337 D(x): 0.1763 D(G(z)): 0.1763 / 0.1767\n[37/100][0/3] Loss_D:1.9283 Loss_G:1.7315 D(x): 0.1767 D(G(z)): 0.1767 / 0.1771\n[37/100][1/3] Loss_D:1.9265 Loss_G:1.7292 D(x): 0.1771 D(G(z)): 0.1771 / 0.1775\n[37/100][2/3] Loss_D:1.9249 Loss_G:1.7270 D(x): 0.1775 D(G(z)): 0.1775 / 0.1779\n[38/100][0/3] Loss_D:1.9230 Loss_G:1.7247 D(x): 0.1779 D(G(z)): 0.1779 / 0.1783\n[38/100][1/3] Loss_D:1.9212 Loss_G:1.7224 D(x): 0.1783 D(G(z)): 0.1783 / 0.1788\n[38/100][2/3] Loss_D:1.9192 Loss_G:1.7201 D(x): 0.1788 D(G(z)): 0.1788 / 0.1792\n[39/100][0/3] Loss_D:1.9177 Loss_G:1.7178 D(x): 0.1792 D(G(z)): 0.1792 / 0.1796\n[39/100][1/3] Loss_D:1.9159 Loss_G:1.7155 D(x): 0.1796 D(G(z)): 0.1796 / 0.1800\n[39/100][2/3] Loss_D:1.9138 Loss_G:1.7132 D(x): 0.1800 D(G(z)): 0.1800 / 0.1804\n[40/100][0/3] Loss_D:1.9122 Loss_G:1.7109 D(x): 0.1804 D(G(z)): 0.1804 / 0.1808\n[40/100][1/3] Loss_D:1.9104 Loss_G:1.7085 D(x): 0.1808 D(G(z)): 0.1808 / 0.1813\n[40/100][2/3] Loss_D:1.9088 Loss_G:1.7062 D(x): 0.1812 D(G(z)): 0.1813 / 0.1817\n[41/100][0/3] Loss_D:1.9068 Loss_G:1.7038 D(x): 0.1817 D(G(z)): 0.1817 / 0.1821\n[41/100][1/3] Loss_D:1.9050 Loss_G:1.7014 D(x): 0.1821 D(G(z)): 0.1821 / 0.1826\n[41/100][2/3] Loss_D:1.9033 Loss_G:1.6991 D(x): 0.1825 D(G(z)): 0.1826 / 0.1830\n[42/100][0/3] Loss_D:1.9014 Loss_G:1.6967 D(x): 0.1830 D(G(z)): 0.1830 / 0.1834\n[42/100][1/3] Loss_D:1.8994 Loss_G:1.6943 D(x): 0.1834 D(G(z)): 0.1834 / 0.1839\n[42/100][2/3] Loss_D:1.8973 Loss_G:1.6919 D(x): 0.1839 D(G(z)): 0.1839 / 0.1843\n[43/100][0/3] Loss_D:1.8957 Loss_G:1.6895 D(x): 0.1843 D(G(z)): 0.1843 / 0.1848\n[43/100][1/3] Loss_D:1.8938 Loss_G:1.6870 D(x): 0.1848 D(G(z)): 0.1848 / 0.1852\n[43/100][2/3] Loss_D:1.8922 Loss_G:1.6846 D(x): 0.1852 D(G(z)): 0.1852 / 0.1857\n[44/100][0/3] Loss_D:1.8900 Loss_G:1.6822 D(x): 0.1857 D(G(z)): 0.1857 / 0.1861\n[44/100][1/3] Loss_D:1.8883 Loss_G:1.6797 D(x): 0.1861 D(G(z)): 0.1861 / 0.1866\n[44/100][2/3] Loss_D:1.8865 Loss_G:1.6772 D(x): 0.1865 D(G(z)): 0.1866 / 0.1871\n[45/100][0/3] Loss_D:1.8845 Loss_G:1.6748 D(x): 0.1870 D(G(z)): 0.1871 / 0.1875\n[45/100][1/3] Loss_D:1.8824 Loss_G:1.6723 D(x): 0.1875 D(G(z)): 0.1875 / 0.1880\n[45/100][2/3] Loss_D:1.8808 Loss_G:1.6699 D(x): 0.1879 D(G(z)): 0.1880 / 0.1884\n[46/100][0/3] Loss_D:1.8787 Loss_G:1.6674 D(x): 0.1884 D(G(z)): 0.1884 / 0.1889\n[46/100][1/3] Loss_D:1.8769 Loss_G:1.6649 D(x): 0.1889 D(G(z)): 0.1889 / 0.1894\n[46/100][2/3] Loss_D:1.8752 Loss_G:1.6624 D(x): 0.1893 D(G(z)): 0.1894 / 0.1899\n[47/100][0/3] Loss_D:1.8731 Loss_G:1.6599 D(x): 0.1898 D(G(z)): 0.1899 / 0.1903\n[47/100][1/3] Loss_D:1.8711 Loss_G:1.6574 D(x): 0.1903 D(G(z)): 0.1903 / 0.1908\n[47/100][2/3] Loss_D:1.8694 Loss_G:1.6548 D(x): 0.1908 D(G(z)): 0.1908 / 0.1913\n[48/100][0/3] Loss_D:1.8673 Loss_G:1.6523 D(x): 0.1913 D(G(z)): 0.1913 / 0.1918\n[48/100][1/3] Loss_D:1.8654 Loss_G:1.6498 D(x): 0.1918 D(G(z)): 0.1918 / 0.1923\n[48/100][2/3] Loss_D:1.8631 Loss_G:1.6472 D(x): 0.1923 D(G(z)): 0.1923 / 0.1928\n[49/100][0/3] Loss_D:1.8615 Loss_G:1.6446 D(x): 0.1928 D(G(z)): 0.1928 / 0.1933\n[49/100][1/3] Loss_D:1.8595 Loss_G:1.6421 D(x): 0.1933 D(G(z)): 0.1933 / 0.1938\n[49/100][2/3] Loss_D:1.8573 Loss_G:1.6395 D(x): 0.1938 D(G(z)): 0.1938 / 0.1943\n[50/100][0/3] Loss_D:1.8556 Loss_G:1.6369 D(x): 0.1943 D(G(z)): 0.1943 / 0.1948\n[50/100][1/3] Loss_D:1.8537 Loss_G:1.6343 D(x): 0.1948 D(G(z)): 0.1948 / 0.1953\n[50/100][2/3] Loss_D:1.8514 Loss_G:1.6316 D(x): 0.1954 D(G(z)): 0.1953 / 0.1958\n[51/100][0/3] Loss_D:1.8498 Loss_G:1.6290 D(x): 0.1958 D(G(z)): 0.1958 / 0.1963\n[51/100][1/3] Loss_D:1.8477 Loss_G:1.6264 D(x): 0.1963 D(G(z)): 0.1963 / 0.1969\n[51/100][2/3] Loss_D:1.8454 Loss_G:1.6237 D(x): 0.1969 D(G(z)): 0.1969 / 0.1974\n[52/100][0/3] Loss_D:1.8437 Loss_G:1.6210 D(x): 0.1974 D(G(z)): 0.1974 / 0.1979\n[52/100][1/3] Loss_D:1.8417 Loss_G:1.6184 D(x): 0.1979 D(G(z)): 0.1979 / 0.1985\n[52/100][2/3] Loss_D:1.8399 Loss_G:1.6157 D(x): 0.1984 D(G(z)): 0.1985 / 0.1990\n[53/100][0/3] Loss_D:1.8377 Loss_G:1.6130 D(x): 0.1990 D(G(z)): 0.1990 / 0.1995\n[53/100][1/3] Loss_D:1.8357 Loss_G:1.6103 D(x): 0.1995 D(G(z)): 0.1995 / 0.2001\n[53/100][2/3] Loss_D:1.8333 Loss_G:1.6075 D(x): 0.2001 D(G(z)): 0.2001 / 0.2006\n[54/100][0/3] Loss_D:1.8316 Loss_G:1.6048 D(x): 0.2006 D(G(z)): 0.2006 / 0.2012\n[54/100][1/3] Loss_D:1.8296 Loss_G:1.6021 D(x): 0.2012 D(G(z)): 0.2012 / 0.2017\n[54/100][2/3] Loss_D:1.8272 Loss_G:1.5993 D(x): 0.2018 D(G(z)): 0.2017 / 0.2023\n[55/100][0/3] Loss_D:1.8255 Loss_G:1.5966 D(x): 0.2023 D(G(z)): 0.2023 / 0.2029\n[55/100][1/3] Loss_D:1.8235 Loss_G:1.5938 D(x): 0.2028 D(G(z)): 0.2029 / 0.2034\n[55/100][2/3] Loss_D:1.8210 Loss_G:1.5910 D(x): 0.2035 D(G(z)): 0.2034 / 0.2040\n[56/100][0/3] Loss_D:1.8193 Loss_G:1.5882 D(x): 0.2040 D(G(z)): 0.2040 / 0.2046\n[56/100][1/3] Loss_D:1.8172 Loss_G:1.5854 D(x): 0.2046 D(G(z)): 0.2046 / 0.2052\n[56/100][2/3] Loss_D:1.8154 Loss_G:1.5826 D(x): 0.2051 D(G(z)): 0.2052 / 0.2057\n[57/100][0/3] Loss_D:1.8130 Loss_G:1.5797 D(x): 0.2057 D(G(z)): 0.2057 / 0.2063\n[57/100][1/3] Loss_D:1.8110 Loss_G:1.5769 D(x): 0.2063 D(G(z)): 0.2063 / 0.2069\n[57/100][2/3] Loss_D:1.8091 Loss_G:1.5740 D(x): 0.2069 D(G(z)): 0.2069 / 0.2075\n[58/100][0/3] Loss_D:1.8068 Loss_G:1.5712 D(x): 0.2075 D(G(z)): 0.2075 / 0.2081\n[58/100][1/3] Loss_D:1.8046 Loss_G:1.5683 D(x): 0.2081 D(G(z)): 0.2081 / 0.2087\n[58/100][2/3] Loss_D:1.8028 Loss_G:1.5654 D(x): 0.2087 D(G(z)): 0.2087 / 0.2093\n[59/100][0/3] Loss_D:1.8004 Loss_G:1.5625 D(x): 0.2093 D(G(z)): 0.2093 / 0.2099\n[59/100][1/3] Loss_D:1.7983 Loss_G:1.5596 D(x): 0.2099 D(G(z)): 0.2099 / 0.2105\n[59/100][2/3] Loss_D:1.7958 Loss_G:1.5567 D(x): 0.2106 D(G(z)): 0.2105 / 0.2112\n[60/100][0/3] Loss_D:1.7941 Loss_G:1.5537 D(x): 0.2111 D(G(z)): 0.2112 / 0.2118\n[60/100][1/3] Loss_D:1.7919 Loss_G:1.5508 D(x): 0.2118 D(G(z)): 0.2118 / 0.2124\n[60/100][2/3] Loss_D:1.7894 Loss_G:1.5478 D(x): 0.2125 D(G(z)): 0.2124 / 0.2131\n[61/100][0/3] Loss_D:1.7876 Loss_G:1.5448 D(x): 0.2130 D(G(z)): 0.2131 / 0.2137\n[61/100][1/3] Loss_D:1.7855 Loss_G:1.5419 D(x): 0.2137 D(G(z)): 0.2137 / 0.2143\n[61/100][2/3] Loss_D:1.7835 Loss_G:1.5389 D(x): 0.2143 D(G(z)): 0.2143 / 0.2150\n[62/100][0/3] Loss_D:1.7810 Loss_G:1.5358 D(x): 0.2150 D(G(z)): 0.2150 / 0.2156\n[62/100][1/3] Loss_D:1.7790 Loss_G:1.5328 D(x): 0.2156 D(G(z)): 0.2156 / 0.2163\n[62/100][2/3] Loss_D:1.7770 Loss_G:1.5298 D(x): 0.2162 D(G(z)): 0.2163 / 0.2170\n[63/100][0/3] Loss_D:1.7746 Loss_G:1.5267 D(x): 0.2169 D(G(z)): 0.2170 / 0.2176\n[63/100][1/3] Loss_D:1.7723 Loss_G:1.5237 D(x): 0.2176 D(G(z)): 0.2176 / 0.2183\n[63/100][2/3] Loss_D:1.7704 Loss_G:1.5206 D(x): 0.2182 D(G(z)): 0.2183 / 0.2190\n[64/100][0/3] Loss_D:1.7679 Loss_G:1.5175 D(x): 0.2190 D(G(z)): 0.2190 / 0.2197\n[64/100][1/3] Loss_D:1.7658 Loss_G:1.5144 D(x): 0.2196 D(G(z)): 0.2197 / 0.2203\n[64/100][2/3] Loss_D:1.7638 Loss_G:1.5113 D(x): 0.2203 D(G(z)): 0.2203 / 0.2210\n[65/100][0/3] Loss_D:1.7614 Loss_G:1.5082 D(x): 0.2210 D(G(z)): 0.2210 / 0.2217\n[65/100][1/3] Loss_D:1.7591 Loss_G:1.5051 D(x): 0.2217 D(G(z)): 0.2217 / 0.2224\n[65/100][2/3] Loss_D:1.7571 Loss_G:1.5019 D(x): 0.2224 D(G(z)): 0.2224 / 0.2231\n[66/100][0/3] Loss_D:1.7545 Loss_G:1.4988 D(x): 0.2231 D(G(z)): 0.2231 / 0.2238\n[66/100][1/3] Loss_D:1.7525 Loss_G:1.4956 D(x): 0.2238 D(G(z)): 0.2238 / 0.2246\n[66/100][2/3] Loss_D:1.7504 Loss_G:1.4924 D(x): 0.2245 D(G(z)): 0.2246 / 0.2253\n[67/100][0/3] Loss_D:1.7478 Loss_G:1.4892 D(x): 0.2253 D(G(z)): 0.2253 / 0.2260\n[67/100][1/3] Loss_D:1.7457 Loss_G:1.4860 D(x): 0.2260 D(G(z)): 0.2260 / 0.2267\n[67/100][2/3] Loss_D:1.7436 Loss_G:1.4828 D(x): 0.2267 D(G(z)): 0.2267 / 0.2275\n[68/100][0/3] Loss_D:1.7411 Loss_G:1.4796 D(x): 0.2275 D(G(z)): 0.2275 / 0.2282\n[68/100][1/3] Loss_D:1.7389 Loss_G:1.4763 D(x): 0.2282 D(G(z)): 0.2282 / 0.2290\n[68/100][2/3] Loss_D:1.7368 Loss_G:1.4731 D(x): 0.2289 D(G(z)): 0.2290 / 0.2297\n[69/100][0/3] Loss_D:1.7343 Loss_G:1.4698 D(x): 0.2297 D(G(z)): 0.2297 / 0.2305\n[69/100][1/3] Loss_D:1.7321 Loss_G:1.4665 D(x): 0.2304 D(G(z)): 0.2305 / 0.2312\n[69/100][2/3] Loss_D:1.7293 Loss_G:1.4632 D(x): 0.2313 D(G(z)): 0.2312 / 0.2320\n[70/100][0/3] Loss_D:1.7274 Loss_G:1.4599 D(x): 0.2320 D(G(z)): 0.2320 / 0.2328\n[70/100][1/3] Loss_D:1.7252 Loss_G:1.4566 D(x): 0.2328 D(G(z)): 0.2328 / 0.2336\n[70/100][2/3] Loss_D:1.7224 Loss_G:1.4532 D(x): 0.2337 D(G(z)): 0.2336 / 0.2344\n[71/100][0/3] Loss_D:1.7205 Loss_G:1.4499 D(x): 0.2344 D(G(z)): 0.2344 / 0.2352\n[71/100][1/3] Loss_D:1.7183 Loss_G:1.4465 D(x): 0.2351 D(G(z)): 0.2352 / 0.2360\n[71/100][2/3] Loss_D:1.7162 Loss_G:1.4432 D(x): 0.2359 D(G(z)): 0.2360 / 0.2368\n[72/100][0/3] Loss_D:1.7136 Loss_G:1.4398 D(x): 0.2367 D(G(z)): 0.2368 / 0.2376\n[72/100][1/3] Loss_D:1.7113 Loss_G:1.4364 D(x): 0.2376 D(G(z)): 0.2376 / 0.2384\n[72/100][2/3] Loss_D:1.7093 Loss_G:1.4330 D(x): 0.2383 D(G(z)): 0.2384 / 0.2392\n[73/100][0/3] Loss_D:1.7066 Loss_G:1.4295 D(x): 0.2392 D(G(z)): 0.2392 / 0.2400\n[73/100][1/3] Loss_D:1.7043 Loss_G:1.4261 D(x): 0.2400 D(G(z)): 0.2400 / 0.2409\n[73/100][2/3] Loss_D:1.7023 Loss_G:1.4227 D(x): 0.2408 D(G(z)): 0.2409 / 0.2417\n[74/100][0/3] Loss_D:1.6996 Loss_G:1.4192 D(x): 0.2417 D(G(z)): 0.2417 / 0.2426\n[74/100][1/3] Loss_D:1.6974 Loss_G:1.4157 D(x): 0.2425 D(G(z)): 0.2426 / 0.2434\n[74/100][2/3] Loss_D:1.6945 Loss_G:1.4122 D(x): 0.2435 D(G(z)): 0.2434 / 0.2443\n[75/100][0/3] Loss_D:1.6927 Loss_G:1.4087 D(x): 0.2442 D(G(z)): 0.2443 / 0.2451\n[75/100][1/3] Loss_D:1.6902 Loss_G:1.4052 D(x): 0.2451 D(G(z)): 0.2451 / 0.2460\n[75/100][2/3] Loss_D:1.6874 Loss_G:1.4017 D(x): 0.2461 D(G(z)): 0.2460 / 0.2469\n[76/100][0/3] Loss_D:1.6856 Loss_G:1.3982 D(x): 0.2468 D(G(z)): 0.2469 / 0.2478\n[76/100][1/3] Loss_D:1.6832 Loss_G:1.3946 D(x): 0.2477 D(G(z)): 0.2478 / 0.2486\n[76/100][2/3] Loss_D:1.6804 Loss_G:1.3911 D(x): 0.2487 D(G(z)): 0.2486 / 0.2495\n[77/100][0/3] Loss_D:1.6785 Loss_G:1.3875 D(x): 0.2495 D(G(z)): 0.2495 / 0.2504\n[77/100][1/3] Loss_D:1.6761 Loss_G:1.3839 D(x): 0.2504 D(G(z)): 0.2504 / 0.2513\n[77/100][2/3] Loss_D:1.6733 Loss_G:1.3803 D(x): 0.2514 D(G(z)): 0.2513 / 0.2523\n[78/100][0/3] Loss_D:1.6713 Loss_G:1.3767 D(x): 0.2523 D(G(z)): 0.2523 / 0.2532\n[78/100][1/3] Loss_D:1.6691 Loss_G:1.3730 D(x): 0.2531 D(G(z)): 0.2532 / 0.2541\n[78/100][2/3] Loss_D:1.6669 Loss_G:1.3694 D(x): 0.2540 D(G(z)): 0.2541 / 0.2551\n[79/100][0/3] Loss_D:1.6643 Loss_G:1.3657 D(x): 0.2550 D(G(z)): 0.2551 / 0.2560\n[79/100][1/3] Loss_D:1.6618 Loss_G:1.3620 D(x): 0.2560 D(G(z)): 0.2560 / 0.2570\n[79/100][2/3] Loss_D:1.6598 Loss_G:1.3583 D(x): 0.2569 D(G(z)): 0.2570 / 0.2579\n[80/100][0/3] Loss_D:1.6572 Loss_G:1.3546 D(x): 0.2578 D(G(z)): 0.2579 / 0.2589\n[80/100][1/3] Loss_D:1.6546 Loss_G:1.3509 D(x): 0.2589 D(G(z)): 0.2589 / 0.2599\n[80/100][2/3] Loss_D:1.6518 Loss_G:1.3472 D(x): 0.2600 D(G(z)): 0.2599 / 0.2608\n[81/100][0/3] Loss_D:1.6500 Loss_G:1.3435 D(x): 0.2608 D(G(z)): 0.2608 / 0.2618\n[81/100][1/3] Loss_D:1.6473 Loss_G:1.3397 D(x): 0.2618 D(G(z)): 0.2618 / 0.2628\n[81/100][2/3] Loss_D:1.6454 Loss_G:1.3360 D(x): 0.2627 D(G(z)): 0.2628 / 0.2638\n[82/100][0/3] Loss_D:1.6427 Loss_G:1.3322 D(x): 0.2638 D(G(z)): 0.2638 / 0.2648\n[82/100][1/3] Loss_D:1.6403 Loss_G:1.3284 D(x): 0.2648 D(G(z)): 0.2648 / 0.2658\n[82/100][2/3] Loss_D:1.6382 Loss_G:1.3246 D(x): 0.2657 D(G(z)): 0.2658 / 0.2669\n[83/100][0/3] Loss_D:1.6355 Loss_G:1.3209 D(x): 0.2668 D(G(z)): 0.2669 / 0.2679\n[83/100][1/3] Loss_D:1.6332 Loss_G:1.3171 D(x): 0.2678 D(G(z)): 0.2679 / 0.2689\n[83/100][2/3] Loss_D:1.6303 Loss_G:1.3133 D(x): 0.2690 D(G(z)): 0.2689 / 0.2699\n[84/100][0/3] Loss_D:1.6287 Loss_G:1.3095 D(x): 0.2698 D(G(z)): 0.2699 / 0.2710\n[84/100][1/3] Loss_D:1.6258 Loss_G:1.3057 D(x): 0.2710 D(G(z)): 0.2710 / 0.2720\n[84/100][2/3] Loss_D:1.6240 Loss_G:1.3019 D(x): 0.2719 D(G(z)): 0.2720 / 0.2731\n[85/100][0/3] Loss_D:1.6214 Loss_G:1.2981 D(x): 0.2730 D(G(z)): 0.2731 / 0.2741\n[85/100][1/3] Loss_D:1.6189 Loss_G:1.2943 D(x): 0.2741 D(G(z)): 0.2741 / 0.2752\n[85/100][2/3] Loss_D:1.6170 Loss_G:1.2904 D(x): 0.2751 D(G(z)): 0.2752 / 0.2762\n[86/100][0/3] Loss_D:1.6142 Loss_G:1.2866 D(x): 0.2762 D(G(z)): 0.2762 / 0.2773\n[86/100][1/3] Loss_D:1.6120 Loss_G:1.2828 D(x): 0.2773 D(G(z)): 0.2773 / 0.2784\n[86/100][2/3] Loss_D:1.6099 Loss_G:1.2789 D(x): 0.2783 D(G(z)): 0.2784 / 0.2795\n[87/100][0/3] Loss_D:1.6073 Loss_G:1.2750 D(x): 0.2794 D(G(z)): 0.2795 / 0.2806\n[87/100][1/3] Loss_D:1.6050 Loss_G:1.2712 D(x): 0.2805 D(G(z)): 0.2806 / 0.2817\n[87/100][2/3] Loss_D:1.6020 Loss_G:1.2673 D(x): 0.2818 D(G(z)): 0.2817 / 0.2828\n[88/100][0/3] Loss_D:1.6003 Loss_G:1.2634 D(x): 0.2828 D(G(z)): 0.2828 / 0.2839\n[88/100][1/3] Loss_D:1.5979 Loss_G:1.2595 D(x): 0.2839 D(G(z)): 0.2839 / 0.2850\n[88/100][2/3] Loss_D:1.5959 Loss_G:1.2556 D(x): 0.2849 D(G(z)): 0.2850 / 0.2862\n[89/100][0/3] Loss_D:1.5934 Loss_G:1.2517 D(x): 0.2861 D(G(z)): 0.2862 / 0.2873\n[89/100][1/3] Loss_D:1.5909 Loss_G:1.2477 D(x): 0.2873 D(G(z)): 0.2873 / 0.2884\n[89/100][2/3] Loss_D:1.5881 Loss_G:1.2438 D(x): 0.2886 D(G(z)): 0.2884 / 0.2896\n[90/100][0/3] Loss_D:1.5865 Loss_G:1.2399 D(x): 0.2895 D(G(z)): 0.2896 / 0.2907\n[90/100][1/3] Loss_D:1.5841 Loss_G:1.2360 D(x): 0.2907 D(G(z)): 0.2907 / 0.2919\n[90/100][2/3] Loss_D:1.5812 Loss_G:1.2320 D(x): 0.2920 D(G(z)): 0.2919 / 0.2931\n[91/100][0/3] Loss_D:1.5795 Loss_G:1.2281 D(x): 0.2930 D(G(z)): 0.2931 / 0.2942\n[91/100][1/3] Loss_D:1.5773 Loss_G:1.2241 D(x): 0.2942 D(G(z)): 0.2942 / 0.2954\n[91/100][2/3] Loss_D:1.5753 Loss_G:1.2202 D(x): 0.2953 D(G(z)): 0.2954 / 0.2966\n[92/100][0/3] Loss_D:1.5729 Loss_G:1.2162 D(x): 0.2965 D(G(z)): 0.2966 / 0.2978\n[92/100][1/3] Loss_D:1.5703 Loss_G:1.2122 D(x): 0.2978 D(G(z)): 0.2978 / 0.2990\n[92/100][2/3] Loss_D:1.5686 Loss_G:1.2083 D(x): 0.2989 D(G(z)): 0.2990 / 0.3002\n[93/100][0/3] Loss_D:1.5660 Loss_G:1.2043 D(x): 0.3002 D(G(z)): 0.3002 / 0.3014\n[93/100][1/3] Loss_D:1.5638 Loss_G:1.2003 D(x): 0.3014 D(G(z)): 0.3014 / 0.3026\n[93/100][2/3] Loss_D:1.5619 Loss_G:1.1963 D(x): 0.3025 D(G(z)): 0.3026 / 0.3038\n[94/100][0/3] Loss_D:1.5593 Loss_G:1.1924 D(x): 0.3038 D(G(z)): 0.3038 / 0.3051\n[94/100][1/3] Loss_D:1.5572 Loss_G:1.1884 D(x): 0.3050 D(G(z)): 0.3051 / 0.3063\n[94/100][2/3] Loss_D:1.5553 Loss_G:1.1844 D(x): 0.3062 D(G(z)): 0.3063 / 0.3075\n[95/100][0/3] Loss_D:1.5527 Loss_G:1.1804 D(x): 0.3075 D(G(z)): 0.3075 / 0.3088\n[95/100][1/3] Loss_D:1.5506 Loss_G:1.1764 D(x): 0.3087 D(G(z)): 0.3088 / 0.3100\n[95/100][2/3] Loss_D:1.5488 Loss_G:1.1724 D(x): 0.3099 D(G(z)): 0.3100 / 0.3113\n[96/100][0/3] Loss_D:1.5463 Loss_G:1.1684 D(x): 0.3112 D(G(z)): 0.3113 / 0.3126\n[96/100][1/3] Loss_D:1.5441 Loss_G:1.1644 D(x): 0.3125 D(G(z)): 0.3126 / 0.3138\n[96/100][2/3] Loss_D:1.5424 Loss_G:1.1604 D(x): 0.3137 D(G(z)): 0.3138 / 0.3151\n[97/100][0/3] Loss_D:1.5399 Loss_G:1.1564 D(x): 0.3150 D(G(z)): 0.3151 / 0.3164\n[97/100][1/3] Loss_D:1.5377 Loss_G:1.1524 D(x): 0.3164 D(G(z)): 0.3164 / 0.3177\n[97/100][2/3] Loss_D:1.5360 Loss_G:1.1484 D(x): 0.3175 D(G(z)): 0.3177 / 0.3190\n[98/100][0/3] Loss_D:1.5335 Loss_G:1.1444 D(x): 0.3189 D(G(z)): 0.3190 / 0.3202\n[98/100][1/3] Loss_D:1.5316 Loss_G:1.1405 D(x): 0.3202 D(G(z)): 0.3202 / 0.3215\n[98/100][2/3] Loss_D:1.5288 Loss_G:1.1365 D(x): 0.3217 D(G(z)): 0.3215 / 0.3229\n[99/100][0/3] Loss_D:1.5273 Loss_G:1.1325 D(x): 0.3228 D(G(z)): 0.3229 / 0.3242\n[99/100][1/3] Loss_D:1.5255 Loss_G:1.1285 D(x): 0.3241 D(G(z)): 0.3242 / 0.3255\n[99/100][2/3] Loss_D:1.5237 Loss_G:1.1245 D(x): 0.3253 D(G(z)): 0.3255 / 0.3268\n\u001b[32m[I 2022-09-11 10:01:32,435]\u001b[0m Trial 0 finished with value: 0.6215788722038269 and parameters: {'lr': 0.004599396160155632, 'dropoutG': 0.2, 'dropoutD': 0.0, 'optimizer_name': 'Adadelta'}. Best is trial 0 with value: 0.6215788722038269.\u001b[0m\n[0/100][0/3] Loss_D:2.0808 Loss_G:1.9164 D(x): 0.1466 D(G(z)): 0.1465 / 0.1475\n[0/100][1/3] Loss_D:2.0756 Loss_G:1.9120 D(x): 0.1475 D(G(z)): 0.1474 / 0.1481\n[0/100][2/3] Loss_D:2.0735 Loss_G:1.9084 D(x): 0.1480 D(G(z)): 0.1486 / 0.1486\n[1/100][0/3] Loss_D:2.0685 Loss_G:1.9021 D(x): 0.1488 D(G(z)): 0.1489 / 0.1496\n[1/100][1/3] Loss_D:2.0643 Loss_G:1.8953 D(x): 0.1496 D(G(z)): 0.1495 / 0.1506\n[1/100][2/3] Loss_D:2.0599 Loss_G:1.8931 D(x): 0.1503 D(G(z)): 0.1501 / 0.1509\n[2/100][0/3] Loss_D:2.0552 Loss_G:1.8876 D(x): 0.1512 D(G(z)): 0.1511 / 0.1518\n[2/100][1/3] Loss_D:2.0512 Loss_G:1.8818 D(x): 0.1520 D(G(z)): 0.1519 / 0.1527\n[2/100][2/3] Loss_D:2.0452 Loss_G:1.8772\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n884 D(x): 0.4674 D(G(z)): 0.4582 / 0.4670\n[98/100][0/3] Loss_D:1.4434 Loss_G:0.8006 D(x): 0.4599 D(G(z)): 0.4628 / 0.4616\n[98/100][1/3] Loss_D:1.4344 Loss_G:0.7977 D(x): 0.4625 D(G(z)): 0.4613 / 0.4630\n[98/100][2/3] Loss_D:1.4252 Loss_G:0.8263 D(x): 0.4615 D(G(z)): 0.4563 / 0.4503\n[99/100][0/3] Loss_D:1.4375 Loss_G:0.7968 D(x): 0.4639 D(G(z)): 0.4645 / 0.4636\n[99/100][1/3] Loss_D:1.4350 Loss_G:0.7867 D(x): 0.4652 D(G(z)): 0.4646 / 0.4680\n[99/100][2/3] Loss_D:1.4680 Loss_G:0.7643 D(x): 0.4647 D(G(z)): 0.4793 / 0.4778\n\u001b[32m[I 2022-09-11 10:52:34,247]\u001b[0m Trial 8 finished with value: 0.6019051671028137 and parameters: {'lr': 0.005743848041527131, 'dropoutG': 0.1, 'dropoutD': 0.4, 'optimizer_name': 'Adadelta'}. Best is trial 1 with value: 0.5526883006095886.\u001b[0m\n[0/100][0/3] Loss_D:2.0808 Loss_G:1.8852 D(x): 0.1466 D(G(z)): 0.1465 / 0.1522\n[0/100][1/3] Loss_D:2.0499 Loss_G:1.8489 D(x): 0.1522 D(G(z)): 0.1521 / 0.1578\n[0/100][2/3] Loss_D:2.0224 Loss_G:1.8133 D(x): 0.1577 D(G(z)): 0.1583 / 0.1636\n[1/100][0/3] Loss_D:1.9914 Loss_G:1.7750 D(x): 0.1637 D(G(z)): 0.1638 / 0.1700\n[1/100][1/3] Loss_D:1.9611 Loss_G:1.7341 D(x): 0.1701 D(G(z)): 0.1699 / 0.1772\n[1/100][2/3] Loss_D:1.9316 Loss_G:1.6980 D(x): 0.1766 D(G(z)): 0.1766 / 0.1837\n[2/100][0/3] Loss_D:1.8999 Loss_G:1.6591 D(x): 0.1840 D(G(z)): 0.1838 / 0.1911\n[2/100][1/3] Loss_D:1.8693 Loss_G:1.6169 D(x): 0.1916 D(G(z)): 0.1913 / 0.1995\n[2/100][2/3] Loss_D:1.8383 Loss_G:1.5780 D(x): 0.1994 D(G(z)): 0.1983 / 0.2075\n[3/100][0/3] Loss_D:1.8091 Loss_G:1.5316 D(x): 0.2080 D(G(z)): 0.2078 / 0.2175\n[3/100][1/3] Loss_D:1.7762 Loss_G:1.4899 D(x): 0.2175 D(G(z)): 0.2169 / 0.2269\n[3/100][2/3] Loss_D:1.7440 Loss_G:1.4498 D(x): 0.2274 D(G(z)): 0.2257 / 0.2362\n[4/100][0/3] Loss_D:1.7152 Loss_G:1.3950 D(x): 0.2380 D(G(z)): 0.2378 / 0.2499\n[4/100][1/3] Loss_D:1.6823 Loss_G:1.3466 D(x): 0.2497 D(G(z)): 0.2487 / 0.2625\n[4/100][2/3] Loss_D:1.6476 Loss_G:1.3019 D(x): 0.2629 D(G(z)): 0.2602 / 0.2747\n[5/100][0/3] Loss_D:1.6252 Loss_G:1.2521 D(x): 0.2745 D(G(z)): 0.2749 / 0.2889\n[5/100][1/3] Loss_D:1.5953 Loss_G:1.2025 D(x): 0.2891 D(G(z)): 0.2896 / 0.3040\n[5/100][2/3] Loss_D:1.5658 Loss_G:1.1337 D(x): 0.3034 D(G(z)): 0.3025 / 0.3259\n[6/100][0/3] Loss_D:1.5388 Loss_G:1.0974 D(x): 0.3217 D(G(z)): 0.3224 / 0.3383\n[6/100][1/3] Loss_D:1.5107 Loss_G:1.0386 D(x): 0.3395 D(G(z)): 0.3388 / 0.3592\n[6/100][2/3] Loss_D:1.4807 Loss_G:0.9837 D(x): 0.3590 D(G(z)): 0.3548 / 0.3799\n[7/100][0/3] Loss_D:1.4681 Loss_G:0.9290 D(x): 0.3798 D(G(z)): 0.3805 / 0.4019\n[7/100][1/3] Loss_D:1.4466 Loss_G:0.8774 D(x): 0.4026 D(G(z)): 0.4018 / 0.4234\n[7/100][2/3] Loss_D:1.4177 Loss_G:0.8175 D(x): 0.4262 D(G(z)): 0.4170 / 0.4505\n[8/100][0/3] Loss_D:1.4201 Loss_G:0.7708 D(x): 0.4487 D(G(z)): 0.4456 / 0.4718\n[8/100][1/3] Loss_D:1.4205 Loss_G:0.7237 D(x): 0.4712 D(G(z)): 0.4704 / 0.4950\n[8/100][2/3] Loss_D:1.4118 Loss_G:0.6806 D(x): 0.4983 D(G(z)): 0.4933 / 0.5172\n[9/100][0/3] Loss_D:1.4250 Loss_G:0.6406 D(x): 0.5145 D(G(z)): 0.5137 / 0.5384\n[9/100][1/3] Loss_D:1.4335 Loss_G:0.6268 D(x): 0.5319 D(G(z)): 0.5322 / 0.5463\n[9/100][2/3] Loss_D:1.4531 Loss_G:0.6112 D(x): 0.5446 D(G(z)): 0.5503 / 0.5544\n[10/100][0/3] Loss_D:1.4432 Loss_G:0.5966 D(x): 0.5603 D(G(z)): 0.5579 / 0.5631\n[10/100][1/3] Loss_D:1.4495 Loss_G:0.5928 D(x): 0.5625 D(G(z)): 0.5622 / 0.5650\n[10/100][2/3] Loss_D:1.4789 Loss_G:0.6143 D(x): 0.5524 D(G(z)): 0.5665 / 0.5532\n[11/100][0/3] Loss_D:1.4493 Loss_G:0.6103 D(x): 0.5593 D(G(z)): 0.5600 / 0.5551\n[11/100][1/3] Loss_D:1.4516 Loss_G:0.6280 D(x): 0.5534 D(G(z)): 0.5567 / 0.5452\n[11/100][2/3] Loss_D:1.4382 Loss_G:0.6390 D(x): 0.5494 D(G(z)): 0.5487 / 0.5388\n[12/100][0/3] Loss_D:1.4394 Loss_G:0.6603 D(x): 0.5341 D(G(z)): 0.5374 / 0.5274\n[12/100][1/3] Loss_D:1.4295 Loss_G:0.6820 D(x): 0.5251 D(G(z)): 0.5257 / 0.5159\n[12/100][2/3] Loss_D:1.4131 Loss_G:0.6913 D(x): 0.5162 D(G(z)): 0.5100 / 0.5105\n[13/100][0/3] Loss_D:1.4251 Loss_G:0.7212 D(x): 0.5037 D(G(z)): 0.5052 / 0.4954\n[13/100][1/3] Loss_D:1.4155 Loss_G:0.7389 D(x): 0.4964 D(G(z)): 0.4942 / 0.4866\n[13/100][2/3] Loss_D:1.4042 Loss_G:0.7671 D(x): 0.4941 D(G(z)): 0.4861 / 0.4724\n[14/100][0/3] Loss_D:1.4148 Loss_G:0.7710 D(x): 0.4781 D(G(z)): 0.4760 / 0.4710\n[14/100][1/3] Loss_D:1.4190 Loss_G:0.7811 D(x): 0.4717 D(G(z)): 0.4717 / 0.4661\n[14/100][2/3] Loss_D:1.4223 Loss_G:0.8009 D(x): 0.4592 D(G(z)): 0.4601 / 0.4570\n[15/100][0/3] Loss_D:1.4203 Loss_G:0.8001 D(x): 0.4615 D(G(z)): 0.4617 / 0.4571\n[15/100][1/3] Loss_D:1.4245 Loss_G:0.8088 D(x): 0.4553 D(G(z)): 0.4572 / 0.4530\n[15/100][2/3] Loss_D:1.4398 Loss_G:0.8121 D(x): 0.4504 D(G(z)): 0.4598 / 0.4516\n[16/100][0/3] Loss_D:1.4242 Loss_G:0.8129 D(x): 0.4512 D(G(z)): 0.4524 / 0.4511\n[16/100][1/3] Loss_D:1.4208 Loss_G:0.8083 D(x): 0.4519 D(G(z)): 0.4514 / 0.4530\n[16/100][2/3] Loss_D:1.4205 Loss_G:0.8162 D(x): 0.4475 D(G(z)): 0.4467 / 0.4497\n[17/100][0/3] Loss_D:1.4209 Loss_G:0.8087 D(x): 0.4531 D(G(z)): 0.4530 / 0.4529\n[17/100][1/3] Loss_D:1.4178 Loss_G:0.8020 D(x): 0.4552 D(G(z)): 0.4539 / 0.4558\n[17/100][2/3] Loss_D:1.4095 Loss_G:0.8029 D(x): 0.4604 D(G(z)): 0.4554 / 0.4555\n[18/100][0/3] Loss_D:1.4198 Loss_G:0.7885 D(x): 0.4584 D(G(z)): 0.4587 / 0.4622\n[18/100][1/3] Loss_D:1.4229 Loss_G:0.7885 D(x): 0.4597 D(G(z)): 0.4615 / 0.4622\n[18/100][2/3] Loss_D:1.4202 Loss_G:0.7705 D(x): 0.4656 D(G(z)): 0.4671 / 0.4711\n[19/100][0/3] Loss_D:1.4234 Loss_G:0.7692 D(x): 0.4649 D(G(z)): 0.4675 / 0.4712\n[19/100][1/3] Loss_D:1.4198 Loss_G:0.7620 D(x): 0.4715 D(G(z)): 0.4731 / 0.4746\n[19/100][2/3] Loss_D:1.4151 Loss_G:0.7435 D(x): 0.4721 D(G(z)): 0.4710 / 0.4833\n[20/100][0/3] Loss_D:1.4213 Loss_G:0.7435 D(x): 0.4775 D(G(z)): 0.4798 / 0.4835\n[20/100][1/3] Loss_D:1.4174 Loss_G:0.7391 D(x): 0.4801 D(G(z)): 0.4805 / 0.4858\n[20/100][2/3] Loss_D:1.4285 Loss_G:0.7283 D(x): 0.4810 D(G(z)): 0.4869 / 0.4911\n[21/100][0/3] Loss_D:1.4140 Loss_G:0.7334 D(x): 0.4862 D(G(z)): 0.4850 / 0.4883\n[21/100][1/3] Loss_D:1.4186 Loss_G:0.7306 D(x): 0.4889 D(G(z)): 0.4902 / 0.4898\n[21/100][2/3] Loss_D:1.4351 Loss_G:0.7222 D(x): 0.4859 D(G(z)): 0.4948 / 0.4940\n[22/100][0/3] Loss_D:1.4181 Loss_G:0.7267 D(x): 0.4910 D(G(z)): 0.4920 / 0.4918\n[22/100][1/3] Loss_D:1.4126 Loss_G:0.7254 D(x): 0.4932 D(G(z)): 0.4916 / 0.4923\n[22/100][2/3] Loss_D:1.4203 Loss_G:0.7375 D(x): 0.4879 D(G(z)): 0.4905 / 0.4863\n[23/100][0/3] Loss_D:1.4182 Loss_G:0.7276 D(x): 0.4914 D(G(z)): 0.4926 / 0.4911\n[23/100][1/3] Loss_D:1.4101 Loss_G:0.7274 D(x): 0.4933 D(G(z)): 0.4906 / 0.4910\n[23/100][2/3] Loss_D:1.3944 Loss_G:0.7326 D(x): 0.4954 D(G(z)): 0.4850 / 0.4887\n[24/100][0/3] Loss_D:1.4163 Loss_G:0.7348 D(x): 0.4883 D(G(z)): 0.4886 / 0.4874\n[24/100][1/3] Loss_D:1.4137 Loss_G:0.7334 D(x): 0.4914 D(G(z)): 0.4908 / 0.4882\n[24/100][2/3] Loss_D:1.3939 Loss_G:0.7300 D(x): 0.4956 D(G(z)): 0.4860 / 0.4894\n[25/100][0/3] Loss_D:1.4160 Loss_G:0.7401 D(x): 0.4885 D(G(z)): 0.4890 / 0.4848\n[25/100][1/3] Loss_D:1.4070 Loss_G:0.7340 D(x): 0.4882 D(G(z)): 0.4844 / 0.4877\n[25/100][2/3] Loss_D:1.4281 Loss_G:0.7328 D(x): 0.4813 D(G(z)): 0.4882 / 0.4881\n[26/100][0/3] Loss_D:1.4054 Loss_G:0.7391 D(x): 0.4874 D(G(z)): 0.4831 / 0.4851\n[26/100][1/3] Loss_D:1.4086 Loss_G:0.7481 D(x): 0.4861 D(G(z)): 0.4834 / 0.4808\n[26/100][2/3] Loss_D:1.4073 Loss_G:0.7512 D(x): 0.4842 D(G(z)): 0.4811 / 0.4790\n[27/100][0/3] Loss_D:1.4066 Loss_G:0.7424 D(x): 0.4841 D(G(z)): 0.4806 / 0.4834\n[27/100][1/3] Loss_D:1.4106 Loss_G:0.7466 D(x): 0.4847 D(G(z)): 0.4829 / 0.4815\n[27/100][2/3] Loss_D:1.4103 Loss_G:0.7457 D(x): 0.4877 D(G(z)): 0.4869 / 0.4818\n[28/100][0/3] Loss_D:1.4010 Loss_G:0.7483 D(x): 0.4845 D(G(z)): 0.4781 / 0.4804\n[28/100][1/3] Loss_D:1.4129 Loss_G:0.7494 D(x): 0.4808 D(G(z)): 0.4805 / 0.4799\n[28/100][2/3] Loss_D:1.3821 Loss_G:0.7512 D(x): 0.4912 D(G(z)): 0.4757 / 0.4791\n[29/100][0/3] Loss_D:1.4019 Loss_G:0.7512 D(x): 0.4853 D(G(z)): 0.4796 / 0.4791\n[29/100][1/3] Loss_D:1.4025 Loss_G:0.7476 D(x): 0.4857 D(G(z)): 0.4803 / 0.4805\n[29/100][2/3] Loss_D:1.4098 Loss_G:0.7623 D(x): 0.4853 D(G(z)): 0.4838 / 0.4736\n[30/100][0/3] Loss_D:1.3785 Loss_G:0.7419 D(x): 0.4975 D(G(z)): 0.4805 / 0.4835\n[30/100][1/3] Loss_D:1.4076 Loss_G:0.7432 D(x): 0.4871 D(G(z)): 0.4843 / 0.4828\n[30/100][2/3] Loss_D:1.4151 Loss_G:0.7242 D(x): 0.4841 D(G(z)): 0.4849 / 0.4920\n[31/100][0/3] Loss_D:1.3897 Loss_G:0.7364 D(x): 0.4966 D(G(z)): 0.4848 / 0.4859\n[31/100][1/3] Loss_D:1.3698 Loss_G:0.7390 D(x): 0.5074 D(G(z)): 0.4856 / 0.4847\n[31/100][2/3] Loss_D:1.3500 Loss_G:0.7215 D(x): 0.5241 D(G(z)): 0.4920 / 0.4929\n[32/100][0/3] Loss_D:1.3610 Loss_G:0.7300 D(x): 0.5116 D(G(z)): 0.4854 / 0.4890\n[32/100][1/3] Loss_D:1.3821 Loss_G:0.7327 D(x): 0.5029 D(G(z)): 0.4873 / 0.4875\n[32/100][2/3] Loss_D:1.3263 Loss_G:0.7356 D(x): 0.5371 D(G(z)): 0.4925 / 0.4863\n[33/100][0/3] Loss_D:1.3580 Loss_G:0.7304 D(x): 0.5143 D(G(z)): 0.4865 / 0.4885\n[33/100][1/3] Loss_D:1.3381 Loss_G:0.6910 D(x): 0.5258 D(G(z)): 0.4874 / 0.5093\n[33/100][2/3] Loss_D:1.4423 Loss_G:0.6347 D(x): 0.5295 D(G(z)): 0.5373 / 0.5385\n[34/100][0/3] Loss_D:1.4311 Loss_G:0.6426 D(x): 0.5351 D(G(z)): 0.5382 / 0.5345\n[34/100][1/3] Loss_D:1.4144 Loss_G:0.6542 D(x): 0.5367 D(G(z)): 0.5327 / 0.5280\n[34/100][2/3] Loss_D:1.4199 Loss_G:0.6607 D(x): 0.5290 D(G(z)): 0.5283 / 0.5240\n[35/100][0/3] Loss_D:1.4157 Loss_G:0.6816 D(x): 0.5234 D(G(z)): 0.5226 / 0.5133\n[35/100][1/3] Loss_D:1.4125 Loss_G:0.7003 D(x): 0.5140 D(G(z)): 0.5128 / 0.5037\n[35/100][2/3] Loss_D:1.4071 Loss_G:0.7112 D(x): 0.5084 D(G(z)): 0.5054 / 0.4978\n[36/100][0/3] Loss_D:1.4134 Loss_G:0.7312 D(x): 0.4979 D(G(z)): 0.4987 / 0.4882\n[36/100][1/3] Loss_D:1.4149 Loss_G:0.7490 D(x): 0.4883 D(G(z)): 0.4901 / 0.4793\n[36/100][2/3] Loss_D:1.3978 Loss_G:0.7589 D(x): 0.4868 D(G(z)): 0.4809 / 0.4745\n[37/100][0/3] Loss_D:1.4119 Loss_G:0.7704 D(x): 0.4745 D(G(z)): 0.4749 / 0.4689\n[37/100][1/3] Loss_D:1.4126 Loss_G:0.7720 D(x): 0.4696 D(G(z)): 0.4701 / 0.4681\n[37/100][2/3] Loss_D:1.4013 Loss_G:0.7792 D(x): 0.4648 D(G(z)): 0.4585 / 0.4649\n[38/100][0/3] Loss_D:1.4191 Loss_G:0.7869 D(x): 0.4603 D(G(z)): 0.4632 / 0.4610\n[38/100][1/3] Loss_D:1.4174 Loss_G:0.7863 D(x): 0.4607 D(G(z)): 0.4630 / 0.4613\n[38/100][2/3] Loss_D:1.4289 Loss_G:0.7739 D(x): 0.4578 D(G(z)): 0.4656 / 0.4676\n[39/100][0/3] Loss_D:1.4123 Loss_G:0.7853 D(x): 0.4606 D(G(z)): 0.4604 / 0.4617\n[39/100][1/3] Loss_D:1.4149 Loss_G:0.7772 D(x): 0.4619 D(G(z)): 0.4631 / 0.4655\n[39/100][2/3] Loss_D:1.4308 Loss_G:0.7705 D(x): 0.4603 D(G(z)): 0.4696 / 0.4685\n[40/100][0/3] Loss_D:1.4127 Loss_G:0.7651 D(x): 0.4659 D(G(z)): 0.4666 / 0.4712\n[40/100][1/3] Loss_D:1.4081 Loss_G:0.7557 D(x): 0.4708 D(G(z)): 0.4696 / 0.4756\n[40/100][2/3] Loss_D:1.4100 Loss_G:0.7532 D(x): 0.4752 D(G(z)): 0.4751 / 0.4768\n[41/100][0/3] Loss_D:1.4074 Loss_G:0.7379 D(x): 0.4789 D(G(z)): 0.4780 / 0.4842\n[41/100][1/3] Loss_D:1.4095 Loss_G:0.7328 D(x): 0.4835 D(G(z)): 0.4839 / 0.4867\n[41/100][2/3] Loss_D:1.4324 Loss_G:0.7267 D(x): 0.4813 D(G(z)): 0.4928 / 0.4892\n[42/100][0/3] Loss_D:1.4037 Loss_G:0.7166 D(x): 0.4884 D(G(z)): 0.4860 / 0.4946\n[42/100][1/3] Loss_D:1.4026 Loss_G:0.7176 D(x): 0.4935 D(G(z)): 0.4904 / 0.4942\n[42/100][2/3] Loss_D:1.4081 Loss_G:0.7106 D(x): 0.4963 D(G(z)): 0.4956 / 0.4969\n[43/100][0/3] Loss_D:1.4109 Loss_G:0.7092 D(x): 0.4957 D(G(z)): 0.4966 / 0.4984\n[43/100][1/3] Loss_D:1.4085 Loss_G:0.7101 D(x): 0.4991 D(G(z)): 0.4988 / 0.4977\n[43/100][2/3] Loss_D:1.4025 Loss_G:0.7205 D(x): 0.5028 D(G(z)): 0.4992 / 0.4929\n[44/100][0/3] Loss_D:1.4149 Loss_G:0.7116 D(x): 0.4962 D(G(z)): 0.4991 / 0.4969\n[44/100][1/3] Loss_D:1.4115 Loss_G:0.7160 D(x): 0.4947 D(G(z)): 0.4961 / 0.4948\n[44/100][2/3] Loss_D:1.3929 Loss_G:0.7231 D(x): 0.5015 D(G(z)): 0.4935 / 0.4909\n[45/100][0/3] Loss_D:1.4049 Loss_G:0.7249 D(x): 0.4934 D(G(z)): 0.4918 / 0.4903\n[45/100][1/3] Loss_D:1.4151 Loss_G:0.7312 D(x): 0.4891 D(G(z)): 0.4926 / 0.4871\n[45/100][2/3] Loss_D:1.4059 Loss_G:0.7366 D(x): 0.4883 D(G(z)): 0.4867 / 0.4846\n[46/100][0/3] Loss_D:1.4043 Loss_G:0.7377 D(x): 0.4876 D(G(z)): 0.4859 / 0.4839\n[46/100][1/3] Loss_D:1.4089 Loss_G:0.7367 D(x): 0.4860 D(G(z)): 0.4865 / 0.4844\n[46/100][2/3] Loss_D:1.4006 Loss_G:0.7343 D(x): 0.4878 D(G(z)): 0.4839 / 0.4854\n[47/100][0/3] Loss_D:1.4044 Loss_G:0.7423 D(x): 0.4855 D(G(z)): 0.4842 / 0.4816\n[47/100][1/3] Loss_D:1.4078 Loss_G:0.7414 D(x): 0.4815 D(G(z)): 0.4817 / 0.4820\n[47/100][2/3] Loss_D:1.3861 Loss_G:0.7507 D(x): 0.4837 D(G(z)): 0.4731 / 0.4774\n[48/100][0/3] Loss_D:1.4031 Loss_G:0.7386 D(x): 0.4831 D(G(z)): 0.4812 / 0.4832\n[48/100][1/3] Loss_D:1.4058 Loss_G:0.7373 D(x): 0.4843 D(G(z)): 0.4836 / 0.4838\n[48/100][2/3] Loss_D:1.3989 Loss_G:0.7425 D(x): 0.4882 D(G(z)): 0.4839 / 0.4814\n[49/100][0/3] Loss_D:1.4104 Loss_G:0.7310 D(x): 0.4839 D(G(z)): 0.4855 / 0.4870\n[49/100][1/3] Loss_D:1.4042 Loss_G:0.7341 D(x): 0.4889 D(G(z)): 0.4874 / 0.4854\n[49/100][2/3] Loss_D:1.4103 Loss_G:0.7487 D(x): 0.4819 D(G(z)): 0.4842 / 0.4782\n[50/100][0/3] Loss_D:1.4091 Loss_G:0.7246 D(x): 0.4880 D(G(z)): 0.4893 / 0.4900\n[50/100][1/3] Loss_D:1.4116 Loss_G:0.7233 D(x): 0.4876 D(G(z)): 0.4902 / 0.4907\n[50/100][2/3] Loss_D:1.4141 Loss_G:0.7244 D(x): 0.4872 D(G(z)): 0.4912 / 0.4899\n[51/100][0/3] Loss_D:1.4004 Loss_G:0.7231 D(x): 0.4908 D(G(z)): 0.4877 / 0.4906\n[51/100][1/3] Loss_D:1.4063 Loss_G:0.7279 D(x): 0.4907 D(G(z)): 0.4907 / 0.4881\n[51/100][2/3] Loss_D:1.4056 Loss_G:0.7266 D(x): 0.4881 D(G(z)): 0.4878 / 0.4887\n[52/100][0/3] Loss_D:1.4021 Loss_G:0.7242 D(x): 0.4919 D(G(z)): 0.4901 / 0.4900\n[52/100][1/3] Loss_D:1.4115 Loss_G:0.7259 D(x): 0.4880 D(G(z)): 0.4908 / 0.4891\n[52/100][2/3] Loss_D:1.4162 Loss_G:0.7441 D(x): 0.4916 D(G(z)): 0.4966 / 0.4802\n[53/100][0/3] Loss_D:1.4120 Loss_G:0.7288 D(x): 0.4877 D(G(z)): 0.4910 / 0.4875\n[53/100][1/3] Loss_D:1.4077 Loss_G:0.7276 D(x): 0.4884 D(G(z)): 0.4895 / 0.4883\n[53/100][2/3] Loss_D:1.3958 Loss_G:0.7303 D(x): 0.4918 D(G(z)): 0.4875 / 0.4864\n[54/100][0/3] Loss_D:1.4029 Loss_G:0.7293 D(x): 0.4877 D(G(z)): 0.4866 / 0.4873\n[54/100][1/3] Loss_D:1.4035 Loss_G:0.7329 D(x): 0.4882 D(G(z)): 0.4875 / 0.4854\n[54/100][2/3] Loss_D:1.3985 Loss_G:0.7422 D(x): 0.4864 D(G(z)): 0.4836 / 0.4809\n[55/100][0/3] Loss_D:1.4097 Loss_G:0.7252 D(x): 0.4849 D(G(z)): 0.4873 / 0.4892\n[55/100][1/3] Loss_D:1.4069 Loss_G:0.7257 D(x): 0.4868 D(G(z)): 0.4880 / 0.4889\n[55/100][2/3] Loss_D:1.4148 Loss_G:0.7127 D(x): 0.4863 D(G(z)): 0.4909 / 0.4951\n[56/100][0/3] Loss_D:1.4069 Loss_G:0.7248 D(x): 0.4884 D(G(z)): 0.4894 / 0.4893\n[56/100][1/3] Loss_D:1.4099 Loss_G:0.7286 D(x): 0.4863 D(G(z)): 0.4889 / 0.4874\n[56/100][2/3] Loss_D:1.4111 Loss_G:0.7242 D(x): 0.4864 D(G(z)): 0.4895 / 0.4894\n[57/100][0/3] Loss_D:1.4056 Loss_G:0.7248 D(x): 0.4881 D(G(z)): 0.4888 / 0.4893\n[57/100][1/3] Loss_D:1.4038 Loss_G:0.7290 D(x): 0.4906 D(G(z)): 0.4903 / 0.4871\n[57/100][2/3] Loss_D:1.4181 Loss_G:0.7266 D(x): 0.4844 D(G(z)): 0.4913 / 0.4881\n[58/100][0/3] Loss_D:1.4046 Loss_G:0.7274 D(x): 0.4885 D(G(z)): 0.4887 / 0.4878\n[58/100][1/3] Loss_D:1.4004 Loss_G:0.7250 D(x): 0.4895 D(G(z)): 0.4877 / 0.4890\n[58/100][2/3] Loss_D:1.4059 Loss_G:0.7251 D(x): 0.4877 D(G(z)): 0.4888 / 0.4890\n[59/100][0/3] Loss_D:1.4022 Loss_G:0.7235 D(x): 0.4907 D(G(z)): 0.4901 / 0.4896\n[59/100][1/3] Loss_D:1.3996 Loss_G:0.7184 D(x): 0.4917 D(G(z)): 0.4900 / 0.4921\n[59/100][2/3] Loss_D:1.3887 Loss_G:0.7181 D(x): 0.4918 D(G(z)): 0.4846 / 0.4924\n[60/100][0/3] Loss_D:1.4017 Loss_G:0.7175 D(x): 0.4919 D(G(z)): 0.4913 / 0.4925\n[60/100][1/3] Loss_D:1.4042 Loss_G:0.7171 D(x): 0.4911 D(G(z)): 0.4918 / 0.4927\n[60/100][2/3] Loss_D:1.4303 Loss_G:0.7113 D(x): 0.4853 D(G(z)): 0.4985 / 0.4953\n[61/100][0/3] Loss_D:1.4046 Loss_G:0.7176 D(x): 0.4922 D(G(z)): 0.4930 / 0.4923\n[61/100][1/3] Loss_D:1.4026 Loss_G:0.7241 D(x): 0.4918 D(G(z)): 0.4918 / 0.4891\n[61/100][2/3] Loss_D:1.3870 Loss_G:0.7211 D(x): 0.4967 D(G(z)): 0.4894 / 0.4905\n[62/100][0/3] Loss_D:1.3976 Loss_G:0.7190 D(x): 0.4910 D(G(z)): 0.4886 / 0.4916\n[62/100][1/3] Loss_D:1.4021 Loss_G:0.7194 D(x): 0.4909 D(G(z)): 0.4907 / 0.4914\n[62/100][2/3] Loss_D:1.4164 Loss_G:0.7205 D(x): 0.4918 D(G(z)): 0.4984 / 0.4906\n[63/100][0/3] Loss_D:1.4010 Loss_G:0.7233 D(x): 0.4901 D(G(z)): 0.4895 / 0.4894\n[63/100][1/3] Loss_D:1.3997 Loss_G:0.7199 D(x): 0.4924 D(G(z)): 0.4912 / 0.4909\n[63/100][2/3] Loss_D:1.3993 Loss_G:0.7066 D(x): 0.4860 D(G(z)): 0.4846 / 0.4976\n[64/100][0/3] Loss_D:1.3993 Loss_G:0.7209 D(x): 0.4911 D(G(z)): 0.4899 / 0.4904\n[64/100][1/3] Loss_D:1.3987 Loss_G:0.7204 D(x): 0.4914 D(G(z)): 0.4899 / 0.4907\n[64/100][2/3] Loss_D:1.3962 Loss_G:0.7192 D(x): 0.4928 D(G(z)): 0.4899 / 0.4908\n[65/100][0/3] Loss_D:1.3967 Loss_G:0.7179 D(x): 0.4916 D(G(z)): 0.4891 / 0.4919\n[65/100][1/3] Loss_D:1.4044 Loss_G:0.7120 D(x): 0.4921 D(G(z)): 0.4935 / 0.4948\n[65/100][2/3] Loss_D:1.4038 Loss_G:0.7084 D(x): 0.4919 D(G(z)): 0.4933 / 0.4967\n[66/100][0/3] Loss_D:1.3992 Loss_G:0.7180 D(x): 0.4931 D(G(z)): 0.4918 / 0.4918\n[66/100][1/3] Loss_D:1.4007 Loss_G:0.7134 D(x): 0.4942 D(G(z)): 0.4938 / 0.4940\n[66/100][2/3] Loss_D:1.3988 Loss_G:0.7234 D(x): 0.4909 D(G(z)): 0.4897 / 0.4892\n[67/100][0/3] Loss_D:1.4062 Loss_G:0.7211 D(x): 0.4908 D(G(z)): 0.4933 / 0.4901\n[67/100][1/3] Loss_D:1.3975 Loss_G:0.7209 D(x): 0.4923 D(G(z)): 0.4905 / 0.4903\n[67/100][2/3] Loss_D:1.3929 Loss_G:0.7315 D(x): 0.4931 D(G(z)): 0.4889 / 0.4850\n[68/100][0/3] Loss_D:1.4028 Loss_G:0.7177 D(x): 0.4897 D(G(z)): 0.4905 / 0.4917\n[68/100][1/3] Loss_D:1.4028 Loss_G:0.7262 D(x): 0.4891 D(G(z)): 0.4899 / 0.4877\n[68/100][2/3] Loss_D:1.3977 Loss_G:0.7291 D(x): 0.4898 D(G(z)): 0.4880 / 0.4859\n[69/100][0/3] Loss_D:1.3979 Loss_G:0.7200 D(x): 0.4892 D(G(z)): 0.4877 / 0.4906\n[69/100][1/3] Loss_D:1.4006 Loss_G:0.7166 D(x): 0.4906 D(G(z)): 0.4906 / 0.4922\n[69/100][2/3] Loss_D:1.3955 Loss_G:0.7293 D(x): 0.4966 D(G(z)): 0.4938 / 0.4859\n[70/100][0/3] Loss_D:1.4035 Loss_G:0.7171 D(x): 0.4913 D(G(z)): 0.4927 / 0.4919\n[70/100][1/3] Loss_D:1.4008 Loss_G:0.7168 D(x): 0.4925 D(G(z)): 0.4926 / 0.4921\n[70/100][2/3] Loss_D:1.3979 Loss_G:0.7263 D(x): 0.4963 D(G(z)): 0.4942 / 0.4874\n[71/100][0/3] Loss_D:1.3991 Loss_G:0.7165 D(x): 0.4938 D(G(z)): 0.4932 / 0.4922\n[71/100][1/3] Loss_D:1.4033 Loss_G:0.7170 D(x): 0.4923 D(G(z)): 0.4938 / 0.4920\n[71/100][2/3] Loss_D:1.3905 Loss_G:0.7174 D(x): 0.4983 D(G(z)): 0.4934 / 0.4919\n[72/100][0/3] Loss_D:1.3977 Loss_G:0.7190 D(x): 0.4921 D(G(z)): 0.4908 / 0.4909\n[72/100][1/3] Loss_D:1.3956 Loss_G:0.7186 D(x): 0.4926 D(G(z)): 0.4904 / 0.4911\n[72/100][2/3] Loss_D:1.3931 Loss_G:0.7166 D(x): 0.4935 D(G(z)): 0.4901 / 0.4921\n[73/100][0/3] Loss_D:1.3971 Loss_G:0.7156 D(x): 0.4938 D(G(z)): 0.4924 / 0.4926\n[73/100][1/3] Loss_D:1.4030 Loss_G:0.7167 D(x): 0.4900 D(G(z)): 0.4915 / 0.4920\n[73/100][2/3] Loss_D:1.3962 Loss_G:0.7122 D(x): 0.4931 D(G(z)): 0.4915 / 0.4943\n[74/100][0/3] Loss_D:1.3968 Loss_G:0.7124 D(x): 0.4928 D(G(z)): 0.4912 / 0.4940\n[74/100][1/3] Loss_D:1.4018 Loss_G:0.7148 D(x): 0.4910 D(G(z)): 0.4922 / 0.4929\n[74/100][2/3] Loss_D:1.4073 Loss_G:0.7164 D(x): 0.4882 D(G(z)): 0.4921 / 0.4922\n[75/100][0/3] Loss_D:1.3998 Loss_G:0.7144 D(x): 0.4933 D(G(z)): 0.4933 / 0.4930\n[75/100][1/3] Loss_D:1.4003 Loss_G:0.7126 D(x): 0.4936 D(G(z)): 0.4939 / 0.4940\n[75/100][2/3] Loss_D:1.4162 Loss_G:0.7173 D(x): 0.4940 D(G(z)): 0.5021 / 0.4916\n[76/100][0/3] Loss_D:1.4004 Loss_G:0.7186 D(x): 0.4929 D(G(z)): 0.4934 / 0.4909\n[76/100][1/3] Loss_D:1.3970 Loss_G:0.7182 D(x): 0.4915 D(G(z)): 0.4904 / 0.4912\n[76/100][2/3] Loss_D:1.4011 Loss_G:0.7226 D(x): 0.4902 D(G(z)): 0.4909 / 0.4886\n[77/100][0/3] Loss_D:1.3981 Loss_G:0.7232 D(x): 0.4893 D(G(z)): 0.4886 / 0.4886\n[77/100][1/3] Loss_D:1.4011 Loss_G:0.7255 D(x): 0.4862 D(G(z)): 0.4871 / 0.4873\n[77/100][2/3] Loss_D:1.4121 Loss_G:0.7305 D(x): 0.4820 D(G(z)): 0.4882 / 0.4848\n[78/100][0/3] Loss_D:1.3977 Loss_G:0.7245 D(x): 0.4872 D(G(z)): 0.4864 / 0.4879\n[78/100][1/3] Loss_D:1.3945 Loss_G:0.7206 D(x): 0.4889 D(G(z)): 0.4866 / 0.4897\n[78/100][2/3] Loss_D:1.4062 Loss_G:0.7090 D(x): 0.4887 D(G(z)): 0.4923 / 0.4955\n[79/100][0/3] Loss_D:1.3988 Loss_G:0.7230 D(x): 0.4905 D(G(z)): 0.4904 / 0.4886\n[79/100][1/3] Loss_D:1.4017 Loss_G:0.7187 D(x): 0.4894 D(G(z)): 0.4908 / 0.4907\n[79/100][2/3] Loss_D:1.4020 Loss_G:0.7071 D(x): 0.4904 D(G(z)): 0.4919 / 0.4962\n[80/100][0/3] Loss_D:1.4019 Loss_G:0.7137 D(x): 0.4917 D(G(z)): 0.4933 / 0.4931\n[80/100][1/3] Loss_D:1.3944 Loss_G:0.7116 D(x): 0.4928 D(G(z)): 0.4906 / 0.4942\n[80/100][2/3] Loss_D:1.3851 Loss_G:0.7354 D(x): 0.4963 D(G(z)): 0.4895 / 0.4824\n[81/100][0/3] Loss_D:1.4030 Loss_G:0.7127 D(x): 0.4924 D(G(z)): 0.4946 / 0.4936\n[81/100][1/3] Loss_D:1.4031 Loss_G:0.7133 D(x): 0.4916 D(G(z)): 0.4939 / 0.4933\n[81/100][2/3] Loss_D:1.3982 Loss_G:0.7191 D(x): 0.4897 D(G(z)): 0.4897 / 0.4908\n[82/100][0/3] Loss_D:1.3980 Loss_G:0.7106 D(x): 0.4945 D(G(z)): 0.4943 / 0.4945\n[82/100][1/3] Loss_D:1.3982 Loss_G:0.7138 D(x): 0.4949 D(G(z)): 0.4947 / 0.4931\n[82/100][2/3] Loss_D:1.4086 Loss_G:0.7126 D(x): 0.4953 D(G(z)): 0.5005 / 0.4935\n[83/100][0/3] Loss_D:1.3997 Loss_G:0.7186 D(x): 0.4918 D(G(z)): 0.4926 / 0.4906\n[83/100][1/3] Loss_D:1.3972 Loss_G:0.7203 D(x): 0.4902 D(G(z)): 0.4896 / 0.4897\n[83/100][2/3] Loss_D:1.4025 Loss_G:0.7173 D(x): 0.4834 D(G(z)): 0.4852 / 0.4912\n[84/100][0/3] Loss_D:1.4003 Loss_G:0.7245 D(x): 0.4875 D(G(z)): 0.4884 / 0.4877\n[84/100][1/3] Loss_D:1.3972 Loss_G:0.7215 D(x): 0.4909 D(G(z)): 0.4904 / 0.4891\n[84/100][2/3] Loss_D:1.4053 Loss_G:0.7299 D(x): 0.4880 D(G(z)): 0.4915 / 0.4849\n[85/100][0/3] Loss_D:1.3979 Loss_G:0.7156 D(x): 0.4911 D(G(z)): 0.4911 / 0.4920\n[85/100][1/3] Loss_D:1.3986 Loss_G:0.7163 D(x): 0.4906 D(G(z)): 0.4908 / 0.4916\n[85/100][2/3] Loss_D:1.4129 Loss_G:0.7137 D(x): 0.4827 D(G(z)): 0.4899 / 0.4928\n[86/100][0/3] Loss_D:1.4016 Loss_G:0.7156 D(x): 0.4912 D(G(z)): 0.4930 / 0.4920\n[86/100][1/3] Loss_D:1.3971 Loss_G:0.7125 D(x): 0.4924 D(G(z)): 0.4921 / 0.4934\n[86/100][2/3] Loss_D:1.4077 Loss_G:0.7094 D(x): 0.4852 D(G(z)): 0.4900 / 0.4949\n[87/100][0/3] Loss_D:1.3972 Loss_G:0.7146 D(x): 0.4930 D(G(z)): 0.4925 / 0.4925\n[87/100][1/3] Loss_D:1.3975 Loss_G:0.7088 D(x): 0.4936 D(G(z)): 0.4934 / 0.4952\n[87/100][2/3] Loss_D:1.3932 Loss_G:0.7096 D(x): 0.4951 D(G(z)): 0.4929 / 0.4950\n[88/100][0/3] Loss_D:1.3995 Loss_G:0.7106 D(x): 0.4952 D(G(z)): 0.4961 / 0.4944\n[88/100][1/3] Loss_D:1.3984 Loss_G:0.7108 D(x): 0.4937 D(G(z)): 0.4940 / 0.4943\n[88/100][2/3] Loss_D:1.3795 Loss_G:0.7165 D(x): 0.5015 D(G(z)): 0.4927 / 0.4912\n[89/100][0/3] Loss_D:1.3929 Loss_G:0.7119 D(x): 0.4951 D(G(z)): 0.4927 / 0.4937\n[89/100][1/3] Loss_D:1.3948 Loss_G:0.7134 D(x): 0.4940 D(G(z)): 0.4927 / 0.4929\n[89/100][2/3] Loss_D:1.3899 Loss_G:0.7126 D(x): 0.5001 D(G(z)): 0.4961 / 0.4933\n[90/100][0/3] Loss_D:1.4009 Loss_G:0.7144 D(x): 0.4933 D(G(z)): 0.4950 / 0.4925\n[90/100][1/3] Loss_D:1.4007 Loss_G:0.7109 D(x): 0.4943 D(G(z)): 0.4958 / 0.4942\n[90/100][2/3] Loss_D:1.4043 Loss_G:0.7108 D(x): 0.4898 D(G(z)): 0.4926 / 0.4942\n[91/100][0/3] Loss_D:1.3992 Loss_G:0.7115 D(x): 0.4938 D(G(z)): 0.4947 / 0.4938\n[91/100][1/3] Loss_D:1.3996 Loss_G:0.7096 D(x): 0.4931 D(G(z)): 0.4942 / 0.4948\n[91/100][2/3] Loss_D:1.3894 Loss_G:0.7111 D(x): 0.4947 D(G(z)): 0.4907 / 0.4940\n[92/100][0/3] Loss_D:1.3914 Loss_G:0.7171 D(x): 0.4937 D(G(z)): 0.4908 / 0.4911\n[92/100][1/3] Loss_D:1.3970 Loss_G:0.7159 D(x): 0.4920 D(G(z)): 0.4919 / 0.4916\n[92/100][2/3] Loss_D:1.3840 Loss_G:0.7109 D(x): 0.4951 D(G(z)): 0.4881 / 0.4938\n[93/100][0/3] Loss_D:1.3916 Loss_G:0.7119 D(x): 0.4942 D(G(z)): 0.4913 / 0.4936\n[93/100][1/3] Loss_D:1.3946 Loss_G:0.7056 D(x): 0.4950 D(G(z)): 0.4937 / 0.4966\n[93/100][2/3] Loss_D:1.3910 Loss_G:0.7106 D(x): 0.4961 D(G(z)): 0.4930 / 0.4944\n[94/100][0/3] Loss_D:1.3959 Loss_G:0.7027 D(x): 0.4969 D(G(z)): 0.4963 / 0.4981\n[94/100][1/3] Loss_D:1.3928 Loss_G:0.7071 D(x): 0.4978 D(G(z)): 0.4957 / 0.4960\n[94/100][2/3] Loss_D:1.3865 Loss_G:0.7095 D(x): 0.4990 D(G(z)): 0.4940 / 0.4949\n[95/100][0/3] Loss_D:1.3959 Loss_G:0.7021 D(x): 0.4990 D(G(z)): 0.4983 / 0.4985\n[95/100][1/3] Loss_D:1.4012 Loss_G:0.7006 D(x): 0.4975 D(G(z)): 0.4995 / 0.4992\n[95/100][2/3] Loss_D:1.4009 Loss_G:0.7006 D(x): 0.4988 D(G(z)): 0.5006 / 0.4991\n[96/100][0/3] Loss_D:1.3992 Loss_G:0.7105 D(x): 0.4955 D(G(z)): 0.4965 / 0.4942\n[96/100][1/3] Loss_D:1.4001 Loss_G:0.7127 D(x): 0.4919 D(G(z)): 0.4934 / 0.4931\n[96/100][2/3] Loss_D:1.3970 Loss_G:0.7138 D(x): 0.4914 D(G(z)): 0.4913 / 0.4925\n[97/100][0/3] Loss_D:1.3983 Loss_G:0.7166 D(x): 0.4918 D(G(z)): 0.4925 / 0.4912\n[97/100][1/3] Loss_D:1.3952 Loss_G:0.7179 D(x): 0.4907 D(G(z)): 0.4899 / 0.4905\n[97/100][2/3] Loss_D:1.3956 Loss_G:0.7165 D(x): 0.4892 D(G(z)): 0.4886 / 0.4912\n[98/100][0/3] Loss_D:1.4019 Loss_G:0.7186 D(x): 0.4891 D(G(z)): 0.4915 / 0.4901\n[98/100][1/3] Loss_D:1.3976 Loss_G:0.7179 D(x): 0.4897 D(G(z)): 0.4900 / 0.4905\n[98/100][2/3] Loss_D:1.3988 Loss_G:0.7255 D(x): 0.4917 D(G(z)): 0.4924 / 0.4871\n[99/100][0/3] Loss_D:1.3968 Loss_G:0.7134 D(x): 0.4917 D(G(z)): 0.4917 / 0.4927\n[99/100][1/3] Loss_D:1.3950 Loss_G:0.7102 D(x): 0.4920 D(G(z)): 0.4911 / 0.4943\n[99/100][2/3] Loss_D:1.4109 Loss_G:0.7029 D(x): 0.4922 D(G(z)): 0.4987 / 0.4980\n\u001b[32m[I 2022-09-11 10:58:53,185]\u001b[0m Trial 9 finished with value: 1.1483393907546997 and parameters: {'lr': 0.00037268558268820204, 'dropoutG': 0.4, 'dropoutD': 0.30000000000000004, 'optimizer_name': 'Adam'}. Best is trial 1 with value: 0.5526883006095886.\u001b[0m\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "\u001b[32m[I 2022-09-11 09:55:05,615]\u001b[0m A new study created in memory with name: TSGAN_study_2022-09-11T09:55\u001b[0m\n[0/100][0/3] Loss_D:2.0937 Loss_G:1.9368 D(x): 0.1440 D(G(z)): 0.1440 / 0.1442\n[0/100][1/3] Loss_D:2.0925 Loss_G:1.9354 D(x): 0.1442 D(G(z)): 0.1442 / 0.1444\n[0/100][2/3] Loss_D:2.0914 Loss_G:1.9339 D(x): 0.1444 D(G(z)): 0.1444 / 0.1446\n[1/100][0/3] Loss_D:2.0901 Loss_G:1.9324 D(x): 0.1446 D(G(z)): 0.1446 / 0.1448\n[1/100][1/3] Loss_D:2.0889 Loss_G:1.9309 D(x): 0.1448 D(G(z)): 0.1448 / 0.1450\n[1/100][2/3] Loss_D:2.0877 Loss_G:1.9294 D(x): 0.1450 D(G(z)): 0.1450 / 0.1453\n[2/100][0/3] Loss_D:2.0864 Loss_G:1.9279 D(x): 0.1453 D(G(z)): 0.1453 / 0.1455\n[2/100][1/3] Loss_D:2.0851 Loss_G:1.9263 D(x): 0.1455 D(G(z)): 0.1455 / 0.1457\n[2/100][2/3] Loss_D:2.0839 Loss_G:1.9248 D(x): 0.1457 D(G(z)): 0.1457 / 0.1459\n[3/100][0/3] Loss_D:2.0826 Loss_G:1.9232 D(x): 0.1459 D(G(z)): 0.1459 / 0.1462\n[3/100][1/3] Loss_D:2.0813 Loss_G:1.9217 D(x): 0.1462 D(G(z)): 0.1462 / 0.1464\n[3/100][2/3] Loss_D:2.0799 Loss_G:1.9201 D(x): 0.1464 D(G(z)): 0.1464 / 0.1466\n[4/100][0/3] Loss_D:2.0787 Loss_G:1.9185 D(x): 0.1466 D(G(z)): 0.1466 / 0.1468\n[4/100][1/3] Loss_D:2.0774 Loss_G:1.9169 D(x): 0.1468 D(G(z)): 0.1468 / 0.1471\n[4/100][2/3] Loss_D:2.0761 Loss_G:1.9154 D(x): 0.1471 D(G(z)): 0.1471 / 0.1473\n[5/100][0/3] Loss_D:2.0747 Loss_G:1.9138 D(x): 0.1473 D(G(z)): 0.1473 / 0.1476\n[5/100][1/3] Loss_D:2.0735 Loss_G:1.9122 D(x): 0.1475 D(G(z)): 0.1476 / 0.1478\n[5/100][2/3] Loss_D:2.0720 Loss_G:1.9105 D(x): 0.1478 D(G(z)): 0.1478 / 0.1480\n[6/100][0/3] Loss_D:2.0707 Loss_G:1.9089 D(x): 0.1480 D(G(z)): 0.1480 / 0.1483\n[6/100][1/3] Loss_D:2.0695 Loss_G:1.9073 D(x): 0.1483 D(G(z)): 0.1483 / 0.1485\n[6/100][2/3] Loss_D:2.0680 Loss_G:1.9057 D(x): 0.1485 D(G(z)): 0.1485 / 0.1488\n[7/100][0/3] Loss_D:2.0668 Loss_G:1.9040 D(x): 0.1487 D(G(z)): 0.1488 / 0.1490\n[7/100][1/3] Loss_D:2.0654 Loss_G:1.9024 D(x): 0.1490 D(G(z)): 0.1490 / 0.1492\n[7/100][2/3] Loss_D:2.0641 Loss_G:1.9008 D(x): 0.1492 D(G(z)): 0.1492 / 0.1495\n[8/100][0/3] Loss_D:2.0627 Loss_G:1.8991 D(x): 0.1495 D(G(z)): 0.1495 / 0.1497\n[8/100][1/3] Loss_D:2.0614 Loss_G:1.8974 D(x): 0.1497 D(G(z)): 0.1497 / 0.1500\n[8/100][2/3] Loss_D:2.0601 Loss_G:1.8958 D(x): 0.1500 D(G(z)): 0.1500 / 0.1502\n[9/100][0/3] Loss_D:2.0587 Loss_G:1.8941 D(x): 0.1502 D(G(z)): 0.1502 / 0.1505\n[9/100][1/3] Loss_D:2.0572 Loss_G:1.8925 D(x): 0.1505 D(G(z)): 0.1505 / 0.1507\n[9/100][2/3] Loss_D:2.0560 Loss_G:1.8908 D(x): 0.1507 D(G(z)): 0.1507 / 0.1510\n[10/100][0/3] Loss_D:2.0545 Loss_G:1.8891 D(x): 0.1510 D(G(z)): 0.1510 / 0.1512\n[10/100][1/3] Loss_D:2.0531 Loss_G:1.8874 D(x): 0.1512 D(G(z)): 0.1512 / 0.1515\n[10/100][2/3] Loss_D:2.0518 Loss_G:1.8857 D(x): 0.1515 D(G(z)): 0.1515 / 0.1518\n[11/100][0/3] Loss_D:2.0504 Loss_G:1.8840 D(x): 0.1517 D(G(z)): 0.1518 / 0.1520\n[11/100][1/3] Loss_D:2.0489 Loss_G:1.8823 D(x): 0.1520 D(G(z)): 0.1520 / 0.1523\n[11/100][2/3] Loss_D:2.0477 Loss_G:1.8806 D(x): 0.1523 D(G(z)): 0.1523 / 0.1525\n[12/100][0/3] Loss_D:2.0461 Loss_G:1.8789 D(x): 0.1525 D(G(z)): 0.1525 / 0.1528\n[12/100][1/3] Loss_D:2.0448 Loss_G:1.8772 D(x): 0.1528 D(G(z)): 0.1528 / 0.1531\n[12/100][2/3] Loss_D:2.0432 Loss_G:1.8755 D(x): 0.1531 D(G(z)): 0.1531 / 0.1533\n[13/100][0/3] Loss_D:2.0419 Loss_G:1.8737 D(x): 0.1533 D(G(z)): 0.1533 / 0.1536\n[13/100][1/3] Loss_D:2.0405 Loss_G:1.8720 D(x): 0.1536 D(G(z)): 0.1536 / 0.1539\n[13/100][2/3] Loss_D:2.0392 Loss_G:1.8702 D(x): 0.1538 D(G(z)): 0.1539 / 0.1541\n[14/100][0/3] Loss_D:2.0377 Loss_G:1.8685 D(x): 0.1541 D(G(z)): 0.1541 / 0.1544\n[14/100][1/3] Loss_D:2.0362 Loss_G:1.8667 D(x): 0.1544 D(G(z)): 0.1544 / 0.1547\n[14/100][2/3] Loss_D:2.0346 Loss_G:1.8650 D(x): 0.1547 D(G(z)): 0.1547 / 0.1549\n[15/100][0/3] Loss_D:2.0334 Loss_G:1.8632 D(x): 0.1549 D(G(z)): 0.1549 / 0.1552\n[15/100][1/3] Loss_D:2.0319 Loss_G:1.8614 D(x): 0.1552 D(G(z)): 0.1552 / 0.1555\n[15/100][2/3] Loss_D:2.0306 Loss_G:1.8596 D(x): 0.1555 D(G(z)): 0.1555 / 0.1558\n[16/100][0/3] Loss_D:2.0290 Loss_G:1.8578 D(x): 0.1558 D(G(z)): 0.1558 / 0.1561\n[16/100][1/3] Loss_D:2.0276 Loss_G:1.8561 D(x): 0.1560 D(G(z)): 0.1561 / 0.1563\n[16/100][2/3] Loss_D:2.0259 Loss_G:1.8543 D(x): 0.1564 D(G(z)): 0.1563 / 0.1566\n[17/100][0/3] Loss_D:2.0246 Loss_G:1.8525 D(x): 0.1566 D(G(z)): 0.1566 / 0.1569\n[17/100][1/3] Loss_D:2.0232 Loss_G:1.8506 D(x): 0.1569 D(G(z)): 0.1569 / 0.1572\n[17/100][2/3] Loss_D:2.0218 Loss_G:1.8488 D(x): 0.1572 D(G(z)): 0.1572 / 0.1575\n[18/100][0/3] Loss_D:2.0202 Loss_G:1.8470 D(x): 0.1575 D(G(z)): 0.1575 / 0.1578\n[18/100][1/3] Loss_D:2.0187 Loss_G:1.8452 D(x): 0.1578 D(G(z)): 0.1578 / 0.1580\n[18/100][2/3] Loss_D:2.0174 Loss_G:1.8433 D(x): 0.1580 D(G(z)): 0.1580 / 0.1583\n[19/100][0/3] Loss_D:2.0158 Loss_G:1.8415 D(x): 0.1583 D(G(z)): 0.1583 / 0.1586\n[19/100][1/3] Loss_D:2.0143 Loss_G:1.8397 D(x): 0.1586 D(G(z)): 0.1586 / 0.1589\n[19/100][2/3] Loss_D:2.0129 Loss_G:1.8378 D(x): 0.1589 D(G(z)): 0.1589 / 0.1592\n[20/100][0/3] Loss_D:2.0113 Loss_G:1.8359 D(x): 0.1592 D(G(z)): 0.1592 / 0.1595\n[20/100][1/3] Loss_D:2.0098 Loss_G:1.8341 D(x): 0.1595 D(G(z)): 0.1595 / 0.1598\n[20/100][2/3] Loss_D:2.0084 Loss_G:1.8322 D(x): 0.1598 D(G(z)): 0.1598 / 0.1601\n[21/100][0/3] Loss_D:2.0068 Loss_G:1.8303 D(x): 0.1601 D(G(z)): 0.1601 / 0.1604\n[21/100][1/3] Loss_D:2.0052 Loss_G:1.8284 D(x): 0.1604 D(G(z)): 0.1604 / 0.1607\n[21/100][2/3] Loss_D:2.0038 Loss_G:1.8265 D(x): 0.1607 D(G(z)): 0.1607 / 0.1610\n[22/100][0/3] Loss_D:2.0021 Loss_G:1.8246 D(x): 0.1610 D(G(z)): 0.1610 / 0.1613\n[22/100][1/3] Loss_D:2.0007 Loss_G:1.8227 D(x): 0.1613 D(G(z)): 0.1613 / 0.1616\n[22/100][2/3] Loss_D:1.9992 Loss_G:1.8208 D(x): 0.1616 D(G(z)): 0.1616 / 0.1620\n[23/100][0/3] Loss_D:1.9976 Loss_G:1.8189 D(x): 0.1619 D(G(z)): 0.1620 / 0.1623\n[23/100][1/3] Loss_D:1.9960 Loss_G:1.8170 D(x): 0.1623 D(G(z)): 0.1623 / 0.1626\n[23/100][2/3] Loss_D:1.9946 Loss_G:1.8150 D(x): 0.1626 D(G(z)): 0.1626 / 0.1629\n[24/100][0/3] Loss_D:1.9929 Loss_G:1.8131 D(x): 0.1629 D(G(z)): 0.1629 / 0.1632\n[24/100][1/3] Loss_D:1.9914 Loss_G:1.8111 D(x): 0.1632 D(G(z)): 0.1632 / 0.1635\n[24/100][2/3] Loss_D:1.9899 Loss_G:1.8092 D(x): 0.1635 D(G(z)): 0.1635 / 0.1639\n[25/100][0/3] Loss_D:1.9882 Loss_G:1.8072 D(x): 0.1639 D(G(z)): 0.1639 / 0.1642\n[25/100][1/3] Loss_D:1.9867 Loss_G:1.8053 D(x): 0.1642 D(G(z)): 0.1642 / 0.1645\n[25/100][2/3] Loss_D:1.9852 Loss_G:1.8033 D(x): 0.1645 D(G(z)): 0.1645 / 0.1648\n[26/100][0/3] Loss_D:1.9834 Loss_G:1.8013 D(x): 0.1648 D(G(z)): 0.1648 / 0.1652\n[26/100][1/3] Loss_D:1.9819 Loss_G:1.7993 D(x): 0.1651 D(G(z)): 0.1652 / 0.1655\n[26/100][2/3] Loss_D:1.9800 Loss_G:1.7973 D(x): 0.1655 D(G(z)): 0.1655 / 0.1658\n[27/100][0/3] Loss_D:1.9787 Loss_G:1.7953 D(x): 0.1658 D(G(z)): 0.1658 / 0.1661\n[27/100][1/3] Loss_D:1.9770 Loss_G:1.7933 D(x): 0.1661 D(G(z)): 0.1661 / 0.1665\n[27/100][2/3] Loss_D:1.9756 Loss_G:1.7913 D(x): 0.1665 D(G(z)): 0.1665 / 0.1668\n[28/100][0/3] Loss_D:1.9739 Loss_G:1.7892 D(x): 0.1668 D(G(z)): 0.1668 / 0.1672\n[28/100][1/3] Loss_D:1.9722 Loss_G:1.7872 D(x): 0.1672 D(G(z)): 0.1672 / 0.1675\n[28/100][2/3] Loss_D:1.9704 Loss_G:1.7852 D(x): 0.1675 D(G(z)): 0.1675 / 0.1678\n[29/100][0/3] Loss_D:1.9690 Loss_G:1.7831 D(x): 0.1678 D(G(z)): 0.1678 / 0.1682\n[29/100][1/3] Loss_D:1.9673 Loss_G:1.7811 D(x): 0.1682 D(G(z)): 0.1682 / 0.1685\n[29/100][2/3] Loss_D:1.9658 Loss_G:1.7790 D(x): 0.1685 D(G(z)): 0.1685 / 0.1689\n[30/100][0/3] Loss_D:1.9640 Loss_G:1.7769 D(x): 0.1689 D(G(z)): 0.1689 / 0.1692\n[30/100][1/3] Loss_D:1.9624 Loss_G:1.7748 D(x): 0.1692 D(G(z)): 0.1692 / 0.1696\n[30/100][2/3] Loss_D:1.9609 Loss_G:1.7727 D(x): 0.1696 D(G(z)): 0.1696 / 0.1700\n[31/100][0/3] Loss_D:1.9591 Loss_G:1.7706 D(x): 0.1699 D(G(z)): 0.1700 / 0.1703\n[31/100][1/3] Loss_D:1.9574 Loss_G:1.7685 D(x): 0.1703 D(G(z)): 0.1703 / 0.1707\n[31/100][2/3] Loss_D:1.9559 Loss_G:1.7664 D(x): 0.1706 D(G(z)): 0.1707 / 0.1710\n[32/100][0/3] Loss_D:1.9541 Loss_G:1.7643 D(x): 0.1710 D(G(z)): 0.1710 / 0.1714\n[32/100][1/3] Loss_D:1.9524 Loss_G:1.7622 D(x): 0.1714 D(G(z)): 0.1714 / 0.1718\n[32/100][2/3] Loss_D:1.9505 Loss_G:1.7600 D(x): 0.1718 D(G(z)): 0.1718 / 0.1721\n[33/100][0/3] Loss_D:1.9490 Loss_G:1.7579 D(x): 0.1721 D(G(z)): 0.1721 / 0.1725\n[33/100][1/3] Loss_D:1.9473 Loss_G:1.7557 D(x): 0.1725 D(G(z)): 0.1725 / 0.1729\n[33/100][2/3] Loss_D:1.9454 Loss_G:1.7536 D(x): 0.1729 D(G(z)): 0.1729 / 0.1733\n[34/100][0/3] Loss_D:1.9439 Loss_G:1.7514 D(x): 0.1732 D(G(z)): 0.1733 / 0.1736\n[34/100][1/3] Loss_D:1.9422 Loss_G:1.7492 D(x): 0.1736 D(G(z)): 0.1736 / 0.1740\n[34/100][2/3] Loss_D:1.9406 Loss_G:1.7470 D(x): 0.1740 D(G(z)): 0.1740 / 0.1744\n[35/100][0/3] Loss_D:1.9388 Loss_G:1.7448 D(x): 0.1744 D(G(z)): 0.1744 / 0.1748\n[35/100][1/3] Loss_D:1.9370 Loss_G:1.7426 D(x): 0.1748 D(G(z)): 0.1748 / 0.1752\n[35/100][2/3] Loss_D:1.9354 Loss_G:1.7404 D(x): 0.1751 D(G(z)): 0.1752 / 0.1756\n[36/100][0/3] Loss_D:1.9336 Loss_G:1.7382 D(x): 0.1755 D(G(z)): 0.1756 / 0.1759\n[36/100][1/3] Loss_D:1.9318 Loss_G:1.7360 D(x): 0.1759 D(G(z)): 0.1759 / 0.1763\n[36/100][2/3] Loss_D:1.9302 Loss_G:1.7337 D(x): 0.1763 D(G(z)): 0.1763 / 0.1767\n[37/100][0/3] Loss_D:1.9283 Loss_G:1.7315 D(x): 0.1767 D(G(z)): 0.1767 / 0.1771\n[37/100][1/3] Loss_D:1.9265 Loss_G:1.7292 D(x): 0.1771 D(G(z)): 0.1771 / 0.1775\n[37/100][2/3] Loss_D:1.9249 Loss_G:1.7270 D(x): 0.1775 D(G(z)): 0.1775 / 0.1779\n[38/100][0/3] Loss_D:1.9230 Loss_G:1.7247 D(x): 0.1779 D(G(z)): 0.1779 / 0.1783\n[38/100][1/3] Loss_D:1.9212 Loss_G:1.7224 D(x): 0.1783 D(G(z)): 0.1783 / 0.1788\n[38/100][2/3] Loss_D:1.9192 Loss_G:1.7201 D(x): 0.1788 D(G(z)): 0.1788 / 0.1792\n[39/100][0/3] Loss_D:1.9177 Loss_G:1.7178 D(x): 0.1792 D(G(z)): 0.1792 / 0.1796\n[39/100][1/3] Loss_D:1.9159 Loss_G:1.7155 D(x): 0.1796 D(G(z)): 0.1796 / 0.1800\n[39/100][2/3] Loss_D:1.9138 Loss_G:1.7132 D(x): 0.1800 D(G(z)): 0.1800 / 0.1804\n[40/100][0/3] Loss_D:1.9122 Loss_G:1.7109 D(x): 0.1804 D(G(z)): 0.1804 / 0.1808\n[40/100][1/3] Loss_D:1.9104 Loss_G:1.7085 D(x): 0.1808 D(G(z)): 0.1808 / 0.1813\n[40/100][2/3] Loss_D:1.9088 Loss_G:1.7062 D(x): 0.1812 D(G(z)): 0.1813 / 0.1817\n[41/100][0/3] Loss_D:1.9068 Loss_G:1.7038 D(x): 0.1817 D(G(z)): 0.1817 / 0.1821\n[41/100][1/3] Loss_D:1.9050 Loss_G:1.7014 D(x): 0.1821 D(G(z)): 0.1821 / 0.1826\n[41/100][2/3] Loss_D:1.9033 Loss_G:1.6991 D(x): 0.1825 D(G(z)): 0.1826 / 0.1830\n[42/100][0/3] Loss_D:1.9014 Loss_G:1.6967 D(x): 0.1830 D(G(z)): 0.1830 / 0.1834\n[42/100][1/3] Loss_D:1.8994 Loss_G:1.6943 D(x): 0.1834 D(G(z)): 0.1834 / 0.1839\n[42/100][2/3] Loss_D:1.8973 Loss_G:1.6919 D(x): 0.1839 D(G(z)): 0.1839 / 0.1843\n[43/100][0/3] Loss_D:1.8957 Loss_G:1.6895 D(x): 0.1843 D(G(z)): 0.1843 / 0.1848\n[43/100][1/3] Loss_D:1.8938 Loss_G:1.6870 D(x): 0.1848 D(G(z)): 0.1848 / 0.1852\n[43/100][2/3] Loss_D:1.8922 Loss_G:1.6846 D(x): 0.1852 D(G(z)): 0.1852 / 0.1857\n[44/100][0/3] Loss_D:1.8900 Loss_G:1.6822 D(x): 0.1857 D(G(z)): 0.1857 / 0.1861\n[44/100][1/3] Loss_D:1.8883 Loss_G:1.6797 D(x): 0.1861 D(G(z)): 0.1861 / 0.1866\n[44/100][2/3] Loss_D:1.8865 Loss_G:1.6772 D(x): 0.1865 D(G(z)): 0.1866 / 0.1871\n[45/100][0/3] Loss_D:1.8845 Loss_G:1.6748 D(x): 0.1870 D(G(z)): 0.1871 / 0.1875\n[45/100][1/3] Loss_D:1.8824 Loss_G:1.6723 D(x): 0.1875 D(G(z)): 0.1875 / 0.1880\n[45/100][2/3] Loss_D:1.8808 Loss_G:1.6699 D(x): 0.1879 D(G(z)): 0.1880 / 0.1884\n[46/100][0/3] Loss_D:1.8787 Loss_G:1.6674 D(x): 0.1884 D(G(z)): 0.1884 / 0.1889\n[46/100][1/3] Loss_D:1.8769 Loss_G:1.6649 D(x): 0.1889 D(G(z)): 0.1889 / 0.1894\n[46/100][2/3] Loss_D:1.8752 Loss_G:1.6624 D(x): 0.1893 D(G(z)): 0.1894 / 0.1899\n[47/100][0/3] Loss_D:1.8731 Loss_G:1.6599 D(x): 0.1898 D(G(z)): 0.1899 / 0.1903\n[47/100][1/3] Loss_D:1.8711 Loss_G:1.6574 D(x): 0.1903 D(G(z)): 0.1903 / 0.1908\n[47/100][2/3] Loss_D:1.8694 Loss_G:1.6548 D(x): 0.1908 D(G(z)): 0.1908 / 0.1913\n[48/100][0/3] Loss_D:1.8673 Loss_G:1.6523 D(x): 0.1913 D(G(z)): 0.1913 / 0.1918\n[48/100][1/3] Loss_D:1.8654 Loss_G:1.6498 D(x): 0.1918 D(G(z)): 0.1918 / 0.1923\n[48/100][2/3] Loss_D:1.8631 Loss_G:1.6472 D(x): 0.1923 D(G(z)): 0.1923 / 0.1928\n[49/100][0/3] Loss_D:1.8615 Loss_G:1.6446 D(x): 0.1928 D(G(z)): 0.1928 / 0.1933\n[49/100][1/3] Loss_D:1.8595 Loss_G:1.6421 D(x): 0.1933 D(G(z)): 0.1933 / 0.1938\n[49/100][2/3] Loss_D:1.8573 Loss_G:1.6395 D(x): 0.1938 D(G(z)): 0.1938 / 0.1943\n[50/100][0/3] Loss_D:1.8556 Loss_G:1.6369 D(x): 0.1943 D(G(z)): 0.1943 / 0.1948\n[50/100][1/3] Loss_D:1.8537 Loss_G:1.6343 D(x): 0.1948 D(G(z)): 0.1948 / 0.1953\n[50/100][2/3] Loss_D:1.8514 Loss_G:1.6316 D(x): 0.1954 D(G(z)): 0.1953 / 0.1958\n[51/100][0/3] Loss_D:1.8498 Loss_G:1.6290 D(x): 0.1958 D(G(z)): 0.1958 / 0.1963\n[51/100][1/3] Loss_D:1.8477 Loss_G:1.6264 D(x): 0.1963 D(G(z)): 0.1963 / 0.1969\n[51/100][2/3] Loss_D:1.8454 Loss_G:1.6237 D(x): 0.1969 D(G(z)): 0.1969 / 0.1974\n[52/100][0/3] Loss_D:1.8437 Loss_G:1.6210 D(x): 0.1974 D(G(z)): 0.1974 / 0.1979\n[52/100][1/3] Loss_D:1.8417 Loss_G:1.6184 D(x): 0.1979 D(G(z)): 0.1979 / 0.1985\n[52/100][2/3] Loss_D:1.8399 Loss_G:1.6157 D(x): 0.1984 D(G(z)): 0.1985 / 0.1990\n[53/100][0/3] Loss_D:1.8377 Loss_G:1.6130 D(x): 0.1990 D(G(z)): 0.1990 / 0.1995\n[53/100][1/3] Loss_D:1.8357 Loss_G:1.6103 D(x): 0.1995 D(G(z)): 0.1995 / 0.2001\n[53/100][2/3] Loss_D:1.8333 Loss_G:1.6075 D(x): 0.2001 D(G(z)): 0.2001 / 0.2006\n[54/100][0/3] Loss_D:1.8316 Loss_G:1.6048 D(x): 0.2006 D(G(z)): 0.2006 / 0.2012\n[54/100][1/3] Loss_D:1.8296 Loss_G:1.6021 D(x): 0.2012 D(G(z)): 0.2012 / 0.2017\n[54/100][2/3] Loss_D:1.8272 Loss_G:1.5993 D(x): 0.2018 D(G(z)): 0.2017 / 0.2023\n[55/100][0/3] Loss_D:1.8255 Loss_G:1.5966 D(x): 0.2023 D(G(z)): 0.2023 / 0.2029\n[55/100][1/3] Loss_D:1.8235 Loss_G:1.5938 D(x): 0.2028 D(G(z)): 0.2029 / 0.2034\n[55/100][2/3] Loss_D:1.8210 Loss_G:1.5910 D(x): 0.2035 D(G(z)): 0.2034 / 0.2040\n[56/100][0/3] Loss_D:1.8193 Loss_G:1.5882 D(x): 0.2040 D(G(z)): 0.2040 / 0.2046\n[56/100][1/3] Loss_D:1.8172 Loss_G:1.5854 D(x): 0.2046 D(G(z)): 0.2046 / 0.2052\n[56/100][2/3] Loss_D:1.8154 Loss_G:1.5826 D(x): 0.2051 D(G(z)): 0.2052 / 0.2057\n[57/100][0/3] Loss_D:1.8130 Loss_G:1.5797 D(x): 0.2057 D(G(z)): 0.2057 / 0.2063\n[57/100][1/3] Loss_D:1.8110 Loss_G:1.5769 D(x): 0.2063 D(G(z)): 0.2063 / 0.2069\n[57/100][2/3] Loss_D:1.8091 Loss_G:1.5740 D(x): 0.2069 D(G(z)): 0.2069 / 0.2075\n[58/100][0/3] Loss_D:1.8068 Loss_G:1.5712 D(x): 0.2075 D(G(z)): 0.2075 / 0.2081\n[58/100][1/3] Loss_D:1.8046 Loss_G:1.5683 D(x): 0.2081 D(G(z)): 0.2081 / 0.2087\n[58/100][2/3] Loss_D:1.8028 Loss_G:1.5654 D(x): 0.2087 D(G(z)): 0.2087 / 0.2093\n[59/100][0/3] Loss_D:1.8004 Loss_G:1.5625 D(x): 0.2093 D(G(z)): 0.2093 / 0.2099\n[59/100][1/3] Loss_D:1.7983 Loss_G:1.5596 D(x): 0.2099 D(G(z)): 0.2099 / 0.2105\n[59/100][2/3] Loss_D:1.7958 Loss_G:1.5567 D(x): 0.2106 D(G(z)): 0.2105 / 0.2112\n[60/100][0/3] Loss_D:1.7941 Loss_G:1.5537 D(x): 0.2111 D(G(z)): 0.2112 / 0.2118\n[60/100][1/3] Loss_D:1.7919 Loss_G:1.5508 D(x): 0.2118 D(G(z)): 0.2118 / 0.2124\n[60/100][2/3] Loss_D:1.7894 Loss_G:1.5478 D(x): 0.2125 D(G(z)): 0.2124 / 0.2131\n[61/100][0/3] Loss_D:1.7876 Loss_G:1.5448 D(x): 0.2130 D(G(z)): 0.2131 / 0.2137\n[61/100][1/3] Loss_D:1.7855 Loss_G:1.5419 D(x): 0.2137 D(G(z)): 0.2137 / 0.2143\n[61/100][2/3] Loss_D:1.7835 Loss_G:1.5389 D(x): 0.2143 D(G(z)): 0.2143 / 0.2150\n[62/100][0/3] Loss_D:1.7810 Loss_G:1.5358 D(x): 0.2150 D(G(z)): 0.2150 / 0.2156\n[62/100][1/3] Loss_D:1.7790 Loss_G:1.5328 D(x): 0.2156 D(G(z)): 0.2156 / 0.2163\n[62/100][2/3] Loss_D:1.7770 Loss_G:1.5298 D(x): 0.2162 D(G(z)): 0.2163 / 0.2170\n[63/100][0/3] Loss_D:1.7746 Loss_G:1.5267 D(x): 0.2169 D(G(z)): 0.2170 / 0.2176\n[63/100][1/3] Loss_D:1.7723 Loss_G:1.5237 D(x): 0.2176 D(G(z)): 0.2176 / 0.2183\n[63/100][2/3] Loss_D:1.7704 Loss_G:1.5206 D(x): 0.2182 D(G(z)): 0.2183 / 0.2190\n[64/100][0/3] Loss_D:1.7679 Loss_G:1.5175 D(x): 0.2190 D(G(z)): 0.2190 / 0.2197\n[64/100][1/3] Loss_D:1.7658 Loss_G:1.5144 D(x): 0.2196 D(G(z)): 0.2197 / 0.2203\n[64/100][2/3] Loss_D:1.7638 Loss_G:1.5113 D(x): 0.2203 D(G(z)): 0.2203 / 0.2210\n[65/100][0/3] Loss_D:1.7614 Loss_G:1.5082 D(x): 0.2210 D(G(z)): 0.2210 / 0.2217\n[65/100][1/3] Loss_D:1.7591 Loss_G:1.5051 D(x): 0.2217 D(G(z)): 0.2217 / 0.2224\n[65/100][2/3] Loss_D:1.7571 Loss_G:1.5019 D(x): 0.2224 D(G(z)): 0.2224 / 0.2231\n[66/100][0/3] Loss_D:1.7545 Loss_G:1.4988 D(x): 0.2231 D(G(z)): 0.2231 / 0.2238\n[66/100][1/3] Loss_D:1.7525 Loss_G:1.4956 D(x): 0.2238 D(G(z)): 0.2238 / 0.2246\n[66/100][2/3] Loss_D:1.7504 Loss_G:1.4924 D(x): 0.2245 D(G(z)): 0.2246 / 0.2253\n[67/100][0/3] Loss_D:1.7478 Loss_G:1.4892 D(x): 0.2253 D(G(z)): 0.2253 / 0.2260\n[67/100][1/3] Loss_D:1.7457 Loss_G:1.4860 D(x): 0.2260 D(G(z)): 0.2260 / 0.2267\n[67/100][2/3] Loss_D:1.7436 Loss_G:1.4828 D(x): 0.2267 D(G(z)): 0.2267 / 0.2275\n[68/100][0/3] Loss_D:1.7411 Loss_G:1.4796 D(x): 0.2275 D(G(z)): 0.2275 / 0.2282\n[68/100][1/3] Loss_D:1.7389 Loss_G:1.4763 D(x): 0.2282 D(G(z)): 0.2282 / 0.2290\n[68/100][2/3] Loss_D:1.7368 Loss_G:1.4731 D(x): 0.2289 D(G(z)): 0.2290 / 0.2297\n[69/100][0/3] Loss_D:1.7343 Loss_G:1.4698 D(x): 0.2297 D(G(z)): 0.2297 / 0.2305\n[69/100][1/3] Loss_D:1.7321 Loss_G:1.4665 D(x): 0.2304 D(G(z)): 0.2305 / 0.2312\n[69/100][2/3] Loss_D:1.7293 Loss_G:1.4632 D(x): 0.2313 D(G(z)): 0.2312 / 0.2320\n[70/100][0/3] Loss_D:1.7274 Loss_G:1.4599 D(x): 0.2320 D(G(z)): 0.2320 / 0.2328\n[70/100][1/3] Loss_D:1.7252 Loss_G:1.4566 D(x): 0.2328 D(G(z)): 0.2328 / 0.2336\n[70/100][2/3] Loss_D:1.7224 Loss_G:1.4532 D(x): 0.2337 D(G(z)): 0.2336 / 0.2344\n[71/100][0/3] Loss_D:1.7205 Loss_G:1.4499 D(x): 0.2344 D(G(z)): 0.2344 / 0.2352\n[71/100][1/3] Loss_D:1.7183 Loss_G:1.4465 D(x): 0.2351 D(G(z)): 0.2352 / 0.2360\n[71/100][2/3] Loss_D:1.7162 Loss_G:1.4432 D(x): 0.2359 D(G(z)): 0.2360 / 0.2368\n[72/100][0/3] Loss_D:1.7136 Loss_G:1.4398 D(x): 0.2367 D(G(z)): 0.2368 / 0.2376\n[72/100][1/3] Loss_D:1.7113 Loss_G:1.4364 D(x): 0.2376 D(G(z)): 0.2376 / 0.2384\n[72/100][2/3] Loss_D:1.7093 Loss_G:1.4330 D(x): 0.2383 D(G(z)): 0.2384 / 0.2392\n[73/100][0/3] Loss_D:1.7066 Loss_G:1.4295 D(x): 0.2392 D(G(z)): 0.2392 / 0.2400\n[73/100][1/3] Loss_D:1.7043 Loss_G:1.4261 D(x): 0.2400 D(G(z)): 0.2400 / 0.2409\n[73/100][2/3] Loss_D:1.7023 Loss_G:1.4227 D(x): 0.2408 D(G(z)): 0.2409 / 0.2417\n[74/100][0/3] Loss_D:1.6996 Loss_G:1.4192 D(x): 0.2417 D(G(z)): 0.2417 / 0.2426\n[74/100][1/3] Loss_D:1.6974 Loss_G:1.4157 D(x): 0.2425 D(G(z)): 0.2426 / 0.2434\n[74/100][2/3] Loss_D:1.6945 Loss_G:1.4122 D(x): 0.2435 D(G(z)): 0.2434 / 0.2443\n[75/100][0/3] Loss_D:1.6927 Loss_G:1.4087 D(x): 0.2442 D(G(z)): 0.2443 / 0.2451\n[75/100][1/3] Loss_D:1.6902 Loss_G:1.4052 D(x): 0.2451 D(G(z)): 0.2451 / 0.2460\n[75/100][2/3] Loss_D:1.6874 Loss_G:1.4017 D(x): 0.2461 D(G(z)): 0.2460 / 0.2469\n[76/100][0/3] Loss_D:1.6856 Loss_G:1.3982 D(x): 0.2468 D(G(z)): 0.2469 / 0.2478\n[76/100][1/3] Loss_D:1.6832 Loss_G:1.3946 D(x): 0.2477 D(G(z)): 0.2478 / 0.2486\n[76/100][2/3] Loss_D:1.6804 Loss_G:1.3911 D(x): 0.2487 D(G(z)): 0.2486 / 0.2495\n[77/100][0/3] Loss_D:1.6785 Loss_G:1.3875 D(x): 0.2495 D(G(z)): 0.2495 / 0.2504\n[77/100][1/3] Loss_D:1.6761 Loss_G:1.3839 D(x): 0.2504 D(G(z)): 0.2504 / 0.2513\n[77/100][2/3] Loss_D:1.6733 Loss_G:1.3803 D(x): 0.2514 D(G(z)): 0.2513 / 0.2523\n[78/100][0/3] Loss_D:1.6713 Loss_G:1.3767 D(x): 0.2523 D(G(z)): 0.2523 / 0.2532\n[78/100][1/3] Loss_D:1.6691 Loss_G:1.3730 D(x): 0.2531 D(G(z)): 0.2532 / 0.2541\n[78/100][2/3] Loss_D:1.6669 Loss_G:1.3694 D(x): 0.2540 D(G(z)): 0.2541 / 0.2551\n[79/100][0/3] Loss_D:1.6643 Loss_G:1.3657 D(x): 0.2550 D(G(z)): 0.2551 / 0.2560\n[79/100][1/3] Loss_D:1.6618 Loss_G:1.3620 D(x): 0.2560 D(G(z)): 0.2560 / 0.2570\n[79/100][2/3] Loss_D:1.6598 Loss_G:1.3583 D(x): 0.2569 D(G(z)): 0.2570 / 0.2579\n[80/100][0/3] Loss_D:1.6572 Loss_G:1.3546 D(x): 0.2578 D(G(z)): 0.2579 / 0.2589\n[80/100][1/3] Loss_D:1.6546 Loss_G:1.3509 D(x): 0.2589 D(G(z)): 0.2589 / 0.2599\n[80/100][2/3] Loss_D:1.6518 Loss_G:1.3472 D(x): 0.2600 D(G(z)): 0.2599 / 0.2608\n[81/100][0/3] Loss_D:1.6500 Loss_G:1.3435 D(x): 0.2608 D(G(z)): 0.2608 / 0.2618\n[81/100][1/3] Loss_D:1.6473 Loss_G:1.3397 D(x): 0.2618 D(G(z)): 0.2618 / 0.2628\n[81/100][2/3] Loss_D:1.6454 Loss_G:1.3360 D(x): 0.2627 D(G(z)): 0.2628 / 0.2638\n[82/100][0/3] Loss_D:1.6427 Loss_G:1.3322 D(x): 0.2638 D(G(z)): 0.2638 / 0.2648\n[82/100][1/3] Loss_D:1.6403 Loss_G:1.3284 D(x): 0.2648 D(G(z)): 0.2648 / 0.2658\n[82/100][2/3] Loss_D:1.6382 Loss_G:1.3246 D(x): 0.2657 D(G(z)): 0.2658 / 0.2669\n[83/100][0/3] Loss_D:1.6355 Loss_G:1.3209 D(x): 0.2668 D(G(z)): 0.2669 / 0.2679\n[83/100][1/3] Loss_D:1.6332 Loss_G:1.3171 D(x): 0.2678 D(G(z)): 0.2679 / 0.2689\n[83/100][2/3] Loss_D:1.6303 Loss_G:1.3133 D(x): 0.2690 D(G(z)): 0.2689 / 0.2699\n[84/100][0/3] Loss_D:1.6287 Loss_G:1.3095 D(x): 0.2698 D(G(z)): 0.2699 / 0.2710\n[84/100][1/3] Loss_D:1.6258 Loss_G:1.3057 D(x): 0.2710 D(G(z)): 0.2710 / 0.2720\n[84/100][2/3] Loss_D:1.6240 Loss_G:1.3019 D(x): 0.2719 D(G(z)): 0.2720 / 0.2731\n[85/100][0/3] Loss_D:1.6214 Loss_G:1.2981 D(x): 0.2730 D(G(z)): 0.2731 / 0.2741\n[85/100][1/3] Loss_D:1.6189 Loss_G:1.2943 D(x): 0.2741 D(G(z)): 0.2741 / 0.2752\n[85/100][2/3] Loss_D:1.6170 Loss_G:1.2904 D(x): 0.2751 D(G(z)): 0.2752 / 0.2762\n[86/100][0/3] Loss_D:1.6142 Loss_G:1.2866 D(x): 0.2762 D(G(z)): 0.2762 / 0.2773\n[86/100][1/3] Loss_D:1.6120 Loss_G:1.2828 D(x): 0.2773 D(G(z)): 0.2773 / 0.2784\n[86/100][2/3] Loss_D:1.6099 Loss_G:1.2789 D(x): 0.2783 D(G(z)): 0.2784 / 0.2795\n[87/100][0/3] Loss_D:1.6073 Loss_G:1.2750 D(x): 0.2794 D(G(z)): 0.2795 / 0.2806\n[87/100][1/3] Loss_D:1.6050 Loss_G:1.2712 D(x): 0.2805 D(G(z)): 0.2806 / 0.2817\n[87/100][2/3] Loss_D:1.6020 Loss_G:1.2673 D(x): 0.2818 D(G(z)): 0.2817 / 0.2828\n[88/100][0/3] Loss_D:1.6003 Loss_G:1.2634 D(x): 0.2828 D(G(z)): 0.2828 / 0.2839\n[88/100][1/3] Loss_D:1.5979 Loss_G:1.2595 D(x): 0.2839 D(G(z)): 0.2839 / 0.2850\n[88/100][2/3] Loss_D:1.5959 Loss_G:1.2556 D(x): 0.2849 D(G(z)): 0.2850 / 0.2862\n[89/100][0/3] Loss_D:1.5934 Loss_G:1.2517 D(x): 0.2861 D(G(z)): 0.2862 / 0.2873\n[89/100][1/3] Loss_D:1.5909 Loss_G:1.2477 D(x): 0.2873 D(G(z)): 0.2873 / 0.2884\n[89/100][2/3] Loss_D:1.5881 Loss_G:1.2438 D(x): 0.2886 D(G(z)): 0.2884 / 0.2896\n[90/100][0/3] Loss_D:1.5865 Loss_G:1.2399 D(x): 0.2895 D(G(z)): 0.2896 / 0.2907\n[90/100][1/3] Loss_D:1.5841 Loss_G:1.2360 D(x): 0.2907 D(G(z)): 0.2907 / 0.2919\n[90/100][2/3] Loss_D:1.5812 Loss_G:1.2320 D(x): 0.2920 D(G(z)): 0.2919 / 0.2931\n[91/100][0/3] Loss_D:1.5795 Loss_G:1.2281 D(x): 0.2930 D(G(z)): 0.2931 / 0.2942\n[91/100][1/3] Loss_D:1.5773 Loss_G:1.2241 D(x): 0.2942 D(G(z)): 0.2942 / 0.2954\n[91/100][2/3] Loss_D:1.5753 Loss_G:1.2202 D(x): 0.2953 D(G(z)): 0.2954 / 0.2966\n[92/100][0/3] Loss_D:1.5729 Loss_G:1.2162 D(x): 0.2965 D(G(z)): 0.2966 / 0.2978\n[92/100][1/3] Loss_D:1.5703 Loss_G:1.2122 D(x): 0.2978 D(G(z)): 0.2978 / 0.2990\n[92/100][2/3] Loss_D:1.5686 Loss_G:1.2083 D(x): 0.2989 D(G(z)): 0.2990 / 0.3002\n[93/100][0/3] Loss_D:1.5660 Loss_G:1.2043 D(x): 0.3002 D(G(z)): 0.3002 / 0.3014\n[93/100][1/3] Loss_D:1.5638 Loss_G:1.2003 D(x): 0.3014 D(G(z)): 0.3014 / 0.3026\n[93/100][2/3] Loss_D:1.5619 Loss_G:1.1963 D(x): 0.3025 D(G(z)): 0.3026 / 0.3038\n[94/100][0/3] Loss_D:1.5593 Loss_G:1.1924 D(x): 0.3038 D(G(z)): 0.3038 / 0.3051\n[94/100][1/3] Loss_D:1.5572 Loss_G:1.1884 D(x): 0.3050 D(G(z)): 0.3051 / 0.3063\n[94/100][2/3] Loss_D:1.5553 Loss_G:1.1844 D(x): 0.3062 D(G(z)): 0.3063 / 0.3075\n[95/100][0/3] Loss_D:1.5527 Loss_G:1.1804 D(x): 0.3075 D(G(z)): 0.3075 / 0.3088\n[95/100][1/3] Loss_D:1.5506 Loss_G:1.1764 D(x): 0.3087 D(G(z)): 0.3088 / 0.3100\n[95/100][2/3] Loss_D:1.5488 Loss_G:1.1724 D(x): 0.3099 D(G(z)): 0.3100 / 0.3113\n[96/100][0/3] Loss_D:1.5463 Loss_G:1.1684 D(x): 0.3112 D(G(z)): 0.3113 / 0.3126\n[96/100][1/3] Loss_D:1.5441 Loss_G:1.1644 D(x): 0.3125 D(G(z)): 0.3126 / 0.3138\n[96/100][2/3] Loss_D:1.5424 Loss_G:1.1604 D(x): 0.3137 D(G(z)): 0.3138 / 0.3151\n[97/100][0/3] Loss_D:1.5399 Loss_G:1.1564 D(x): 0.3150 D(G(z)): 0.3151 / 0.3164\n[97/100][1/3] Loss_D:1.5377 Loss_G:1.1524 D(x): 0.3164 D(G(z)): 0.3164 / 0.3177\n[97/100][2/3] Loss_D:1.5360 Loss_G:1.1484 D(x): 0.3175 D(G(z)): 0.3177 / 0.3190\n[98/100][0/3] Loss_D:1.5335 Loss_G:1.1444 D(x): 0.3189 D(G(z)): 0.3190 / 0.3202\n[98/100][1/3] Loss_D:1.5316 Loss_G:1.1405 D(x): 0.3202 D(G(z)): 0.3202 / 0.3215\n[98/100][2/3] Loss_D:1.5288 Loss_G:1.1365 D(x): 0.3217 D(G(z)): 0.3215 / 0.3229\n[99/100][0/3] Loss_D:1.5273 Loss_G:1.1325 D(x): 0.3228 D(G(z)): 0.3229 / 0.3242\n[99/100][1/3] Loss_D:1.5255 Loss_G:1.1285 D(x): 0.3241 D(G(z)): 0.3242 / 0.3255\n[99/100][2/3] Loss_D:1.5237 Loss_G:1.1245 D(x): 0.3253 D(G(z)): 0.3255 / 0.3268\n\u001b[32m[I 2022-09-11 10:01:32,435]\u001b[0m Trial 0 finished with value: 0.6215788722038269 and parameters: {'lr': 0.004599396160155632, 'dropoutG': 0.2, 'dropoutD': 0.0, 'optimizer_name': 'Adadelta'}. Best is trial 0 with value: 0.6215788722038269.\u001b[0m\n[0/100][0/3] Loss_D:2.0808 Loss_G:1.9164 D(x): 0.1466 D(G(z)): 0.1465 / 0.1475\n[0/100][1/3] Loss_D:2.0756 Loss_G:1.9120 D(x): 0.1475 D(G(z)): 0.1474 / 0.1481\n[0/100][2/3] Loss_D:2.0735 Loss_G:1.9084 D(x): 0.1480 D(G(z)): 0.1486 / 0.1486\n[1/100][0/3] Loss_D:2.0685 Loss_G:1.9021 D(x): 0.1488 D(G(z)): 0.1489 / 0.1496\n[1/100][1/3] Loss_D:2.0643 Loss_G:1.8953 D(x): 0.1496 D(G(z)): 0.1495 / 0.1506\n[1/100][2/3] Loss_D:2.0599 Loss_G:1.8931 D(x): 0.1503 D(G(z)): 0.1501 / 0.1509\n[2/100][0/3] Loss_D:2.0552 Loss_G:1.8876 D(x): 0.1512 D(G(z)): 0.1511 / 0.1518\n[2/100][1/3] Loss_D:2.0512 Loss_G:1.8818 D(x): 0.1520 D(G(z)): 0.1519 / 0.1527\n[2/100][2/3] Loss_D:2.0452 Loss_G:1.8772\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n884 D(x): 0.4674 D(G(z)): 0.4582 / 0.4670\n[98/100][0/3] Loss_D:1.4434 Loss_G:0.8006 D(x): 0.4599 D(G(z)): 0.4628 / 0.4616\n[98/100][1/3] Loss_D:1.4344 Loss_G:0.7977 D(x): 0.4625 D(G(z)): 0.4613 / 0.4630\n[98/100][2/3] Loss_D:1.4252 Loss_G:0.8263 D(x): 0.4615 D(G(z)): 0.4563 / 0.4503\n[99/100][0/3] Loss_D:1.4375 Loss_G:0.7968 D(x): 0.4639 D(G(z)): 0.4645 / 0.4636\n[99/100][1/3] Loss_D:1.4350 Loss_G:0.7867 D(x): 0.4652 D(G(z)): 0.4646 / 0.4680\n[99/100][2/3] Loss_D:1.4680 Loss_G:0.7643 D(x): 0.4647 D(G(z)): 0.4793 / 0.4778\n\u001b[32m[I 2022-09-11 10:52:34,247]\u001b[0m Trial 8 finished with value: 0.6019051671028137 and parameters: {'lr': 0.005743848041527131, 'dropoutG': 0.1, 'dropoutD': 0.4, 'optimizer_name': 'Adadelta'}. Best is trial 1 with value: 0.5526883006095886.\u001b[0m\n[0/100][0/3] Loss_D:2.0808 Loss_G:1.8852 D(x): 0.1466 D(G(z)): 0.1465 / 0.1522\n[0/100][1/3] Loss_D:2.0499 Loss_G:1.8489 D(x): 0.1522 D(G(z)): 0.1521 / 0.1578\n[0/100][2/3] Loss_D:2.0224 Loss_G:1.8133 D(x): 0.1577 D(G(z)): 0.1583 / 0.1636\n[1/100][0/3] Loss_D:1.9914 Loss_G:1.7750 D(x): 0.1637 D(G(z)): 0.1638 / 0.1700\n[1/100][1/3] Loss_D:1.9611 Loss_G:1.7341 D(x): 0.1701 D(G(z)): 0.1699 / 0.1772\n[1/100][2/3] Loss_D:1.9316 Loss_G:1.6980 D(x): 0.1766 D(G(z)): 0.1766 / 0.1837\n[2/100][0/3] Loss_D:1.8999 Loss_G:1.6591 D(x): 0.1840 D(G(z)): 0.1838 / 0.1911\n[2/100][1/3] Loss_D:1.8693 Loss_G:1.6169 D(x): 0.1916 D(G(z)): 0.1913 / 0.1995\n[2/100][2/3] Loss_D:1.8383 Loss_G:1.5780 D(x): 0.1994 D(G(z)): 0.1983 / 0.2075\n[3/100][0/3] Loss_D:1.8091 Loss_G:1.5316 D(x): 0.2080 D(G(z)): 0.2078 / 0.2175\n[3/100][1/3] Loss_D:1.7762 Loss_G:1.4899 D(x): 0.2175 D(G(z)): 0.2169 / 0.2269\n[3/100][2/3] Loss_D:1.7440 Loss_G:1.4498 D(x): 0.2274 D(G(z)): 0.2257 / 0.2362\n[4/100][0/3] Loss_D:1.7152 Loss_G:1.3950 D(x): 0.2380 D(G(z)): 0.2378 / 0.2499\n[4/100][1/3] Loss_D:1.6823 Loss_G:1.3466 D(x): 0.2497 D(G(z)): 0.2487 / 0.2625\n[4/100][2/3] Loss_D:1.6476 Loss_G:1.3019 D(x): 0.2629 D(G(z)): 0.2602 / 0.2747\n[5/100][0/3] Loss_D:1.6252 Loss_G:1.2521 D(x): 0.2745 D(G(z)): 0.2749 / 0.2889\n[5/100][1/3] Loss_D:1.5953 Loss_G:1.2025 D(x): 0.2891 D(G(z)): 0.2896 / 0.3040\n[5/100][2/3] Loss_D:1.5658 Loss_G:1.1337 D(x): 0.3034 D(G(z)): 0.3025 / 0.3259\n[6/100][0/3] Loss_D:1.5388 Loss_G:1.0974 D(x): 0.3217 D(G(z)): 0.3224 / 0.3383\n[6/100][1/3] Loss_D:1.5107 Loss_G:1.0386 D(x): 0.3395 D(G(z)): 0.3388 / 0.3592\n[6/100][2/3] Loss_D:1.4807 Loss_G:0.9837 D(x): 0.3590 D(G(z)): 0.3548 / 0.3799\n[7/100][0/3] Loss_D:1.4681 Loss_G:0.9290 D(x): 0.3798 D(G(z)): 0.3805 / 0.4019\n[7/100][1/3] Loss_D:1.4466 Loss_G:0.8774 D(x): 0.4026 D(G(z)): 0.4018 / 0.4234\n[7/100][2/3] Loss_D:1.4177 Loss_G:0.8175 D(x): 0.4262 D(G(z)): 0.4170 / 0.4505\n[8/100][0/3] Loss_D:1.4201 Loss_G:0.7708 D(x): 0.4487 D(G(z)): 0.4456 / 0.4718\n[8/100][1/3] Loss_D:1.4205 Loss_G:0.7237 D(x): 0.4712 D(G(z)): 0.4704 / 0.4950\n[8/100][2/3] Loss_D:1.4118 Loss_G:0.6806 D(x): 0.4983 D(G(z)): 0.4933 / 0.5172\n[9/100][0/3] Loss_D:1.4250 Loss_G:0.6406 D(x): 0.5145 D(G(z)): 0.5137 / 0.5384\n[9/100][1/3] Loss_D:1.4335 Loss_G:0.6268 D(x): 0.5319 D(G(z)): 0.5322 / 0.5463\n[9/100][2/3] Loss_D:1.4531 Loss_G:0.6112 D(x): 0.5446 D(G(z)): 0.5503 / 0.5544\n[10/100][0/3] Loss_D:1.4432 Loss_G:0.5966 D(x): 0.5603 D(G(z)): 0.5579 / 0.5631\n[10/100][1/3] Loss_D:1.4495 Loss_G:0.5928 D(x): 0.5625 D(G(z)): 0.5622 / 0.5650\n[10/100][2/3] Loss_D:1.4789 Loss_G:0.6143 D(x): 0.5524 D(G(z)): 0.5665 / 0.5532\n[11/100][0/3] Loss_D:1.4493 Loss_G:0.6103 D(x): 0.5593 D(G(z)): 0.5600 / 0.5551\n[11/100][1/3] Loss_D:1.4516 Loss_G:0.6280 D(x): 0.5534 D(G(z)): 0.5567 / 0.5452\n[11/100][2/3] Loss_D:1.4382 Loss_G:0.6390 D(x): 0.5494 D(G(z)): 0.5487 / 0.5388\n[12/100][0/3] Loss_D:1.4394 Loss_G:0.6603 D(x): 0.5341 D(G(z)): 0.5374 / 0.5274\n[12/100][1/3] Loss_D:1.4295 Loss_G:0.6820 D(x): 0.5251 D(G(z)): 0.5257 / 0.5159\n[12/100][2/3] Loss_D:1.4131 Loss_G:0.6913 D(x): 0.5162 D(G(z)): 0.5100 / 0.5105\n[13/100][0/3] Loss_D:1.4251 Loss_G:0.7212 D(x): 0.5037 D(G(z)): 0.5052 / 0.4954\n[13/100][1/3] Loss_D:1.4155 Loss_G:0.7389 D(x): 0.4964 D(G(z)): 0.4942 / 0.4866\n[13/100][2/3] Loss_D:1.4042 Loss_G:0.7671 D(x): 0.4941 D(G(z)): 0.4861 / 0.4724\n[14/100][0/3] Loss_D:1.4148 Loss_G:0.7710 D(x): 0.4781 D(G(z)): 0.4760 / 0.4710\n[14/100][1/3] Loss_D:1.4190 Loss_G:0.7811 D(x): 0.4717 D(G(z)): 0.4717 / 0.4661\n[14/100][2/3] Loss_D:1.4223 Loss_G:0.8009 D(x): 0.4592 D(G(z)): 0.4601 / 0.4570\n[15/100][0/3] Loss_D:1.4203 Loss_G:0.8001 D(x): 0.4615 D(G(z)): 0.4617 / 0.4571\n[15/100][1/3] Loss_D:1.4245 Loss_G:0.8088 D(x): 0.4553 D(G(z)): 0.4572 / 0.4530\n[15/100][2/3] Loss_D:1.4398 Loss_G:0.8121 D(x): 0.4504 D(G(z)): 0.4598 / 0.4516\n[16/100][0/3] Loss_D:1.4242 Loss_G:0.8129 D(x): 0.4512 D(G(z)): 0.4524 / 0.4511\n[16/100][1/3] Loss_D:1.4208 Loss_G:0.8083 D(x): 0.4519 D(G(z)): 0.4514 / 0.4530\n[16/100][2/3] Loss_D:1.4205 Loss_G:0.8162 D(x): 0.4475 D(G(z)): 0.4467 / 0.4497\n[17/100][0/3] Loss_D:1.4209 Loss_G:0.8087 D(x): 0.4531 D(G(z)): 0.4530 / 0.4529\n[17/100][1/3] Loss_D:1.4178 Loss_G:0.8020 D(x): 0.4552 D(G(z)): 0.4539 / 0.4558\n[17/100][2/3] Loss_D:1.4095 Loss_G:0.8029 D(x): 0.4604 D(G(z)): 0.4554 / 0.4555\n[18/100][0/3] Loss_D:1.4198 Loss_G:0.7885 D(x): 0.4584 D(G(z)): 0.4587 / 0.4622\n[18/100][1/3] Loss_D:1.4229 Loss_G:0.7885 D(x): 0.4597 D(G(z)): 0.4615 / 0.4622\n[18/100][2/3] Loss_D:1.4202 Loss_G:0.7705 D(x): 0.4656 D(G(z)): 0.4671 / 0.4711\n[19/100][0/3] Loss_D:1.4234 Loss_G:0.7692 D(x): 0.4649 D(G(z)): 0.4675 / 0.4712\n[19/100][1/3] Loss_D:1.4198 Loss_G:0.7620 D(x): 0.4715 D(G(z)): 0.4731 / 0.4746\n[19/100][2/3] Loss_D:1.4151 Loss_G:0.7435 D(x): 0.4721 D(G(z)): 0.4710 / 0.4833\n[20/100][0/3] Loss_D:1.4213 Loss_G:0.7435 D(x): 0.4775 D(G(z)): 0.4798 / 0.4835\n[20/100][1/3] Loss_D:1.4174 Loss_G:0.7391 D(x): 0.4801 D(G(z)): 0.4805 / 0.4858\n[20/100][2/3] Loss_D:1.4285 Loss_G:0.7283 D(x): 0.4810 D(G(z)): 0.4869 / 0.4911\n[21/100][0/3] Loss_D:1.4140 Loss_G:0.7334 D(x): 0.4862 D(G(z)): 0.4850 / 0.4883\n[21/100][1/3] Loss_D:1.4186 Loss_G:0.7306 D(x): 0.4889 D(G(z)): 0.4902 / 0.4898\n[21/100][2/3] Loss_D:1.4351 Loss_G:0.7222 D(x): 0.4859 D(G(z)): 0.4948 / 0.4940\n[22/100][0/3] Loss_D:1.4181 Loss_G:0.7267 D(x): 0.4910 D(G(z)): 0.4920 / 0.4918\n[22/100][1/3] Loss_D:1.4126 Loss_G:0.7254 D(x): 0.4932 D(G(z)): 0.4916 / 0.4923\n[22/100][2/3] Loss_D:1.4203 Loss_G:0.7375 D(x): 0.4879 D(G(z)): 0.4905 / 0.4863\n[23/100][0/3] Loss_D:1.4182 Loss_G:0.7276 D(x): 0.4914 D(G(z)): 0.4926 / 0.4911\n[23/100][1/3] Loss_D:1.4101 Loss_G:0.7274 D(x): 0.4933 D(G(z)): 0.4906 / 0.4910\n[23/100][2/3] Loss_D:1.3944 Loss_G:0.7326 D(x): 0.4954 D(G(z)): 0.4850 / 0.4887\n[24/100][0/3] Loss_D:1.4163 Loss_G:0.7348 D(x): 0.4883 D(G(z)): 0.4886 / 0.4874\n[24/100][1/3] Loss_D:1.4137 Loss_G:0.7334 D(x): 0.4914 D(G(z)): 0.4908 / 0.4882\n[24/100][2/3] Loss_D:1.3939 Loss_G:0.7300 D(x): 0.4956 D(G(z)): 0.4860 / 0.4894\n[25/100][0/3] Loss_D:1.4160 Loss_G:0.7401 D(x): 0.4885 D(G(z)): 0.4890 / 0.4848\n[25/100][1/3] Loss_D:1.4070 Loss_G:0.7340 D(x): 0.4882 D(G(z)): 0.4844 / 0.4877\n[25/100][2/3] Loss_D:1.4281 Loss_G:0.7328 D(x): 0.4813 D(G(z)): 0.4882 / 0.4881\n[26/100][0/3] Loss_D:1.4054 Loss_G:0.7391 D(x): 0.4874 D(G(z)): 0.4831 / 0.4851\n[26/100][1/3] Loss_D:1.4086 Loss_G:0.7481 D(x): 0.4861 D(G(z)): 0.4834 / 0.4808\n[26/100][2/3] Loss_D:1.4073 Loss_G:0.7512 D(x): 0.4842 D(G(z)): 0.4811 / 0.4790\n[27/100][0/3] Loss_D:1.4066 Loss_G:0.7424 D(x): 0.4841 D(G(z)): 0.4806 / 0.4834\n[27/100][1/3] Loss_D:1.4106 Loss_G:0.7466 D(x): 0.4847 D(G(z)): 0.4829 / 0.4815\n[27/100][2/3] Loss_D:1.4103 Loss_G:0.7457 D(x): 0.4877 D(G(z)): 0.4869 / 0.4818\n[28/100][0/3] Loss_D:1.4010 Loss_G:0.7483 D(x): 0.4845 D(G(z)): 0.4781 / 0.4804\n[28/100][1/3] Loss_D:1.4129 Loss_G:0.7494 D(x): 0.4808 D(G(z)): 0.4805 / 0.4799\n[28/100][2/3] Loss_D:1.3821 Loss_G:0.7512 D(x): 0.4912 D(G(z)): 0.4757 / 0.4791\n[29/100][0/3] Loss_D:1.4019 Loss_G:0.7512 D(x): 0.4853 D(G(z)): 0.4796 / 0.4791\n[29/100][1/3] Loss_D:1.4025 Loss_G:0.7476 D(x): 0.4857 D(G(z)): 0.4803 / 0.4805\n[29/100][2/3] Loss_D:1.4098 Loss_G:0.7623 D(x): 0.4853 D(G(z)): 0.4838 / 0.4736\n[30/100][0/3] Loss_D:1.3785 Loss_G:0.7419 D(x): 0.4975 D(G(z)): 0.4805 / 0.4835\n[30/100][1/3] Loss_D:1.4076 Loss_G:0.7432 D(x): 0.4871 D(G(z)): 0.4843 / 0.4828\n[30/100][2/3] Loss_D:1.4151 Loss_G:0.7242 D(x): 0.4841 D(G(z)): 0.4849 / 0.4920\n[31/100][0/3] Loss_D:1.3897 Loss_G:0.7364 D(x): 0.4966 D(G(z)): 0.4848 / 0.4859\n[31/100][1/3] Loss_D:1.3698 Loss_G:0.7390 D(x): 0.5074 D(G(z)): 0.4856 / 0.4847\n[31/100][2/3] Loss_D:1.3500 Loss_G:0.7215 D(x): 0.5241 D(G(z)): 0.4920 / 0.4929\n[32/100][0/3] Loss_D:1.3610 Loss_G:0.7300 D(x): 0.5116 D(G(z)): 0.4854 / 0.4890\n[32/100][1/3] Loss_D:1.3821 Loss_G:0.7327 D(x): 0.5029 D(G(z)): 0.4873 / 0.4875\n[32/100][2/3] Loss_D:1.3263 Loss_G:0.7356 D(x): 0.5371 D(G(z)): 0.4925 / 0.4863\n[33/100][0/3] Loss_D:1.3580 Loss_G:0.7304 D(x): 0.5143 D(G(z)): 0.4865 / 0.4885\n[33/100][1/3] Loss_D:1.3381 Loss_G:0.6910 D(x): 0.5258 D(G(z)): 0.4874 / 0.5093\n[33/100][2/3] Loss_D:1.4423 Loss_G:0.6347 D(x): 0.5295 D(G(z)): 0.5373 / 0.5385\n[34/100][0/3] Loss_D:1.4311 Loss_G:0.6426 D(x): 0.5351 D(G(z)): 0.5382 / 0.5345\n[34/100][1/3] Loss_D:1.4144 Loss_G:0.6542 D(x): 0.5367 D(G(z)): 0.5327 / 0.5280\n[34/100][2/3] Loss_D:1.4199 Loss_G:0.6607 D(x): 0.5290 D(G(z)): 0.5283 / 0.5240\n[35/100][0/3] Loss_D:1.4157 Loss_G:0.6816 D(x): 0.5234 D(G(z)): 0.5226 / 0.5133\n[35/100][1/3] Loss_D:1.4125 Loss_G:0.7003 D(x): 0.5140 D(G(z)): 0.5128 / 0.5037\n[35/100][2/3] Loss_D:1.4071 Loss_G:0.7112 D(x): 0.5084 D(G(z)): 0.5054 / 0.4978\n[36/100][0/3] Loss_D:1.4134 Loss_G:0.7312 D(x): 0.4979 D(G(z)): 0.4987 / 0.4882\n[36/100][1/3] Loss_D:1.4149 Loss_G:0.7490 D(x): 0.4883 D(G(z)): 0.4901 / 0.4793\n[36/100][2/3] Loss_D:1.3978 Loss_G:0.7589 D(x): 0.4868 D(G(z)): 0.4809 / 0.4745\n[37/100][0/3] Loss_D:1.4119 Loss_G:0.7704 D(x): 0.4745 D(G(z)): 0.4749 / 0.4689\n[37/100][1/3] Loss_D:1.4126 Loss_G:0.7720 D(x): 0.4696 D(G(z)): 0.4701 / 0.4681\n[37/100][2/3] Loss_D:1.4013 Loss_G:0.7792 D(x): 0.4648 D(G(z)): 0.4585 / 0.4649\n[38/100][0/3] Loss_D:1.4191 Loss_G:0.7869 D(x): 0.4603 D(G(z)): 0.4632 / 0.4610\n[38/100][1/3] Loss_D:1.4174 Loss_G:0.7863 D(x): 0.4607 D(G(z)): 0.4630 / 0.4613\n[38/100][2/3] Loss_D:1.4289 Loss_G:0.7739 D(x): 0.4578 D(G(z)): 0.4656 / 0.4676\n[39/100][0/3] Loss_D:1.4123 Loss_G:0.7853 D(x): 0.4606 D(G(z)): 0.4604 / 0.4617\n[39/100][1/3] Loss_D:1.4149 Loss_G:0.7772 D(x): 0.4619 D(G(z)): 0.4631 / 0.4655\n[39/100][2/3] Loss_D:1.4308 Loss_G:0.7705 D(x): 0.4603 D(G(z)): 0.4696 / 0.4685\n[40/100][0/3] Loss_D:1.4127 Loss_G:0.7651 D(x): 0.4659 D(G(z)): 0.4666 / 0.4712\n[40/100][1/3] Loss_D:1.4081 Loss_G:0.7557 D(x): 0.4708 D(G(z)): 0.4696 / 0.4756\n[40/100][2/3] Loss_D:1.4100 Loss_G:0.7532 D(x): 0.4752 D(G(z)): 0.4751 / 0.4768\n[41/100][0/3] Loss_D:1.4074 Loss_G:0.7379 D(x): 0.4789 D(G(z)): 0.4780 / 0.4842\n[41/100][1/3] Loss_D:1.4095 Loss_G:0.7328 D(x): 0.4835 D(G(z)): 0.4839 / 0.4867\n[41/100][2/3] Loss_D:1.4324 Loss_G:0.7267 D(x): 0.4813 D(G(z)): 0.4928 / 0.4892\n[42/100][0/3] Loss_D:1.4037 Loss_G:0.7166 D(x): 0.4884 D(G(z)): 0.4860 / 0.4946\n[42/100][1/3] Loss_D:1.4026 Loss_G:0.7176 D(x): 0.4935 D(G(z)): 0.4904 / 0.4942\n[42/100][2/3] Loss_D:1.4081 Loss_G:0.7106 D(x): 0.4963 D(G(z)): 0.4956 / 0.4969\n[43/100][0/3] Loss_D:1.4109 Loss_G:0.7092 D(x): 0.4957 D(G(z)): 0.4966 / 0.4984\n[43/100][1/3] Loss_D:1.4085 Loss_G:0.7101 D(x): 0.4991 D(G(z)): 0.4988 / 0.4977\n[43/100][2/3] Loss_D:1.4025 Loss_G:0.7205 D(x): 0.5028 D(G(z)): 0.4992 / 0.4929\n[44/100][0/3] Loss_D:1.4149 Loss_G:0.7116 D(x): 0.4962 D(G(z)): 0.4991 / 0.4969\n[44/100][1/3] Loss_D:1.4115 Loss_G:0.7160 D(x): 0.4947 D(G(z)): 0.4961 / 0.4948\n[44/100][2/3] Loss_D:1.3929 Loss_G:0.7231 D(x): 0.5015 D(G(z)): 0.4935 / 0.4909\n[45/100][0/3] Loss_D:1.4049 Loss_G:0.7249 D(x): 0.4934 D(G(z)): 0.4918 / 0.4903\n[45/100][1/3] Loss_D:1.4151 Loss_G:0.7312 D(x): 0.4891 D(G(z)): 0.4926 / 0.4871\n[45/100][2/3] Loss_D:1.4059 Loss_G:0.7366 D(x): 0.4883 D(G(z)): 0.4867 / 0.4846\n[46/100][0/3] Loss_D:1.4043 Loss_G:0.7377 D(x): 0.4876 D(G(z)): 0.4859 / 0.4839\n[46/100][1/3] Loss_D:1.4089 Loss_G:0.7367 D(x): 0.4860 D(G(z)): 0.4865 / 0.4844\n[46/100][2/3] Loss_D:1.4006 Loss_G:0.7343 D(x): 0.4878 D(G(z)): 0.4839 / 0.4854\n[47/100][0/3] Loss_D:1.4044 Loss_G:0.7423 D(x): 0.4855 D(G(z)): 0.4842 / 0.4816\n[47/100][1/3] Loss_D:1.4078 Loss_G:0.7414 D(x): 0.4815 D(G(z)): 0.4817 / 0.4820\n[47/100][2/3] Loss_D:1.3861 Loss_G:0.7507 D(x): 0.4837 D(G(z)): 0.4731 / 0.4774\n[48/100][0/3] Loss_D:1.4031 Loss_G:0.7386 D(x): 0.4831 D(G(z)): 0.4812 / 0.4832\n[48/100][1/3] Loss_D:1.4058 Loss_G:0.7373 D(x): 0.4843 D(G(z)): 0.4836 / 0.4838\n[48/100][2/3] Loss_D:1.3989 Loss_G:0.7425 D(x): 0.4882 D(G(z)): 0.4839 / 0.4814\n[49/100][0/3] Loss_D:1.4104 Loss_G:0.7310 D(x): 0.4839 D(G(z)): 0.4855 / 0.4870\n[49/100][1/3] Loss_D:1.4042 Loss_G:0.7341 D(x): 0.4889 D(G(z)): 0.4874 / 0.4854\n[49/100][2/3] Loss_D:1.4103 Loss_G:0.7487 D(x): 0.4819 D(G(z)): 0.4842 / 0.4782\n[50/100][0/3] Loss_D:1.4091 Loss_G:0.7246 D(x): 0.4880 D(G(z)): 0.4893 / 0.4900\n[50/100][1/3] Loss_D:1.4116 Loss_G:0.7233 D(x): 0.4876 D(G(z)): 0.4902 / 0.4907\n[50/100][2/3] Loss_D:1.4141 Loss_G:0.7244 D(x): 0.4872 D(G(z)): 0.4912 / 0.4899\n[51/100][0/3] Loss_D:1.4004 Loss_G:0.7231 D(x): 0.4908 D(G(z)): 0.4877 / 0.4906\n[51/100][1/3] Loss_D:1.4063 Loss_G:0.7279 D(x): 0.4907 D(G(z)): 0.4907 / 0.4881\n[51/100][2/3] Loss_D:1.4056 Loss_G:0.7266 D(x): 0.4881 D(G(z)): 0.4878 / 0.4887\n[52/100][0/3] Loss_D:1.4021 Loss_G:0.7242 D(x): 0.4919 D(G(z)): 0.4901 / 0.4900\n[52/100][1/3] Loss_D:1.4115 Loss_G:0.7259 D(x): 0.4880 D(G(z)): 0.4908 / 0.4891\n[52/100][2/3] Loss_D:1.4162 Loss_G:0.7441 D(x): 0.4916 D(G(z)): 0.4966 / 0.4802\n[53/100][0/3] Loss_D:1.4120 Loss_G:0.7288 D(x): 0.4877 D(G(z)): 0.4910 / 0.4875\n[53/100][1/3] Loss_D:1.4077 Loss_G:0.7276 D(x): 0.4884 D(G(z)): 0.4895 / 0.4883\n[53/100][2/3] Loss_D:1.3958 Loss_G:0.7303 D(x): 0.4918 D(G(z)): 0.4875 / 0.4864\n[54/100][0/3] Loss_D:1.4029 Loss_G:0.7293 D(x): 0.4877 D(G(z)): 0.4866 / 0.4873\n[54/100][1/3] Loss_D:1.4035 Loss_G:0.7329 D(x): 0.4882 D(G(z)): 0.4875 / 0.4854\n[54/100][2/3] Loss_D:1.3985 Loss_G:0.7422 D(x): 0.4864 D(G(z)): 0.4836 / 0.4809\n[55/100][0/3] Loss_D:1.4097 Loss_G:0.7252 D(x): 0.4849 D(G(z)): 0.4873 / 0.4892\n[55/100][1/3] Loss_D:1.4069 Loss_G:0.7257 D(x): 0.4868 D(G(z)): 0.4880 / 0.4889\n[55/100][2/3] Loss_D:1.4148 Loss_G:0.7127 D(x): 0.4863 D(G(z)): 0.4909 / 0.4951\n[56/100][0/3] Loss_D:1.4069 Loss_G:0.7248 D(x): 0.4884 D(G(z)): 0.4894 / 0.4893\n[56/100][1/3] Loss_D:1.4099 Loss_G:0.7286 D(x): 0.4863 D(G(z)): 0.4889 / 0.4874\n[56/100][2/3] Loss_D:1.4111 Loss_G:0.7242 D(x): 0.4864 D(G(z)): 0.4895 / 0.4894\n[57/100][0/3] Loss_D:1.4056 Loss_G:0.7248 D(x): 0.4881 D(G(z)): 0.4888 / 0.4893\n[57/100][1/3] Loss_D:1.4038 Loss_G:0.7290 D(x): 0.4906 D(G(z)): 0.4903 / 0.4871\n[57/100][2/3] Loss_D:1.4181 Loss_G:0.7266 D(x): 0.4844 D(G(z)): 0.4913 / 0.4881\n[58/100][0/3] Loss_D:1.4046 Loss_G:0.7274 D(x): 0.4885 D(G(z)): 0.4887 / 0.4878\n[58/100][1/3] Loss_D:1.4004 Loss_G:0.7250 D(x): 0.4895 D(G(z)): 0.4877 / 0.4890\n[58/100][2/3] Loss_D:1.4059 Loss_G:0.7251 D(x): 0.4877 D(G(z)): 0.4888 / 0.4890\n[59/100][0/3] Loss_D:1.4022 Loss_G:0.7235 D(x): 0.4907 D(G(z)): 0.4901 / 0.4896\n[59/100][1/3] Loss_D:1.3996 Loss_G:0.7184 D(x): 0.4917 D(G(z)): 0.4900 / 0.4921\n[59/100][2/3] Loss_D:1.3887 Loss_G:0.7181 D(x): 0.4918 D(G(z)): 0.4846 / 0.4924\n[60/100][0/3] Loss_D:1.4017 Loss_G:0.7175 D(x): 0.4919 D(G(z)): 0.4913 / 0.4925\n[60/100][1/3] Loss_D:1.4042 Loss_G:0.7171 D(x): 0.4911 D(G(z)): 0.4918 / 0.4927\n[60/100][2/3] Loss_D:1.4303 Loss_G:0.7113 D(x): 0.4853 D(G(z)): 0.4985 / 0.4953\n[61/100][0/3] Loss_D:1.4046 Loss_G:0.7176 D(x): 0.4922 D(G(z)): 0.4930 / 0.4923\n[61/100][1/3] Loss_D:1.4026 Loss_G:0.7241 D(x): 0.4918 D(G(z)): 0.4918 / 0.4891\n[61/100][2/3] Loss_D:1.3870 Loss_G:0.7211 D(x): 0.4967 D(G(z)): 0.4894 / 0.4905\n[62/100][0/3] Loss_D:1.3976 Loss_G:0.7190 D(x): 0.4910 D(G(z)): 0.4886 / 0.4916\n[62/100][1/3] Loss_D:1.4021 Loss_G:0.7194 D(x): 0.4909 D(G(z)): 0.4907 / 0.4914\n[62/100][2/3] Loss_D:1.4164 Loss_G:0.7205 D(x): 0.4918 D(G(z)): 0.4984 / 0.4906\n[63/100][0/3] Loss_D:1.4010 Loss_G:0.7233 D(x): 0.4901 D(G(z)): 0.4895 / 0.4894\n[63/100][1/3] Loss_D:1.3997 Loss_G:0.7199 D(x): 0.4924 D(G(z)): 0.4912 / 0.4909\n[63/100][2/3] Loss_D:1.3993 Loss_G:0.7066 D(x): 0.4860 D(G(z)): 0.4846 / 0.4976\n[64/100][0/3] Loss_D:1.3993 Loss_G:0.7209 D(x): 0.4911 D(G(z)): 0.4899 / 0.4904\n[64/100][1/3] Loss_D:1.3987 Loss_G:0.7204 D(x): 0.4914 D(G(z)): 0.4899 / 0.4907\n[64/100][2/3] Loss_D:1.3962 Loss_G:0.7192 D(x): 0.4928 D(G(z)): 0.4899 / 0.4908\n[65/100][0/3] Loss_D:1.3967 Loss_G:0.7179 D(x): 0.4916 D(G(z)): 0.4891 / 0.4919\n[65/100][1/3] Loss_D:1.4044 Loss_G:0.7120 D(x): 0.4921 D(G(z)): 0.4935 / 0.4948\n[65/100][2/3] Loss_D:1.4038 Loss_G:0.7084 D(x): 0.4919 D(G(z)): 0.4933 / 0.4967\n[66/100][0/3] Loss_D:1.3992 Loss_G:0.7180 D(x): 0.4931 D(G(z)): 0.4918 / 0.4918\n[66/100][1/3] Loss_D:1.4007 Loss_G:0.7134 D(x): 0.4942 D(G(z)): 0.4938 / 0.4940\n[66/100][2/3] Loss_D:1.3988 Loss_G:0.7234 D(x): 0.4909 D(G(z)): 0.4897 / 0.4892\n[67/100][0/3] Loss_D:1.4062 Loss_G:0.7211 D(x): 0.4908 D(G(z)): 0.4933 / 0.4901\n[67/100][1/3] Loss_D:1.3975 Loss_G:0.7209 D(x): 0.4923 D(G(z)): 0.4905 / 0.4903\n[67/100][2/3] Loss_D:1.3929 Loss_G:0.7315 D(x): 0.4931 D(G(z)): 0.4889 / 0.4850\n[68/100][0/3] Loss_D:1.4028 Loss_G:0.7177 D(x): 0.4897 D(G(z)): 0.4905 / 0.4917\n[68/100][1/3] Loss_D:1.4028 Loss_G:0.7262 D(x): 0.4891 D(G(z)): 0.4899 / 0.4877\n[68/100][2/3] Loss_D:1.3977 Loss_G:0.7291 D(x): 0.4898 D(G(z)): 0.4880 / 0.4859\n[69/100][0/3] Loss_D:1.3979 Loss_G:0.7200 D(x): 0.4892 D(G(z)): 0.4877 / 0.4906\n[69/100][1/3] Loss_D:1.4006 Loss_G:0.7166 D(x): 0.4906 D(G(z)): 0.4906 / 0.4922\n[69/100][2/3] Loss_D:1.3955 Loss_G:0.7293 D(x): 0.4966 D(G(z)): 0.4938 / 0.4859\n[70/100][0/3] Loss_D:1.4035 Loss_G:0.7171 D(x): 0.4913 D(G(z)): 0.4927 / 0.4919\n[70/100][1/3] Loss_D:1.4008 Loss_G:0.7168 D(x): 0.4925 D(G(z)): 0.4926 / 0.4921\n[70/100][2/3] Loss_D:1.3979 Loss_G:0.7263 D(x): 0.4963 D(G(z)): 0.4942 / 0.4874\n[71/100][0/3] Loss_D:1.3991 Loss_G:0.7165 D(x): 0.4938 D(G(z)): 0.4932 / 0.4922\n[71/100][1/3] Loss_D:1.4033 Loss_G:0.7170 D(x): 0.4923 D(G(z)): 0.4938 / 0.4920\n[71/100][2/3] Loss_D:1.3905 Loss_G:0.7174 D(x): 0.4983 D(G(z)): 0.4934 / 0.4919\n[72/100][0/3] Loss_D:1.3977 Loss_G:0.7190 D(x): 0.4921 D(G(z)): 0.4908 / 0.4909\n[72/100][1/3] Loss_D:1.3956 Loss_G:0.7186 D(x): 0.4926 D(G(z)): 0.4904 / 0.4911\n[72/100][2/3] Loss_D:1.3931 Loss_G:0.7166 D(x): 0.4935 D(G(z)): 0.4901 / 0.4921\n[73/100][0/3] Loss_D:1.3971 Loss_G:0.7156 D(x): 0.4938 D(G(z)): 0.4924 / 0.4926\n[73/100][1/3] Loss_D:1.4030 Loss_G:0.7167 D(x): 0.4900 D(G(z)): 0.4915 / 0.4920\n[73/100][2/3] Loss_D:1.3962 Loss_G:0.7122 D(x): 0.4931 D(G(z)): 0.4915 / 0.4943\n[74/100][0/3] Loss_D:1.3968 Loss_G:0.7124 D(x): 0.4928 D(G(z)): 0.4912 / 0.4940\n[74/100][1/3] Loss_D:1.4018 Loss_G:0.7148 D(x): 0.4910 D(G(z)): 0.4922 / 0.4929\n[74/100][2/3] Loss_D:1.4073 Loss_G:0.7164 D(x): 0.4882 D(G(z)): 0.4921 / 0.4922\n[75/100][0/3] Loss_D:1.3998 Loss_G:0.7144 D(x): 0.4933 D(G(z)): 0.4933 / 0.4930\n[75/100][1/3] Loss_D:1.4003 Loss_G:0.7126 D(x): 0.4936 D(G(z)): 0.4939 / 0.4940\n[75/100][2/3] Loss_D:1.4162 Loss_G:0.7173 D(x): 0.4940 D(G(z)): 0.5021 / 0.4916\n[76/100][0/3] Loss_D:1.4004 Loss_G:0.7186 D(x): 0.4929 D(G(z)): 0.4934 / 0.4909\n[76/100][1/3] Loss_D:1.3970 Loss_G:0.7182 D(x): 0.4915 D(G(z)): 0.4904 / 0.4912\n[76/100][2/3] Loss_D:1.4011 Loss_G:0.7226 D(x): 0.4902 D(G(z)): 0.4909 / 0.4886\n[77/100][0/3] Loss_D:1.3981 Loss_G:0.7232 D(x): 0.4893 D(G(z)): 0.4886 / 0.4886\n[77/100][1/3] Loss_D:1.4011 Loss_G:0.7255 D(x): 0.4862 D(G(z)): 0.4871 / 0.4873\n[77/100][2/3] Loss_D:1.4121 Loss_G:0.7305 D(x): 0.4820 D(G(z)): 0.4882 / 0.4848\n[78/100][0/3] Loss_D:1.3977 Loss_G:0.7245 D(x): 0.4872 D(G(z)): 0.4864 / 0.4879\n[78/100][1/3] Loss_D:1.3945 Loss_G:0.7206 D(x): 0.4889 D(G(z)): 0.4866 / 0.4897\n[78/100][2/3] Loss_D:1.4062 Loss_G:0.7090 D(x): 0.4887 D(G(z)): 0.4923 / 0.4955\n[79/100][0/3] Loss_D:1.3988 Loss_G:0.7230 D(x): 0.4905 D(G(z)): 0.4904 / 0.4886\n[79/100][1/3] Loss_D:1.4017 Loss_G:0.7187 D(x): 0.4894 D(G(z)): 0.4908 / 0.4907\n[79/100][2/3] Loss_D:1.4020 Loss_G:0.7071 D(x): 0.4904 D(G(z)): 0.4919 / 0.4962\n[80/100][0/3] Loss_D:1.4019 Loss_G:0.7137 D(x): 0.4917 D(G(z)): 0.4933 / 0.4931\n[80/100][1/3] Loss_D:1.3944 Loss_G:0.7116 D(x): 0.4928 D(G(z)): 0.4906 / 0.4942\n[80/100][2/3] Loss_D:1.3851 Loss_G:0.7354 D(x): 0.4963 D(G(z)): 0.4895 / 0.4824\n[81/100][0/3] Loss_D:1.4030 Loss_G:0.7127 D(x): 0.4924 D(G(z)): 0.4946 / 0.4936\n[81/100][1/3] Loss_D:1.4031 Loss_G:0.7133 D(x): 0.4916 D(G(z)): 0.4939 / 0.4933\n[81/100][2/3] Loss_D:1.3982 Loss_G:0.7191 D(x): 0.4897 D(G(z)): 0.4897 / 0.4908\n[82/100][0/3] Loss_D:1.3980 Loss_G:0.7106 D(x): 0.4945 D(G(z)): 0.4943 / 0.4945\n[82/100][1/3] Loss_D:1.3982 Loss_G:0.7138 D(x): 0.4949 D(G(z)): 0.4947 / 0.4931\n[82/100][2/3] Loss_D:1.4086 Loss_G:0.7126 D(x): 0.4953 D(G(z)): 0.5005 / 0.4935\n[83/100][0/3] Loss_D:1.3997 Loss_G:0.7186 D(x): 0.4918 D(G(z)): 0.4926 / 0.4906\n[83/100][1/3] Loss_D:1.3972 Loss_G:0.7203 D(x): 0.4902 D(G(z)): 0.4896 / 0.4897\n[83/100][2/3] Loss_D:1.4025 Loss_G:0.7173 D(x): 0.4834 D(G(z)): 0.4852 / 0.4912\n[84/100][0/3] Loss_D:1.4003 Loss_G:0.7245 D(x): 0.4875 D(G(z)): 0.4884 / 0.4877\n[84/100][1/3] Loss_D:1.3972 Loss_G:0.7215 D(x): 0.4909 D(G(z)): 0.4904 / 0.4891\n[84/100][2/3] Loss_D:1.4053 Loss_G:0.7299 D(x): 0.4880 D(G(z)): 0.4915 / 0.4849\n[85/100][0/3] Loss_D:1.3979 Loss_G:0.7156 D(x): 0.4911 D(G(z)): 0.4911 / 0.4920\n[85/100][1/3] Loss_D:1.3986 Loss_G:0.7163 D(x): 0.4906 D(G(z)): 0.4908 / 0.4916\n[85/100][2/3] Loss_D:1.4129 Loss_G:0.7137 D(x): 0.4827 D(G(z)): 0.4899 / 0.4928\n[86/100][0/3] Loss_D:1.4016 Loss_G:0.7156 D(x): 0.4912 D(G(z)): 0.4930 / 0.4920\n[86/100][1/3] Loss_D:1.3971 Loss_G:0.7125 D(x): 0.4924 D(G(z)): 0.4921 / 0.4934\n[86/100][2/3] Loss_D:1.4077 Loss_G:0.7094 D(x): 0.4852 D(G(z)): 0.4900 / 0.4949\n[87/100][0/3] Loss_D:1.3972 Loss_G:0.7146 D(x): 0.4930 D(G(z)): 0.4925 / 0.4925\n[87/100][1/3] Loss_D:1.3975 Loss_G:0.7088 D(x): 0.4936 D(G(z)): 0.4934 / 0.4952\n[87/100][2/3] Loss_D:1.3932 Loss_G:0.7096 D(x): 0.4951 D(G(z)): 0.4929 / 0.4950\n[88/100][0/3] Loss_D:1.3995 Loss_G:0.7106 D(x): 0.4952 D(G(z)): 0.4961 / 0.4944\n[88/100][1/3] Loss_D:1.3984 Loss_G:0.7108 D(x): 0.4937 D(G(z)): 0.4940 / 0.4943\n[88/100][2/3] Loss_D:1.3795 Loss_G:0.7165 D(x): 0.5015 D(G(z)): 0.4927 / 0.4912\n[89/100][0/3] Loss_D:1.3929 Loss_G:0.7119 D(x): 0.4951 D(G(z)): 0.4927 / 0.4937\n[89/100][1/3] Loss_D:1.3948 Loss_G:0.7134 D(x): 0.4940 D(G(z)): 0.4927 / 0.4929\n[89/100][2/3] Loss_D:1.3899 Loss_G:0.7126 D(x): 0.5001 D(G(z)): 0.4961 / 0.4933\n[90/100][0/3] Loss_D:1.4009 Loss_G:0.7144 D(x): 0.4933 D(G(z)): 0.4950 / 0.4925\n[90/100][1/3] Loss_D:1.4007 Loss_G:0.7109 D(x): 0.4943 D(G(z)): 0.4958 / 0.4942\n[90/100][2/3] Loss_D:1.4043 Loss_G:0.7108 D(x): 0.4898 D(G(z)): 0.4926 / 0.4942\n[91/100][0/3] Loss_D:1.3992 Loss_G:0.7115 D(x): 0.4938 D(G(z)): 0.4947 / 0.4938\n[91/100][1/3] Loss_D:1.3996 Loss_G:0.7096 D(x): 0.4931 D(G(z)): 0.4942 / 0.4948\n[91/100][2/3] Loss_D:1.3894 Loss_G:0.7111 D(x): 0.4947 D(G(z)): 0.4907 / 0.4940\n[92/100][0/3] Loss_D:1.3914 Loss_G:0.7171 D(x): 0.4937 D(G(z)): 0.4908 / 0.4911\n[92/100][1/3] Loss_D:1.3970 Loss_G:0.7159 D(x): 0.4920 D(G(z)): 0.4919 / 0.4916\n[92/100][2/3] Loss_D:1.3840 Loss_G:0.7109 D(x): 0.4951 D(G(z)): 0.4881 / 0.4938\n[93/100][0/3] Loss_D:1.3916 Loss_G:0.7119 D(x): 0.4942 D(G(z)): 0.4913 / 0.4936\n[93/100][1/3] Loss_D:1.3946 Loss_G:0.7056 D(x): 0.4950 D(G(z)): 0.4937 / 0.4966\n[93/100][2/3] Loss_D:1.3910 Loss_G:0.7106 D(x): 0.4961 D(G(z)): 0.4930 / 0.4944\n[94/100][0/3] Loss_D:1.3959 Loss_G:0.7027 D(x): 0.4969 D(G(z)): 0.4963 / 0.4981\n[94/100][1/3] Loss_D:1.3928 Loss_G:0.7071 D(x): 0.4978 D(G(z)): 0.4957 / 0.4960\n[94/100][2/3] Loss_D:1.3865 Loss_G:0.7095 D(x): 0.4990 D(G(z)): 0.4940 / 0.4949\n[95/100][0/3] Loss_D:1.3959 Loss_G:0.7021 D(x): 0.4990 D(G(z)): 0.4983 / 0.4985\n[95/100][1/3] Loss_D:1.4012 Loss_G:0.7006 D(x): 0.4975 D(G(z)): 0.4995 / 0.4992\n[95/100][2/3] Loss_D:1.4009 Loss_G:0.7006 D(x): 0.4988 D(G(z)): 0.5006 / 0.4991\n[96/100][0/3] Loss_D:1.3992 Loss_G:0.7105 D(x): 0.4955 D(G(z)): 0.4965 / 0.4942\n[96/100][1/3] Loss_D:1.4001 Loss_G:0.7127 D(x): 0.4919 D(G(z)): 0.4934 / 0.4931\n[96/100][2/3] Loss_D:1.3970 Loss_G:0.7138 D(x): 0.4914 D(G(z)): 0.4913 / 0.4925\n[97/100][0/3] Loss_D:1.3983 Loss_G:0.7166 D(x): 0.4918 D(G(z)): 0.4925 / 0.4912\n[97/100][1/3] Loss_D:1.3952 Loss_G:0.7179 D(x): 0.4907 D(G(z)): 0.4899 / 0.4905\n[97/100][2/3] Loss_D:1.3956 Loss_G:0.7165 D(x): 0.4892 D(G(z)): 0.4886 / 0.4912\n[98/100][0/3] Loss_D:1.4019 Loss_G:0.7186 D(x): 0.4891 D(G(z)): 0.4915 / 0.4901\n[98/100][1/3] Loss_D:1.3976 Loss_G:0.7179 D(x): 0.4897 D(G(z)): 0.4900 / 0.4905\n[98/100][2/3] Loss_D:1.3988 Loss_G:0.7255 D(x): 0.4917 D(G(z)): 0.4924 / 0.4871\n[99/100][0/3] Loss_D:1.3968 Loss_G:0.7134 D(x): 0.4917 D(G(z)): 0.4917 / 0.4927\n[99/100][1/3] Loss_D:1.3950 Loss_G:0.7102 D(x): 0.4920 D(G(z)): 0.4911 / 0.4943\n[99/100][2/3] Loss_D:1.4109 Loss_G:0.7029 D(x): 0.4922 D(G(z)): 0.4987 / 0.4980\n\u001b[32m[I 2022-09-11 10:58:53,185]\u001b[0m Trial 9 finished with value: 1.1483393907546997 and parameters: {'lr': 0.00037268558268820204, 'dropoutG': 0.4, 'dropoutD': 0.30000000000000004, 'optimizer_name': 'Adam'}. Best is trial 1 with value: 0.5526883006095886.\u001b[0m\n"
            ]
          }
        }
      ],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "params_trial = \"\\n\".join([f\"    {k}: {v}\" for k, v in study.best_trial.params.items()])\nprint(f\"Study statistics:\\n Number of finished trials: {len(study.trials)}\\n Best trial:\\n  Trial number: {study.best_trial.number}\\n  Trial value: {study.best_trial.value}\\n  Params:\\n{params_trial}\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6ea98cc6-8a35-493a-bac7-2ef088375711"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "Study statistics:\n Number of finished trials: 10\n Best trial:\n  Trial number: 1\n  Trial value: 0.5526883006095886\n  Params:\n    lr: 0.016045855026896795\n    dropoutG: 0.2\n    dropoutD: 0.30000000000000004\n    optimizer_name: Adadelta\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "Study statistics:\n Number of finished trials: 10\n Best trial:\n  Trial number: 1\n  Trial value: 0.5526883006095886\n  Params:\n    lr: 0.016045855026896795\n    dropoutG: 0.2\n    dropoutD: 0.30000000000000004\n    optimizer_name: Adadelta\n"
            ]
          }
        }
      ],
      "execution_count": 0
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.8.10",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "TSGAN_Hypertuning",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 4033785765254506
    },
    "gpuClass": "standard",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}